[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelado Bayesiano",
    "section": "",
    "text": "‎\nLa estadística trata sobre la recolección, organización, análisis e interpretación de datos, es por ello que la estadística es esencial para el correcto análisis de datos.\nExisten dos grandes conjuntos de herramientas para analizar datos:\nAnálisis Exploratorio de Datos (EDA): Consiste en resúmenes numéricos como la media, moda, desviación estándar, rangos intercuartiles, etc (esto se conoce también como estadística descriptiva). Además hace énfasis en el uso de métodos visuales para inspeccionar los datos, como por ejemplo histogramas y gráficos de dispersión.\nEstadística Inferencial: Consiste en usar datos para generar enunciados que exceden los propios datos. A veces esto implica realizar predicciones, a veces entender los detalles de algún fenómeno en particular o elegir entre varias explicaciones plausibles.\nMuchos de los cursos y libros sobre estadística, principalmente aquellos dirigidos a no-estadísticos, enseñan una serie de recetas que más o menos tienen la siguiente forma.\nLa principal meta de estos cursos es la de enseñar a usar la lata adecuada y con suerte alguna que otra discusión sobre el emplatado. Esta aproximación pedagógica, dificulta entender conceptualmente la unidad de los diferentes métodos enseñados y tiene como resultado la reproducción de prácticas poco transparentes y/o útiles.\nEn este curso se intenta una aproximación totalmente diferente. También aprenderemos recetas, pero intentaremos que los platos tengan un sabor más casero y menos enlatado, aprenderemos a mezclar ingredientes frescos que se acomoden a diferentes situaciones gastronómicas.\nEste enfoque es posible por dos razones:"
  },
  {
    "objectID": "index.html#a-quienes-está-dirijido",
    "href": "index.html#a-quienes-está-dirijido",
    "title": "Modelado Bayesiano",
    "section": "A quienes está dirijido?",
    "text": "A quienes está dirijido?\nEste es un curso introductorio para personas sin conocimiento previo de estadística o ciencia de datos. Se asume familiaridad con Python y librerías de Python usadas en análisis de datos como Numpy, matplotlib, Pandas, etc.\nQuienes no sepan Python, pero tengan familiaridad con otros lenguajes de programación también podrán aprovechar el curso, aunque puede que experimente un poco más de fricción.\nPor último quienes no tengan interés en aprender a usar código para analisis de datos pueden aún aprovechar parte del material para obtener una visión a vuelo de pájaro de los métodos Bayesianos."
  },
  {
    "objectID": "index.html#cómo-usar-este-material",
    "href": "index.html#cómo-usar-este-material",
    "title": "Modelado Bayesiano",
    "section": "Cómo usar este material",
    "text": "Cómo usar este material\n\nVersión estática: Esta página contiene una versión estática del material. Es decir podrás ver el texto y las figuras pero no podrás modificarlos, ni interactuar con el material.\nVersión interactiva online: . Esta versión permite interactuar con el material, modificarlo y ejecutarlo en tu navegador.\nVersión interactiva local: También es posible descargar el material y ejecutarlo en tu propia computadora. Para ello hacé click y seguí las instrucciones de la próxima sección (Instalación)."
  },
  {
    "objectID": "index.html#instalación",
    "href": "index.html#instalación",
    "title": "Modelado Bayesiano",
    "section": "Instalación",
    "text": "Instalación\nPara usar este material es necesario tener instalado Python. Se recomienda la versión 3.9 o superior. Además es necesario instalar los siguientes paquetes:\n\nPyMC 5.3.0\nArviZ 0.15.1\nPreliZ 0.3.0\ngraphviz (una dependencia opcional de PyMC)\n\nSe recomienda instalar primero Anaconda. Luego instalar el resto de los paquetes con los comandos:\nconda install pip\npip install pymc==5.3.0 arviz==0.15.1 preliz==0.3.0 graphviz\nComo alternativa pueden crear un ambiente con los paquetes necesario descargando el archivo y ejecutando el comando\nconda env create -f environment.yml"
  },
  {
    "objectID": "index.html#contribuciones",
    "href": "index.html#contribuciones",
    "title": "Modelado Bayesiano",
    "section": "Contribuciones",
    "text": "Contribuciones\nTodo el contenido de este repositorio es abierto, esto quiere decir que cualquier persona interesada puede contribuir al mismo. Todas las contribuciones serán bien recibidas incluyendo:\n\nCorrecciones ortográficas\nNuevas figuras\nCorrecciones en el código Python, incluidas mejoras de estilo\nMejores ejemplos\nMejores explicaciones\nCorrecciones de errores conceptuales\n\nLa forma de contribuir es vía Github, es decir los cambios deberán ser hechos en forma de pull requests y los problemas/bugs deberán reportarse como Issues."
  },
  {
    "objectID": "00_Probabilidad.html#azahar-azar-y-dados",
    "href": "00_Probabilidad.html#azahar-azar-y-dados",
    "title": "1  Probabilidad",
    "section": "1.1 Azahar, azar y dados",
    "text": "1.1 Azahar, azar y dados\nLas palabras azahar y azar no son similares por casualidad ambas provienen de la misma palabra árabe que significa flor. Desde la antigüedad, y hasta el día de hoy, ciertos juegos, como el juego de la taba, utilizan un hueso con dos lados planos a modo de dado. De hecho se podría decir que la taba es el antecesor del dado moderno. Para facilitar distinguir un lado del otro, es común que uno de los lados esté marcado de alguna forma. Resulta ser que los árabes usaban una flor. Con el tiempo el castellano adoptó azahar, para designar solo ciertas flores como las del naranjo y azar como sinónimo de aleatorio.\nEmpecemos, entonces imaginando que tenemos un dado de 6 caras, cada vez que arrojamos el dado solo es posible obtener un número entero del 1-6 es decir {1, 2, 3, 4, 5, 6}. Al arrojar el dado podemos obtener cualquier de estos números sin preferencia de uno sobre otro. Usando Python podemos programar un dado de 6 caras de la siguiente forma:\n\ndef dado():\n    semilla = time.perf_counter_ns()\n    return semilla % 6 + 1\n\ndado()\n\n6\n\n\nAl ejecutar la celda anterior repetidas veces podrán ver que en cada ejecución se obtiene un número (entero) distinto entre 1 y 6. Entender que está haciendo exactamente esta función no es del todo relevante, pero si es útil entender la idea general detrás de esta función. Veamos:\nLas computadoras son en esencia deterministas. Es decir para una misma entrada la salida será siempre igual. Es como calcular \\(\\sqrt{4}\\) el resultado será siempre \\(2\\). Esta consistencia es útil, en el funcionamiento de un desfibrilador automático, en un sistema de control aéreo, o en una app para reproducir música. Pero existen casos en donde necesitamos números aleatorios, por ejemplo en probabilidad y estadística, pero también otras áreas como cyber-seguridad. Para esos casos existen algoritmos que a partir de un valor inicial llamado semilla son capaces de producir una secuencia de valores, que si bien es determinista, a los fines prácticos tiene toda la pinta de aleatoria. Estos número se llaman pseudoaleatorios\nEn el caso de la función dado la semilla la generamos en la linea semilla = time.perf_counter_ns(), time.perf_counter_ns() es una función que mide el tiempo con precisión de nanosegundos. Cada vez que llamamos a dado ese valor será distinto. Luego en la última linea return semilla % 6 + 1 se calcula el resto de la división de la semilla por 6 y se le suma 1. Por ejemplo si la semilla fuese 12 el resto de dividir por 6 (12%6) sería 0, ya que \\(\\frac{12}{6}=2\\). Pero si la semilla fuese 10 entonces el resto de dividir por 6 (10%6) sería 4.\nSupongamos que ustedes, con justa razón, no me creen que dado realmente se comporta como un dado no trucado. Es decir, que todos los números tienen igual chance de salir. ¿Cómo podríamos hacer para evaluar esta posibilidad?\n\nUna posibilidad es consultar a los astros o los ángeles.\nOtra sería pensar mucho sobre el problema para luego quizá declararse agnóstico sobre la truquez no solo de nuestro dado digital si no de los dados en general y aún más sobre la posibilidad misma de acceder al conocimiento.\nUna tercera alternativa es recolectar datos y analizarlos, esta última es la opción preferida por quienes practican disciplinas científicas, en particular la estadística.\n\nUsando Python podemos simular la recolección de datos de la siguiente forma.\n\ndef experimento(N=100):\n    # llamamos a `dado` N veces y guardamos los resultados en `muestra`\n    muestra = [dado() for i in range(N)]   \n\n    # calculamos la proporción de veces que aparece cada valor en ` muestra` y lo imprimimos en pantalla\n    for i in range(1, 7):\n        print(f'{i}: {muestra.count(i)/N:.2g}')\n\nexperimento()\n\n1: 0.26\n2: 0.13\n3: 0.14\n4: 0.1\n5: 0.18\n6: 0.19\n\n\nLos números en la primer columna son los posibles resultados. Los de la segunda columna corresponden con la frecuencia con la que aparece cada número. La frecuencia es la cantidad de veces que aparece cada uno de los posibles resultados dividido por N. Siendo N el total de veces que arrojamos el dado.\nHay al menos dos aspectos que vale resaltar en este ejemplo:\n\nCada vez que se ejecuta la celda anterior, es decir cada vez que realizamos el experimento, se obtiene un resultado distinto. Esta es precisamente la razón de usar dados en juegos de azar, cada vez que los arrojamos obtenemos un número que no podemos predecir con absoluta certeza.\nSi arrojamos muchas veces un mismo dado la capacidad de predecir cada una de las tiradas no mejora. En ese sentido recolectar datos no nos ayuda. Pero recolectar datos si mejora la capacidad de predecir el listado de las frecuencias, de hecho la capacidad mejora de forma consistente al aumentar N. Para un valor de N=10000 verás que las frecuencias obtenidas son aproximadamente \\(0.17\\) y resulta ser que \\(0.17 \\approx \\frac{1}{6}\\) que es lo que esperado si cada número en el dado tuviera la misma posibilidad de aparecer.\n\nEstas dos observaciones no están restringidas a los dados y los juegos de azar. Si nos pesáramos todos los días obtendríamos distintos valores ya que el peso tiene relación con la cantidad de comida que ingerimos, el agua que tomamos, cuantos orinamos y defecamos, la precisión de la balanza, la ropa que usamos. Por todo ello una sola medida podría no ser representativa de nuestro peso. Es cierto que todas estas variaciones podrían ser demasiado pequeñas para nuestro propósito y podríamos considerarlas irrelevantes, pero eso es adelantarse a nuestra discusión. El punto importante en este momento es que los datos van acompañados de incertidumbre, gran parte de la estadística tiene que ver con métodos y prácticas para lidiar con esa incertidumbre."
  },
  {
    "objectID": "00_Probabilidad.html#probabilidades",
    "href": "00_Probabilidad.html#probabilidades",
    "title": "1  Probabilidad",
    "section": "1.2 Probabilidades",
    "text": "1.2 Probabilidades\nEs posible utilizar probabilidades para asignar números precisos a la incertidumbre de lo que observamos, medimos, modelamos, etc. Por ello Joseph K. Blitzstein y Jessica Hwang dicen La matemática es la lógica de la certeza mientras que la probabilidad es la lógica de la incerteza.\nEntender como pensar en presencia de incerteza es central en Estadística y Ciencia de Datos y prácticamente en cualquier disciplina científica. Esta incerteza proviene de diversas fuentes, incluyendo datos incompletos, errores de medición, límites de los diseños experimentales, dificultad de observar ciertos eventos, aproximaciones, etc.\nA continuación veremos una breve introducción a conceptos centrales en probabilidad a partir de lo cuales podremos comprender mejor los fundamentos del modelado Bayesiano. Para quienes tengan interés en profundizar en el tema recomiendo leer el libro Introduction to Probability de Joseph K. Blitzstein y Jessica Hwang.\nEl marco matemático para trabajar con las probabilidades se construye alrededor de los conjuntos matemáticos.\nEl espacio muestral \\(\\mathcal{X}\\) es el conjunto de todos los posibles resultados de un experimento. Un evento \\(A\\) es un subconjunto de \\(\\mathcal{X}\\). Decimos que \\(A\\) ha ocurrido si al realizar un experimento obtenemos como resultado \\(A\\). Si tuviéramos un típico dado de 6 caras tendríamos que:\n\\[\\mathcal{X} = \\{1, 2, 3, 4, 5, 6\\} \\tag {0.0}\\]\nPodemos definir al evento \\(A\\) como:\n\\[A = \\{2\\} \\tag {0.1}\\]\nSi queremos indicar la probabilidad del evento \\(A\\) escribimos \\(P(A=2)\\) o de forma abreviada \\(P(A)\\).\n\\(P(A)\\) puede tomar cualquier valor en el intervalo comprendido entre 0 y 1 (incluidos ambos extremos), en notación de intervalos esto se escribe como [0, 1]. Es importante notar que no es necesariamente cierto que \\(P(A) = \\frac{1}{6}\\).\nAl definir el evento \\(A\\) podemos usar más de un elemento de \\(\\mathcal{X}\\). Por ejemplo, números impares \\(A = \\{1, 3, 5\\}\\), o números mayores o iguales a 4 \\(A = \\{4,5,6\\}\\), o \\(A = \\{1,2,4,6\\}\\). Para cualquier problema concreto la definición de un evento como \\(A\\) dependerá directamente del problema.\nResumiendo, los eventos son subconjuntos de un espacio muestral definido adecuadamente y las probabilidades son números entre 0 y 1 asociados a la posibilidad que esos eventos ocurran. Si el evento es imposible entonces la probabilidad de ese evento será exactamente 0, si en cambio el evento sucede siempre entonces la probabilidad de ese evento será de 1. Todos los valores intermedios reflejan grados de incerteza. Desde este punto de vista es natural preguntarse cual es la probabilidad que la masa de Saturno sea \\(x\\) kg, o hablar sobre la probabilidad de lluvia durante el 25 de Mayo de 1810, o la probabilidad de que mañana amanezca.\nEsta interpretación del concepto de probabilidad como medida de incertidumbre se suele llamar interpretación Bayesiana o subjetiva. Existen otras interpretaciones, por ej según la interpretación frecuentista una probabilidad es la proporción de veces que un evento sucedería si pudiéramos repetir infinitas veces una observación bajo las mismas condiciones. Es importante destacar que estas interpretaciones son andamiajes conceptuales, interpretaciones filosóficas. El aparato matemático que describe las probabilidades es uno solo y no distingue entre estas u otras interpretaciones."
  },
  {
    "objectID": "00_Probabilidad.html#probabilidad-condicional",
    "href": "00_Probabilidad.html#probabilidad-condicional",
    "title": "1  Probabilidad",
    "section": "1.3 Probabilidad condicional",
    "text": "1.3 Probabilidad condicional\nUna probabilidad condicional es simplemente la probabilidad de un evento dado que conocemos que otro evento ha sucedido. Al preguntar cual es la probabilidad que llueva dado que está nublado estamos planteando una probabilidad condicional.\nDado dos eventos \\(A\\) y \\(B\\) siendo \\(P(B) > 0\\), la probabilidad de \\(A\\) dado \\(B\\) es definida como:\n\\[\nP(A \\mid B) \\triangleq \\frac{P(A, B)}{P(B)} \\tag{0.2}\n\\]\n\\(P(A, B)\\) es la probabilidad conjunta, es decir la probabilidad que suceda el evento \\(A\\) y que ocurra el evento \\(B\\), también se suele escribir como \\(P(A \\cap B)\\), el símbolo \\(\\cap\\) indica intersección de conjuntos.\n\\(P(A \\mid B)\\) es lo que se conoce como probabilidad condicional, y es la probabilidad de que ocurra el evento A condicionada por el conocimiento que B ha ocurrido. Por ejemplo la probabilidad que una vereda esté mojada puede ser diferente de la probabilidad que esa vereda esté mojada dado que está lloviendo.\nUna probabilidad condicional se puede visualizar como la reducción del espacio muestral. Para ver esto de forma más clara vamos a usar una figura adaptada del libro Introduction to Probability de Joseph K. Blitzstein y Jessica Hwang. En ella se puede ver como pasamos de tener los eventos \\(A\\) y \\(B\\) en el espacio muestral \\(\\mathcal{X}\\), en el primer cuadro, a tener \\(P(A \\mid B)\\) en el último cuadro donde el espacio muestral se redujo de \\(\\mathcal{X}\\) a \\(B\\).\n\nEl concepto de probabilidad condicional está en el corazón de la estadística y es central para pensar en como debemos actualizar el conocimiento que tenemos de un evento a la luz de nuevos datos. Veremos más sobre esto en los próximos capítulos. Por ahora dejamos este tema con la siguiente aclaración. Desde el punto de vista práctico, todas las probabilidades son condicionales (respecto de algún supuesto o modelo) aún cuando no lo expresemos explícitamente, no existen probabilidades sin contexto."
  },
  {
    "objectID": "00_Probabilidad.html#distribuciones-de-probabilidad",
    "href": "00_Probabilidad.html#distribuciones-de-probabilidad",
    "title": "1  Probabilidad",
    "section": "1.4 Distribuciones de probabilidad",
    "text": "1.4 Distribuciones de probabilidad\nA nosotros en general no nos interesará calcular la probabilidad de eventos concretos sino que nos interesará calcular distribuciones de probabilidad. Es decir, en vez de calcular la probabilidad de obtener el número 5 al arrojar un dado, nos interesará averiguar el listado de todas las posibilidades del dado (1 al 6). Una vez obtenido este listado podremos hacer preguntas como ¿Cuál es la probabilidad de obtener el número 5?, ¿Cuánto más probable es obtener número pares que impares? u otras preguntas relacionadas. El nombre formal de este listado es distribución de probabilidad.\nEn el ejemplo del dado obtuvimos una distribución de probabilidad empírica, es decir una distribución calculada a partir de datos. Pero también existen distribuciones teóricas, las cuales son centrales en estadística entre otras razones por que permiten construir modelos probabilistas.\nLas distribuciones de probabilidad teóricas tienen formulas matemáticas precisas, de forma similar a como las circunferencias tienen una definición matemática precisa.\n\nUna circunferencia es el lugar geométrico de los puntos de un plano que equidistan a otro punto llamado centro.\n\nDado el parámetro radio una circunferencia queda perfectamente definida. Si necesitáramos ubicar la circunferencia respecto de otros objetos en el plano, necesitaríamos además las coordenadas del centro, pero omitamos ese detalle por el momento.\nVeamos el siguiente ejemplo:\n\ndef dibuja_circ(radio):\n    _, ax = plt.subplots(figsize=(2, 2))\n    x = np.linspace(0, 2*np.pi, 100)\n    ax.plot(radio*np.cos(x), radio*np.sin(x))\n    ax.set_xlim(-11, 11)\n    ax.set_ylim(-11, 11)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n\ninteract(dibuja_circ,\n         radio=ipyw.FloatSlider(min=0.5, max=10, step=0.5, value=2.));\n\n\n\n\nPodríamos decir que no existe una sola circunferencia, sino una familia de circunferencias donde cada miembro se diferencia del resto solo por el valor del parámetro radio, ya que una vez definido este parámetro la circunferencia queda definida.\nDe forma similar las distribuciones de probabilidad vienen en familias cuyos miembros quedan definidos por uno o más parámetros. Es común que los nombres de los parámetros de las distribuciones de probabilidad sean letras del alfabeto griego, aunque esto no es siempre así.\nEn el siguiente ejemplo tenemos una distribución de probabilidad que podríamos usar para representar un dado y que es controlada por dos parámetros \\(\\alpha\\) y \\(\\beta\\):\n\ndef dist_dado(α, β):\n    n = 5\n    x = np.arange(0, 6)\n    dist_pmf = special.binom(n, x) * (special.beta(x+α, n-x+β) / special.beta(α, β))\n    plt.vlines(x, 0, dist_pmf, colors='C0', lw=4)\n    plt.ylim(0, 1)\n    plt.xticks(x, x+1)\n\n\ninteract(dist_dado,\n         α=ipyw.FloatSlider(min=0.5, max=10, step=0.5, value=1),\n         β=ipyw.FloatSlider(min=0.5, max=10, step=0.5, value=1));\n\n\n\n\nEsta distribución (o familia de distribuciones) se llama beta-binomial, si cambiamos los parámetros \\(\\alpha\\) y \\(\\beta\\) la “forma particular” de la distribución cambiará, podemos hacer que sea plana o concentrada más hacia el medio o hacia uno u otro extremo, etc. Así como el radio de la circunferencia debe ser positivo, los parámetros \\(\\alpha\\) y \\(\\beta\\) también están restringidos a ser positivos."
  },
  {
    "objectID": "00_Probabilidad.html#variables-aleatorias-discretas-y-distribuciones-de-probabilidad",
    "href": "00_Probabilidad.html#variables-aleatorias-discretas-y-distribuciones-de-probabilidad",
    "title": "1  Probabilidad",
    "section": "1.5 Variables aleatorias discretas y distribuciones de probabilidad",
    "text": "1.5 Variables aleatorias discretas y distribuciones de probabilidad\nUna variable aleatoria es una función que asocia números reales \\(\\mathbb{R}\\) con un espacio muestral. Continuando con el ejemplo del dado si los eventos de interés fuesen los números del dado entonces el mapeo es simple, ya que asociamos ⚀ con el número 1, ⚁ con el 2, etc. Si tuviéramos dos dados podríamos definir una variable aleatoria \\(S\\) como la suma de ambos dados. En este caso la variable aleatoria tomaría los valores \\(\\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\}\\) y si los dados no están trucados la distribución de probabilidad de la variable sería\n\nOtra variable aleatoria podría ser \\(C\\) cuyo espacio muestral es \\(\\{rojo, verde, azul\\}\\). Si los eventos de interés fuesen rojo, verde, azul, entonces podríamos codificarlos de la siguiente forma:\nC(rojo) = 0, C(verde)=1, C(azul)=2\nEsta codificación es útil ya que en general es más fácil operar con números que con cadenas (strings), ya sea que las operaciones las hagamos manualmente o con una computadora.\nUna variable es aleatoria en el sentido de que en cada experimento es posible obtener un evento distinto sin que la sucesión de eventos siga un patrón determinista. Por ejemplo si preguntamos cual es el valor de \\(C\\) tres veces seguida podríamos obtener, rojo, rojo, azul o quizá azul, verde, azul, etc.\nCuando se habla de variables aleatorias, es común que surjan algunos malos entendidos:\n\nLa variable NO puede tomar cualquier valor imaginable, en el ejemplo de los colores solo son posibles 3 valores. En el ejemplo del dado solo 6 valores son posibles.\nAleatorio NO implica que todos los eventos tengan igual probabilidad.\n\nbien podría darse el siguiente ejemplo:\n\\[P(C=rojo) = \\frac{1}{2}, P(C=verde) = \\frac{1}{4}, P(C=azul) = \\frac{1}{4}\\]\nLa equiprobabilidad de los eventos es solo un caso especial.\n\nUna variable aleatoria discreta es una variable que puede tomar valores discretos, los cuales forman un conjunto finito (o infinito numerable). En nuestro ejemplo \\(C\\) es discreta ya que solo puede tomar 3 valores, sin posibilidad de valores intermedios entre ellos, no es posible obtener el valor verde-rojizo! \\(S\\) también es discreta y como ya dijimos solo es posible obtener los enteros en el intervalo [2-12].\nSi en vez de “rótulos” hubiéramos usado el espectro continuo de longitudes onda visibles otro sería el caso, ya que podríamos haber definido a \\(C=\\{400 \\text{ nm} ... 750\\text{ nm}\\}\\) y en este caso no hay dudas que sería posible obtener un valor a mitad de camino entre rojo (\\(\\approx 700 \\text{ nm}\\)) y verde (\\(\\approx 530 \\text{ nm}\\)), de hecho podemos encontrar infinitos valores entre ellos. Este sería el ejemplo de una variable aleatoria continua.\nUna variable aleatoria tiene una lista asociada con la probabilidad de cada evento. El nombre formal de esta lista es distribución de probabilidad, en el caso particular de variables aleatorias discretas se le suele llamar también función de masa de probabilidad (o pmf por su sigla en inglés). Es importante destacar que la \\(pmf\\) es una función que devuelve probabilidades, por lo tanto siempre obtendremos valores comprendidos entre [0, 1] y cuya suma total (sobre todos los eventos) dará 1.\nEn principio nada impide que uno defina su propia distribución de probabilidad. Pero a lo largo de los últimos 3 siglos se han identificado y estudiado muchas distribuciones de probabilidad que dado su utilidad se les ha asignado “nombre propio”, por lo que conviene saber sobre su existencia. El siguiente listado no es exhaustivo ni tiene como propósito que memoricen las distribuciones y sus propiedades, solo que ganen cierta familiaridad con las mismas. Si en el futuro necesitan utilizar alguna \\(pmf\\) pueden volver a esta notebook o pueden revisar Wikipedia donde encontrarán información muy completa.\nEn las siguientes gráficas las alturas de los puntos azules indican la probabilidad de cada valor de \\(x\\). Se indican, además, la media (\\(\\mu\\)) y desviación estándar (\\(\\sigma\\)) de las distribuciones, es importante destacar que estos valores NO son calculados a partir de datos, de hecho no hay datos solo objetos matemáticos. Los valores de \\(\\mu\\) y \\(\\sigma\\) son propiedades matemáticas de las distribuciones, de la misma forma que el área de un círculo es una propiedad que queda definida una vez que fijamos el parámetro radio.\n\n1.5.1 Distribución uniforme discreta\nEs una distribución que asigna igual probabilidad a un conjunto finitos de valores, su \\(pmf\\) es:\n\\[p(k \\mid a, b)={\\frac {1}{b - a + 1}} = \\frac{1}{n}\\tag {0.3}\\]\nPara valores de \\(k\\) en el intervalo [a, b], fuera de este intervalo \\(p(k) = 0\\), donde \\(n=b-a+1\\) es la cantidad total de valores que puede tomar \\(k\\).\nPodemos usar esta distribución para modelar, por ejemplo un dado no cargado.\n\ndist = pz.DiscreteUniform(lower=1, upper=6)\n# PreliZ usa \"plot_pdf\" tanto para distribuciones discretas como continuas\nax = dist.plot_pdf(moments=\"md\", support=(0, 7))\nax.set_xlabel('x')\nax.set_ylabel('p(x)', rotation=0, labelpad=25);\n\n\n\n\nEn la figura anterior la altura de cada punto indica la probabilidad de cada evento, usamos puntos y lineas punteadas para remarcar que la distribución es discreta. En este ejemplo en concreto la distribución uniforme está definida en el intervalo [1, 6]. Por lo tanto todos los valores menores a 1 y mayores a 6 tienen probabilidad 0. Al ser una distribución uniforme todos los puntos tienen la misma altura y esa altura es \\(\\frac{1}{6}\\).\nLos parámetros de la distribución discreta uniforme son dos: * El límite inferior representado con la letra “a” en la expresión 0.3 * El límite superior representado con la letra “b” en la expresión 0.3\nSi cambiamos los parámetros la “forma particular” de la distribución cambiará (prueben por ejemplo reemplazar upper=6 en el bloque de código anterior por upper=4). Es por ello que se suele hablar de familia de distribuciones, cada miembro de esa familia es una distribución con una combinación particular y válida de parámetros. Por ejemplo la familia de distribuciones discretas uniforme es aquella indicada en la expresión 0.3, siempre y cuando: * \\(a < b\\) * \\(a \\in \\mathbb {Z}\\) * \\(b \\in \\mathbb {Z}\\)\ndonde \\(\\mathbb {Z}\\) es el conjunto de los números enteros.\nEs común vincular los parámetros con cantidades que tienen sentido físico por ejemplo en un dado de 6 caras tiene sentido que \\(a=1\\) y \\(b=6\\). A veces desconocemos los valores de los parámetros y es nuestro trabajo utilizar datos y métodos estadísticos para encontrar esos valores.\n\n\n1.5.2 Distribución binomial\nEs la distribución de probabilidad discreta que cuenta el número de éxitos en una secuencia de \\(n\\) ensayos de Bernoulli (experimentos si/no) independientes entre sí, con una probabilidad fija \\(p\\) de ocurrencia del éxito entre los ensayos. Cuando \\(n=1\\) esta distribución se reduce a la distribución de Bernoulli.\n\\[p(x \\mid n,p) = \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\tag {0.4}\\]\nEl término \\(p^x(1-p)^{n-x}\\) indica la probabilidad de obtener \\(x\\) éxitos en \\(n\\) intentos. Este término solo tiene en cuenta el número total de éxitos obtenidos pero no la secuencia en la que aparecieron. El primer término conocido como coeficiente binomial calcula todas las posibles combinaciones de \\(n\\) en \\(x\\), es decir el número de subconjuntos de \\(x\\) elementos escogidos de un conjunto con \\(n\\) elementos.\n\ndist = pz.Binomial(n=4, p=0.5)\nax = dist.plot_pdf(moments=\"md\")\nax.set_xlabel('x')\nax.set_ylabel('p(x)', rotation=0, labelpad=25);\n\n\n\n\n\n\n1.5.3 Distribución de Poisson\nEs una distribución de probabilidad discreta que expresa la probabilidad que \\(x\\) eventos sucedan en un intervalo fijo de tiempo (o espacio o volumen) cuando estos eventos suceden con una tasa promedio \\(\\mu\\) y de forma independiente entre si. Se la utiliza para modelar eventos con probabilidades pequeñas (sucesos raros) como accidentes de tráfico o decaimiento radiactivo.\n\\[\np(x \\mid \\mu) = \\frac{\\mu^{x} e^{-\\mu}}{x!} \\tag {0.5}\n\\]\ndonde x es el número de eventos \\((x =0,1,2,\\ldots)\\)\nTanto la media como la varianza de esta distribución están dadas por \\(\\mu\\).\nA medida que \\(\\mu\\) aumenta la distribución de Poisson se aproxima a una distribución Gaussiana (aunque sigue siendo discreta). La distribución de Poisson tiene estrecha relación con otra distribución de probabilidad, la binomial. Una distribución binomial puede ser aproximada con una distribución de Poisson, cuando \\(n >> p\\), es decir, cuando la cantidad de “éxitos” (\\(p\\)) es baja respecto de la cantidad de “intentos” (p) entonces \\(\\text{Poisson}(np) \\approx \\text{Binon}(n, p)\\). Por esta razón la distribución de Poisson también se conoce como “ley de los pequeños números” o “ley de los eventos raros”. Ojo que esto no implica que \\(\\mu\\) deba ser pequeña, quien es pequeño/raro es \\(p\\) respecto de \\(n\\).\n\ndist = pz.Poisson(mu=2.3)  # número de veces que se espera que ocurra un evento.\n# PreliZ conoce cuales son los límites de una distribución y los usa al grafica\n# Para distribuciones que no tienen límites PreliZ usa los cuantiles 0.001 y 0.999\n# En el caso de la distribución de Poisson, el gráfico va de 0 a el cuantil 0.999\nax = dist.plot_pdf(moments=\"md\")\nax.set_xlabel('x')\nax.set_ylabel('p(x)', rotation=0, labelpad=25);"
  },
  {
    "objectID": "00_Probabilidad.html#variables-aleatorias-y-distribuciones-de-probabilidad-continuas",
    "href": "00_Probabilidad.html#variables-aleatorias-y-distribuciones-de-probabilidad-continuas",
    "title": "1  Probabilidad",
    "section": "1.6 Variables aleatorias y distribuciones de probabilidad continuas",
    "text": "1.6 Variables aleatorias y distribuciones de probabilidad continuas\nHasta ahora hemos visto variables aleatorias discretas y distribuciones de masa de probabilidad. Existe otro tipo de variable aleatoria que son muy usadas y son las llamadas variables aleatorias continuas, ya que toman valores en \\(\\mathbb{R}\\).\nLa diferencia más importante entre variables aleatoria discretas y continuas es que para las continuas \\(P(X=x) = 0\\), es decir, la probabilidad de cualquier valor es exactamente 0.\nEn las gráficas anteriores, para variables discretas, es la altura de los puntos lo que define la probabilidad de cada evento. Si sumamos todas las alturas siempre obtenemos 1. En una distribución continua no tenemos una cantidad finita de puntos que sumar, en cambio tenemos una cantidad infinita de puntos que definen una curva continua, la altura de esa curva es la densidad de probabilidad. Si queremos averiguar cuanto más probable es el valor \\(x_1\\) respecto de \\(x_2\\) basta calcular:\n\\[\\frac{pdf(x_1)}{pdf(x_2)} \\tag {0.6}\\]\nDonde \\(pdf\\) es la función de densidad de probabilidad (por su sigla en inglés). Y es análoga a la \\(pmf\\) que vimos para variables discretas. Una diferencia importante es que la \\(pdf(x)\\) puede devolver mayores a 1. Para obtener una probabilidad a partir de una \\(pdf\\) debemos integrar en un intervalo dado, ya que es el área bajo la curva y no la altura lo que nos da la probabilidad, es decir es esta integral la que debe dar entre 0 y 1.\n\\[P(a \\lt X \\lt b) =  \\int_a^b pdf(x) dx\\]\nEn muchos textos es común usar \\(p\\) para referirse a la probabilidad de un evento en particular o a la \\(pmf\\) o a la \\(pdf\\), esperando que la diferencia se entienda por contexto.\nA continuación veremos varias distribuciones continuas.\n\n1.6.1 Distribución uniforme\nAún siendo simple, la distribución uniforme es muy usada en estadística, por ejemplo para representar nuestra ignorancia sobre el valor que pueda tomar un parámetro.\n\\[\np(x \\mid a,b)=\\begin{cases} \\frac{1}{b-a} & para\\ a \\le x \\le b \\\\ 0 &  \\text{para el resto} \\end{cases} \\tag {0.7}\n\\]\n\ndist = pz.Uniform(0, 1)  \nx_rvs = dist.rvs(500)  # muestrear 500 valores de la distribución\nax = dist.plot_pdf(moments=\"md\")\nax.hist(x_rvs, density=True)\nax.set_xlabel('x')\nax.set_ylabel('pdf(x)', rotation=0, labelpad=25);\n\n\n\n\nEn la figura anterior la curva azul representa la \\(pdf\\). La \\(pdf\\) es un objeto matemático que da la descripción exacta de la distribución, no es algo que exista en la realidad si no una construcción matemática que es útil para aproximar o modelar algún aspecto de la realidad. La \\(pdf\\) es como las esferas, las esferas no existen pero pueden ser útiles para describir objetos tales como pelotas, planetas, átomos, aún cuando ni las pelotas, planetas o átomos sean esferas.\nEl histograma en turquesa representa una muestra tomadas a partir de la \\(pdf\\) representada en azul. A diferencia de la curva azul, que es un objeto (matemático) concreto. Una muestra es aleatoria. Cada vez que ejecutemos la celda anterior la curva azul será la misma pero el histograma cambiará.\nUna aclaración antes de continuar. Los histogramas no son lo mismo que los gráficos de barras. Los histogramas son una forma de representación visual de datos que usa barras a fin de aproximar una distribución continua. Si bien la cantidad de barras es discreta, la distribución que intenta aproximar es continua, es por ello que las barras se dibujan de forma contigua, mientras que en los gráficos de barras (que representan distribuciones discretas) las barras se dibujan espaciadas.\nLuego de estas aclaraciones continuemos con otras distribuciones de probabilidad continuas.\n\n\n1.6.2 Distribución Gaussiana (o normal)\nEs quizá la distribución más conocida. Por un lado por que muchos fenómenos pueden ser descriptos (aproximadamente) usando esta distribución. Por otro lado por que posee ciertas propiedades matemáticas que facilitan trabajar con ella de forma analítica. Es por ello que muchos de los resultados de la estadística se basan en asumir una distribución Gaussiana.\nLa distribución Gaussiana queda definida por dos parámetros, la media \\(\\mu\\) y la desviación estándar \\(\\sigma\\). Una distribución Gaussiana con \\(\\mu = 0\\) y \\(\\sigma = 1\\) es conocida como la distribución Gaussiana estándar.\n\\[\np(x \\mid \\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{ 2 \\pi}} e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2}} \\tag {0.8}\n\\]\n\ndist = pz.Normal(mu=0, sigma=1)  \nx_rvs = dist.rvs(500)  # muestrear 500 valores de la distribución\nax = dist.plot_pdf(moments=\"md\")\nax.hist(x_rvs, density=True)\nax.set_xlabel('x')\nax.set_ylabel('pdf(x)', rotation=0, labelpad=25);\n\n\n\n\n\n\n1.6.3 Distribución t de Student\nHistóricamente esta distribución surgió para estimar la media de una población normalmente distribuida cuando el tamaño de la muestra es pequeño. En estadística Bayesiana su uso más frecuente es el de generar modelos robustos a datos aberrantes.\n\\[p(x \\mid \\nu,\\mu,\\sigma) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\pi\\nu}\\sigma} \\left(1+\\frac{1}{\\nu}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)^{-\\frac{\\nu+1}{2}} \\tag {0.9}\n\\]\ndonde \\(\\Gamma\\) es la función gamma y donde \\(\\nu\\) es un parámetro llamado grados de libertad en la mayoría de los textos aunque también se le dice grado de normalidad, ya que a medida que \\(\\nu\\) aumenta la distribución se aproxima a una Gaussiana. En el caso extremo de \\(\\lim_{\\nu\\to\\infty}\\) la distribución es exactamente igual a una Gaussiana.\nEn el otro extremo, cuando \\(\\nu=1\\), (aunque en realidad \\(\\nu\\) puede tomar valores por debajo de 1) estamos frente a una distribución de Cauchy. Es similar a una Gaussiana pero las colas decrecen muy lentamente, eso provoca que en teoría esta distribución no poseen una media o varianza definidas. Es decir, es posible calcular a partir de un conjunto de datos una media, pero si los datos provienen de una distribución de Cauchy, la dispersión alrededor de la media será alta y esta dispersión no disminuirá a medida que aumente el tamaño de la muestra. La razón de este comportamiento extraño es que en distribuciones como la Cauchy están dominadas por lo que sucede en las colas de la distribución, contrario a lo que sucede por ejemplo con la distribución Gaussiana.\nPara esta distribución \\(\\sigma\\) no es la desviación estándar, que como ya se dijo podría estar indefinida, \\(\\sigma\\) es la escala. A medida que \\(\\nu\\) aumenta la escala converge a la desviación estándar de una distribución Gaussiana.\n\ndist = pz.StudentT(nu=4, mu=0, sigma=2)  \nx_rvs = dist.rvs(500)  # muestrear 500 valores de la distribución\nax = dist.plot_pdf(moments=\"md\")\nax.hist(x_rvs, density=True)\nax.set_xlabel('x')\nax.set_ylabel('pdf(x)', rotation=0, labelpad=25);\n\n\n\n\n\n\n1.6.4 Distribución exponencial\nLa distribución exponencial se define solo para \\(x > 0\\). Esta distribución se suele usar para describir el tiempo que transcurre entre dos eventos que ocurren de forma continua e independiente a una taza fija. El número de tales eventos para un tiempo fijo lo da la distribución de Poisson.\n\\[\np(x \\mid \\lambda) = \\lambda e^{-\\lambda x} \\tag {0.10}\n\\]\nLa media y la desviación estándar de esta distribución están dadas por \\(\\frac{1}{\\lambda}\\)\n\ndist = pz.Exponential(3)  \nx_rvs = dist.rvs(500)  # muestrear 500 valores de la distribución\nax = dist.plot_pdf(moments=\"md\")\nax.hist(x_rvs, density=True)\nax.set_xlabel('x')\nax.set_ylabel('pdf(x)', rotation=0, labelpad=25);\n\n\n\n\n\n\n1.6.5 Distribución de Laplace\nTambién llamada distribución doble exponencial, ya que puede pensarse como una distribución exponencial “más su imagen especular”. Esta distribución surge de medir la diferencia entre dos variables exponenciales (idénticamente distribuidas).\n\\[p(x \\mid \\mu, b) = \\frac{1}{2b} \\exp \\left\\{ - \\frac{|x - \\mu|}{b} \\right\\} \\tag {0.11}\\]\n\ndist = pz.Laplace(0, 0.7)  \nx_rvs = dist.rvs(500)  # muestrear 500 valores de la distribución\nax = dist.plot_pdf(moments=\"md\")\nax.hist(x_rvs, density=True)\nax.set_xlabel('x')\nax.set_ylabel('pdf(x)', rotation=0, labelpad=25);\n\n\n\n\n\n\n1.6.6 Distribución Beta\nEs una distribución definida en el intervalo [0, 1]. Se usa para modelar el comportamiento de variables aleatorias limitadas a un intervalo finito. Es útil para modelar proporciones o porcentajes.\n\\[\np(x \\mid \\alpha, \\beta)= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, x^{\\alpha-1}(1-x)^{\\beta-1} \\tag {0.12}\n\\]\nEl primer término es simplemente una constante de normalización que asegura que la integral de la pdf de 1. \\(\\Gamma\\) es la función gamma. Cuando \\(\\alpha=1\\) y \\(\\beta=1\\) la distribución beta se reduce a la distribución uniforme.\nSi queremos expresar la distribución beta en función de la media y la dispersión alrededor de la media podemos hacerlo de la siguiente forma.\n\\[\\alpha = \\mu \\kappa\\] \\[\\beta = (1 − \\mu) \\kappa\\]\nSiendo \\(\\mu\\) la media y \\(\\kappa\\) una parámetro llamado concentración a media que \\(\\kappa\\) aumenta la dispersión disminuye. Nótese, además que \\(\\kappa = \\alpha + \\beta\\).\n\ndist = pz.Beta(5, 2)  \nx_rvs = dist.rvs(500)  # muestrear 500 valores de la distribución\nax = dist.plot_pdf(moments=\"md\")\nax.hist(x_rvs, density=True)\nax.set_xlabel('x')\nax.set_ylabel('pdf(x)', rotation=0, labelpad=25);\n\n\n\n\n\n\n1.6.7 Distribución Gamma\nExisten varias parametrizaciones para la distribución Gamma. Nosotros usaremos la siguiente:\n\\[\np(x \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-\\beta x}}{\\Gamma(\\alpha)} \\tag {0.14}\n\\]\nLa distribución gamma se reduce a la exponencial cuando \\(\\alpha=1\\).\nOtra parametrización, disponible por ej en paquetes como PyMC y Preliz, es en términos de la media y la desviación estándar.\n\ndist = pz.Gamma(alpha=3, beta=0.5)\nx_rvs = dist.rvs(500)  # muestrear 500 valores de la distribución\nax = dist.plot_pdf(moments=\"md\")\nax.hist(x_rvs, density=True)\nax.set_xlabel('x')\nax.set_ylabel('pdf(x)', rotation=0, labelpad=25);"
  },
  {
    "objectID": "00_Probabilidad.html#valor-esperado",
    "href": "00_Probabilidad.html#valor-esperado",
    "title": "1  Probabilidad",
    "section": "1.7 Valor esperado",
    "text": "1.7 Valor esperado\nEl valor esperado (también conocido como esperanza o media) es un número que resume el centro de masa de una distribución. Por ejemplo, si \\(X\\) es una variable aleatoria discreta, podemos calcular su valor esperado como:\n\\[\n    \\mathbb{E}[X] = \\sum_{i=1}^n x_i P(X=x_i)\n\\]\nEn estadística usualmente también queremos medir la dispersión de una distribución, por ejemplo, para representar la incertidumbre en torno a una estimación puntual como la media. Podemos hacer esto con la varianza, que también es un valor esperado:\n\\[\n\\mathbb{V}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\]\nLa varianza aparece naturalmente en muchos cálculos estadísticos. Sin embargo, para reportar los resultados de un análisis suele ser más útil la desviación estándar, que es la raíz cuadrada de la varianza. La principal ventaja es que esta última está en la mismas escala/unidades que la variable aleatoria.\nEl \\(n\\)-ésimo momento de una variable aleatoria \\(X\\) es \\(\\mathbb{E}[X^n]\\), por lo que el valor esperado (media) y la varianza también se conocen como el primer y segundo momento de una distribución.\nUna vez fijados los parámetros de una distribución de probabilidad. Es posible calcularle sus momentos, el primer momento es la media y el segundo la varianza. Es importante notar que estos valores son propiedades de la distribución y no propiedades de una muestra. Por ejemplo la media y varianza de una distribución Beta(4, 6) es \\(0.4\\) y \\(\\approx 0.22\\), respectivamente\n\npz.Beta(4, 6).plot_pdf(moments=\"mv\");\n\n\n\n\nExisten otros momentos, el tercer momento se conoce como sesgo y habla de la asimetría de una distribución\n\npz.Beta(4, 4).plot_pdf(moments=\"s\");\npz.Beta(4, 10).plot_pdf(moments=\"s\");\npz.Beta(10, 4).plot_pdf(moments=\"s\");\n\n\n\n\nEl cuarto momento, conocido como curtosis, nos habla del comportamiento de las colas o valores extremos. Suele calcularse de forma tal que de 0 para una Normal. Además las discusiones suelen centrarse en torno a distribuciones con curtosis positiva (como la distribución de Laplace o Student T con \\(\\nu > 4\\)) por lo que es común hablar de exceso de curtosis. Mientras más grande la curtosis de una distribución más pesadas sus colas, es decir es más factible observar valores alejados de la media.\n\npz.Normal(0, 1).plot_pdf(moments=\"k\");\npz.StudentT(4.1, 0, 1).plot_pdf(moments=\"k\");\npz.Uniform(-3, 3).plot_pdf(moments=\"k\");"
  },
  {
    "objectID": "00_Probabilidad.html#distribución-acumulada",
    "href": "00_Probabilidad.html#distribución-acumulada",
    "title": "1  Probabilidad",
    "section": "1.8 Distribución acumulada",
    "text": "1.8 Distribución acumulada\nLa pdf (o la pmf) son formas comunes de representar y trabajar con variables aleatorias, pero no son las únicas formas posibles. Existen otras representaciones equivalentes. Por ejemplo la función de distribución acumulada (cdf en inglés). Al integrar una pdf se obtiene la correspondiente cdf, y al derivar la cdf se obtiene la pdf.\nLa integral de la pdf es llamada función de distribución acumulada (cdf):\n\\[\ncdf(x) = \\int_{-\\infty}^{x} pdf(x) d(x) \\tag {0.17}\n\\]\nEn algunas situaciones se prefiere hablar de la función de supervivencia:\n\\[\nS(x) = 1 - cdf  \\tag {0.18}\n\\]\nA continuación un ejemplo de la pdf y cdf para 4 distribuciones de la familia Gaussiana.\n\n_, ax = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\nx_valores = np.linspace(-4, 4, 500)\nvalores = [(0., 1.), (0., 2.), (2., .5)]\nfor val in valores:\n    pz.Normal(*val).plot_pdf(ax=ax[0])\n    pz.Normal(*val).plot_cdf(ax=ax[1])\n    ax[1].get_legend().remove()\n\n\n\n\nLa siguiente figura tomada del libro Think Stats resume las relaciones entre la cdf, pdf y pmf."
  },
  {
    "objectID": "00_Probabilidad.html#relación-entre-probabilidad-conjunta-condicional-y-marginal",
    "href": "00_Probabilidad.html#relación-entre-probabilidad-conjunta-condicional-y-marginal",
    "title": "1  Probabilidad",
    "section": "1.9 Relación entre probabilidad conjunta, condicional y marginal",
    "text": "1.9 Relación entre probabilidad conjunta, condicional y marginal\nAl definir probabilidad condicional usamos la expresión 0.2. Ahora que ya estamos familiarizados con las distribuciones de probabilidad podemos representar gráficamente los tres términos en la expresión 0.2, tal como se muestra en la siguiente figura.\n\n\nProbabilidad conjunta \\(p(A, B)\\)\nProbabilidad marginal \\(p(A)\\) o \\(p(B)\\)\nProbabilidad condicional \\(p(A \\mid B)\\)\n\nPodemos re-escribir la expresión 0.2 de la siguiente manera:\n\\[\np(A, B) = p(A \\mid B) {p(B)}  \\tag {0.15}\n\\]\nEs decir si tomo una probabilidad condicional y la evalúo para todos los valores de la cantidad condicionante (\\(B\\) en este caso), obtengo la distribución conjunta. Esto se puede ver gráficamente si pensamos que \\(p(A \\mid B)\\) es una rebanada de p(A, B); rebanada que tomamos a la altura de \\(B\\). Si tomamos todas las rebanadas entonces obtendremos \\(p(A, B)\\).\nPara obtener las probabilidades marginales, que se encuentran en los margenes 😉, podemos calcular algo similar:\n\\[\np(A) = \\sum_B p(A, B) = \\sum_B p(A \\mid B) {p(B)} \\tag {0.16}\n\\]\nCambiando la sumatoria por una integral para distribuciones continuas."
  },
  {
    "objectID": "00_Probabilidad.html#límites",
    "href": "00_Probabilidad.html#límites",
    "title": "1  Probabilidad",
    "section": "1.10 Límites",
    "text": "1.10 Límites\nLos dos teoremas más conocidos y usados en probabilidad son la ley de los grandes números y el teorema del límite central. Ambos nos dicen que le sucede a la media muestral a medida que el tamaño de la muestra aumenta.\n\n1.10.1 La ley de los grandes números\nEl valor promedio calculado para una muestra converge al valor esperado (media) de dicha distribución. Esto no es cierto para algunas distribuciones como la distribución de Cauchy (la cual no tiene media ni varianza finita).\nLa ley de los grandes números se suele malinterpretar y dar lugar a la paradoja del apostador. Un ejemplo de esta paradoja es creer que conviene apostar en la lotería/quiniela a un número atrasado, es decir un número que hace tiempo que no sale. El razonamiento, erróneo, es que como todos los números tienen la misma probabilidad a largo plazo si un número viene atrasado entonces hay alguna especie de fuerza que aumenta la probabilidad de ese número en los próximo sorteos para así re-establecer la equiprobabilidad de los números.\n\ntamaño_muestra = 200\nmuestras = range(1, tamaño_muestra)\ndist = pz.Uniform(0, 1)\nmedia_verdadera = dist.rv_frozen.stats('m')\n\nfor _ in range(3):\n    muestra = dist.rvs(tamaño_muestra)\n    media_estimada = [muestra[:i].mean() for i in muestras]\n    plt.plot(muestras, media_estimada, lw=1.5)\n\nplt.hlines(media_verdadera, 0, tamaño_muestra, linestyle='--', color='k')\nplt.ylabel(\"media\", fontsize=14)\nplt.xlabel(\"# de muestras\", fontsize=14);\n\n\n\n\n\n\n1.10.2 El teorema central del límite\nEl teorema central del límite (también llamado teorema del límite central) establece que si tomamos \\(n\\) valores (de forma independiente) de una distribución arbitraria la media \\(\\bar X\\) de esos valores se distribuirá aproximadamente como una Gaussiana a medida que \\({n \\rightarrow \\infty}\\):\n\\[\n\\bar X_n \\dot\\sim \\mathcal{N} \\left(\\mu,  \\frac{\\sigma^2}{n}\\right) \\tag {0.19}\n\\]\nDonde \\(\\mu\\) y \\(\\sigma^2\\) son la media y varianza poblacionales.\nPara que el teorema del límite central se cumpla se deben cumplir los siguientes supuestos:\n\nLas variables se muestrean de forma independiente\nLas variables provienen de la misma distribución\nLa media y la desviación estándar de la distribución tiene que ser finitas\n\nLos criterios 1 y 2 se pueden relajar bastante y aún así obtendremos aproximadamente una Gaussiana, pero del criterio 3 no hay forma de escapar. Para distribuciones como la distribución de Cauchy, que no posen media ni varianza definida este teorema no se aplica. El promedio de \\(N\\) valores provenientes de una distribución Cauchy no siguen una Gaussiana sino una distribución de Cauchy.\nEl teorema del límite central explica la prevalencia de la distribución Gaussiana en la naturaleza. Muchos de los fenómenos que estudiamos se pueden explicar como fluctuaciones alrededor de una media, o ser el resultado de la suma de muchos factores diferentes. Además, las Gaussianas son muy comunes en probabilidad, estadística y machine learning ya que que esta familia de distribuciones son más simples de manipular matemáticamente que muchas otras distribuciones.\nA continuación vemos una simulación que nos muestra el teorema del límite central en acción.\n\niters = 2000\ndist = pz.Exponential(1)\nmedia, var = dist.rv_frozen.stats('mv')\n\n_, ax = plt.subplots(2, 3)\n\nfor i, n in enumerate([1, 5, 100]):\n    sample = np.mean(dist.rvs((n, iters)), axis=0)\n\n    sd = (var/n)**0.5 \n    x = np.linspace(media - 4 * sd, media + 4 * sd, 200)\n    ax[0, i].plot(x, pz.Normal(media, sd).pdf(x))\n    ax[0, i].hist(sample, density=True, bins=20)\n    ax[0, i].set_title('n = {}'.format(n))\n    osm, osr = stats.probplot(sample, dist=pz.Normal(media, sd), fit=False)\n    ax[1, i].plot(osm, osm)\n    ax[1, i].plot(osm, osr, 'o')\nax[1, 0].set_ylabel('observados')\nax[1, 1].set_xlabel('esperados');"
  },
  {
    "objectID": "00_Probabilidad.html#ejercicios",
    "href": "00_Probabilidad.html#ejercicios",
    "title": "1  Probabilidad",
    "section": "1.11 Ejercicios",
    "text": "1.11 Ejercicios\n\nDe las siguientes expresiones cual(es) se corresponde(n) con el enunciado “la probabilidad de lluvia dado que es 25 de Mayo de 1810”?\n\np(lluvia)\np(lluvia | mayo)\np(25 de Mayo de 1810 | lluvia)\np(lluvia | 25 de Mayo de 1810 )\np(lluvia, 25 de Mayo de 1810) / p(25 de Mayo de 1810)\n\nEnuncie con palabras cada una de las expresiones del punto anterior.\nSegún la definición de probabilidad condicional\n\nCual es el valor de \\(P(A \\mid A)\\)?\nCual es la probabilidad de \\(P(A, B)\\)?\nCual es la probabilidad de \\(P(A, B)\\) en el caso especial que \\(A\\) y \\(B\\) sean independientes?\nCuando se cumple que \\(P(A \\mid B) = P(A)\\)?\nEs posible que \\(P(A \\mid B) > P(A)\\), cuando?\nEs posible que \\(P(A \\mid B) < P(A)\\), cuando?\n\n\nLos siguientes ejercicios se deben realizar usando Python (y NumPy, PreliZ, Matplotlib) 1. Ilustrar que la distribución de Poisson se aproxima a una binomial cuando para la binomial \\(n >> p\\).\n\nPara alguna de las distribuciones discretas presentadas en esta notebook verificar que la probabilidad total es 1.\nPara alguna de las distribuciones continuas presentadas en esta notebook verificar que el área bajo la curva es 1.\nObtener la cdf a partir de la pdf (usar el método pdf provisto por PreliZ). La función np.cumsum puede ser de utilidad.\nObtener la pdf a partir de la cdf (usar el método cdf provisto por PreliZ). La función np.diff puede ser de utilidad.\nRepetir la simulación para la ley de los grandes números para al menos 3 distribuciones de probabilidad. Para cada distribución probar más de un conjunto de parámetros.\nRepetir la simulación para el teorema central del límite para al menos 3 distribuciones de probabilidad. Para cada distribución probar más de un conjunto de parámetros.\nMostrar en un gráfico que la media \\(\\bar X\\) converge a \\(\\mu\\) y la varianza converge a \\(\\frac{\\sigma^2}{n}\\) a medida que aumenta el tamaño de la muestra."
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#el-universo-bayesiano",
    "href": "01_Inferencia_Bayesiana.html#el-universo-bayesiano",
    "title": "2  Inferencia Bayesiana",
    "section": "2.1 El universo Bayesiano",
    "text": "2.1 El universo Bayesiano\nEn este curso aprenderemos sobre una forma de hacer estadística llamada usualmente estadística Bayesiana. El nombre se debe a Thomas Bayes (1702-1761) un ministro presbiteriano, y matemático aficionado, quien derivó por primera vez lo que ahora conocemos como el teorema de Bayes, el cual fue publicado (postumanente) en 1763. Sin embargo una de las primeras personas en realmente desarrollar métodos Bayesianos, fue Pierre-Simon Laplace (1749-1827), por lo que tal vez sería un poco más correcto hablar de Estadística Laplaciana y no Bayesiana.\nExiste otro paradigma estadístico llamado estadística clásica o frecuentista. Si ustedes han tenido un curso de estadística (ya sea en el grado o posgrado) es casi seguro que dicho curso fue sobre métodos frecuentistas (aun cuando esto no haya sido explicitado). Es interesante notar que mientras los orígenes de las estadística Bayesiana se remontan al siglo XVIII. Los métodos “clásicos” (o frecuentistas) fueron desarrollados principalmente durante el siglo XX! De hecho una de las motivaciones para desarrollar métodos frecuentistas fue un sentimiento e ideología anti-bayesiano. A lo largo del curso nos centraremos en los métodos Bayesianos.\nHay dos ideas centrales que hacen que un método sea Bayesiano:\n\nToda cantidad desconocida es modelada utilizando una distribución de probabilidad de algún tipo.\nEl teorema de Bayes es usado para actualizar dicha distribución a la luz de los datos.\n\nEn el universo Bayesiano las cantidades conocidas son consideradas fijas y usualmente les llamamos datos. Por el contrario toda cantidad desconocida es considerada como una variable aleatoria y modelada usando una distribución de probabilidad.\n\n2.1.1 Teorema de Bayes\nEl teorema de Bayes es una consecuencia directa de la regla del producto, veamos.\n\\[\np(\\theta, Y) = p(\\theta \\mid Y)\\; p(Y) \\\\\np(\\theta, Y) = p(Y \\mid \\theta)\\; p(\\theta)\n\\] Dado que los dos términos a la derecha de la igualdad son iguales entre si podemos escribir que:\n\\[\np(\\theta \\mid Y) \\; p(Y) = p(Y \\mid \\theta)\\; p(\\theta)\n\\]\nReordenando llegamos al Teorema de Bayes!\n\\[\np(\\theta \\mid Y) = \\frac{p(Y \\mid \\theta) p(\\theta)}{p(Y)}\n\\]\nEl cual también suele ser escrito de la siguiente forma:\n\\[\n\\overbrace{p(\\theta \\mid Y)}^{\\text{posterior}} = \\frac{\\overbrace{p(Y \\mid \\theta)}^{\\text{likelihood}} \\overbrace{p(\\theta)}^{\\text{prior}}}{\\underbrace{\\int_{\\Theta} p(Y \\mid \\theta) p(\\theta) \\text{d}\\theta}_{\\text{likelihood marginal}}}\n\\]\nEl a priori es la forma de introducir conocimiento previo sobre los valores que pueden tomar los parámetros. A veces cuando no sabemos demasiado se suelen usar a prioris que asignan igual probabilidad a todos los valores de los parámetros, otras veces se puede elegir a prioris que restrinjan los valores de los parámetros a rangos razonables, algo que se conoce como regularización, por ejemplo solo valores positivos. Muchas veces contamos con información mucho más precisa como medidas experimentales previas o límites impuesto por alguna teoría.\nEl likelihood es la forma de incluir nuestros datos en el análisis. Es una expresión matemática que especifica la plausibilidad de los datos. El likelihood es central tanto en estadística Bayesiana como en estadística no-Bayesiana. A medida que la cantidad de datos aumenta el likelihood tiene cada vez más peso en los resultados, esto explica el porqué a veces los resultados de la estadística Bayesiana y frecuentista coinciden cuando la muestra es grande.\nEl a posteriori es la distribución de probabilidad para los parámetros. Es la consecuencia lógica de haber usado un conjunto de datos, un likelihood y un a priori. Se lo suele pensar como la versión actualizada del a priori. De hecho un a posteriori puede ser un a priori de un análisis a futuro.\nLa likelihood marginal (también llamado evidencia) es el likelihood promediado sobre todas los posibles hipótesis (o conjunto de parámetros) \\(\\theta\\), esto es equivalente a \\(p(Y)\\). En general, la evidencia puede ser vista como una simple constante de normalización que en la mayoría de los problemas prácticos puede (y suele) omitirse. Por lo que el teorema de Bayes suele aparecer escrito como:\n\\[\np(\\theta \\mid Y) \\propto p(Y \\mid \\theta) p(\\theta)\n\\]\nEl rol de todos estos términos irá quedando más claro a medida que avancemos.\n\n\n2.1.2 El a posteriori como único estimador\nEl a posteriori representa todo lo que sabemos de un problema, dado un modelo y un conjunto de datos. Y por lo tanto cualquier cantidad que nos interese sobre el problema puede deducirse a partir de él. Típicamente esto toma la forma de integrales como la siguiente.\n\\[\nJ = \\int \\varphi(\\theta) \\ \\ p(\\theta \\mid Y) d\\theta\n\\]\nPor ejemplo, para calcular la media de \\(\\theta\\) deberíamos reemplazar \\(\\varphi(\\theta)\\), por \\(\\theta\\):\n\\[\n\\bar \\theta = \\int \\theta \\ \\ p(\\theta \\mid Y) d\\theta\n\\]\nEsto no es más que la definición de un promedio pesado, donde cada valor de \\(\\theta\\) es pesado según la probabilidad asignada por el a posteriori.\nEn la práctica, y al usar métodos computacionales como los usados en este curso, estas integrales pueden aproximarse usando sumas.\n\n\n2.1.3 Estadística Bayesiana en tres pasos\nEl teorema de Bayes es el único estimador usado en estadística Bayesiana. Por lo que conceptualmente la estadística Bayesiana resulta muy simple. Según George Box y Andrew Gelman et al. (2013) la estadística Bayesiana se reduce a tres pasos:\n\nCrear un modelo probabilístico. Los modelos probabilísticos son historias que dan cuenta de como se generan los datos observados (o por observar). Los modelos se expresan usando distribuciones de probabilidad.\nCondicionar el modelo a los datos observados a fin de obtener el a posteriori. Usando el teorema de Bayes se actualizan las probabilidades asignadas a priori de acuerdo a los datos observados obteniéndose las probabilidades a posteriori.\nCriticar el ajuste del modelo generado a los datos y evaluar las consecuencias del modelo. Se puede demostrar que dada la información previa y los datos observados no existe otro mecanismo capaz de generar una mejor inferencia que la estadística Bayesiana. Esto parece maravilloso, pero hay un problema, solo es cierto si se asumen que los datos y el modelo son correctos. En la práctica, los datos pueden contener errores y los modelos son a duras penas aproximaciones de fenómenos reales. Por lo tanto es necesario realizar varias evaluaciones, incluyendo si las predicciones generadas por el modelo se ajustan a los datos observados, si las conclusiones obtenidas tienen sentido dado el marco conceptual en el que uno trabaja, la sensibilidad de los resultados a los detalles del modelo (sobre todo a detalles para los cuales no tenemos demasiada información), etc."
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#inferencia-bayesiana",
    "href": "01_Inferencia_Bayesiana.html#inferencia-bayesiana",
    "title": "2  Inferencia Bayesiana",
    "section": "2.2 Inferencia Bayesiana",
    "text": "2.2 Inferencia Bayesiana\nEn la práctica la mayoría de los modelos tendrán más de un parámetro, pero al usar software como PyMC modelar 1 o 1000 parámetros es más o menos lo mismo. Sin embargo, esos modelos pueden distraernos de los conceptos esenciales, por lo que considero importante comenzar por el caso más sencillo.\n\n2.2.1 El problema de la moneda\nA juzgar por la cantidad de ejemplos sobre monedas arrojadas al aires en libros de estadística y probabilidad, pareciera que las monedas son uno de los objetos de estudio centrales de estas disciplinas.\nUna de las razones detrás de la ubiquidad de este ejemplo es que las monedas son objetos familiares que facilitan discutir conceptos que de otra forma podrían sonar demasiado abstractos. De todas formas quizá la razón más importante sea que el problema puede ser modelado de forma simple y que muchos problemas reales son conceptualmente similares, de hecho cualquier problema en donde obtengamos resultados binarios (0/1, enfermo/sano, spam/no-spam, etc) puede ser pensado como si estuviéramos hablando de monedas. En definitiva el modelo que veremos a continuación (ejemplificado con monedas) sirve para cualquier situación en la cual los datos observados solo pueden tomar dos valores mutuamente excluyentes. Debido a que estos valores son nominales y son dos, a este modelo se le llama binomial.\nEn el siguiente ejemplo trataremos de determinar el grado en que una moneda está sesgada. En general cuando se habla de sesgo se hace referencia a la desviación de algún valor (por ejemplo, igual proporción de caras y cecas), pero aquí usaremos el termino sesgo de forma más general. Diremos que el sesgo es un valor en el intervalo [0, 1], siendo 0 para una moneda que siempre cae ceca y 1 para una moneda que siempre cae cara y lo representaremos con la variable \\(\\theta\\). A fin de cuantificar \\(\\theta\\) arrojaremos una moneda al aire repetidas veces, por practicidad arrojaremos la moneda de forma computacional (¡pero nada nos impide hacerlo manualmente!). Llevaremos registro del resultado en la variable \\(y\\). Siendo \\(y\\) la cantidad de caras obtenidas en un experimento.\nHabiendo definido nuestro problema debemos expresarlo en términos del teorema de Bayes,\n\\[\np(\\theta \\mid Y) \\propto p(Y \\mid  \\theta) p(\\theta)\n\\]\nDonde, como dijimos \\(\\theta = 1\\) quiere decir 100% cara y \\(\\theta = 0\\) 100% ceca.\nAhora solo restar reemplazar los dos términos a la derecha de la igualdad, el a priori y el likelihood, por distribuciones de probabilidad adecuadas y luego multiplicarlas para obtener el término a la izquierda, el a posteriori. Como es la primera vez que haremos ésto, lo haremos paso a paso y analíticamente. En el próximo capítulo veremos cómo hacerlo computacionalmente.\n\n\n2.2.2 Definiendo el a priori\nEl a priori lo modelaremos usando una distribución beta, que es una distribución muy usada en estadística Bayesiana. La \\(pdf\\) de esta distribución es:\n\\[\np(\\theta)= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n\\]\nEl primer término es una constante de normalización. Por suerte para nuestro problema nos basta con establecer una proporcionalidad, por lo que podemos simplificar esta expresión y escribir la distribución beta de la siguiente forma.\n\\[\np(\\theta) \\propto  \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n\\]\nHay varias razones para usar una distribución beta para este y otros problemas:\n\nLa distribución beta varía entre 0 y 1, de igual forma que lo hace \\(\\theta\\) en nuestro modelo.\nEsta distribución combinada con la que elegiremos como likelihood (ver más adelante), nos permitirá resolver el problema de forma analítica.\nEs una distribución versátil para expresar distintas situaciones.\n\nRespecto al último punto, veamos un ejemplo. Supongamos que el experimento de la moneda es realizado por tres personas. Una de ellas dice no saber nada de la moneda por lo tanto a priori todos los valores de \\(\\theta\\) son igualmente probables. La segunda persona desconfía de la moneda, ya que sospecha que es una moneda trucada, por lo tanto considera que está sesgada, pero no sabe para cual de las dos opciones. Por último, la tercer persona asegura que lo más probable es que \\(\\theta\\) tome un valor alrededor de 0.5 ya que así lo indican experimentos previos y análisis teóricos sobre tiradas de monedas. Todas estas situaciones pueden ser modeladas por la distribución beta, como se ve a continuación.\n\n_, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\nx = np.linspace(0, 1, 100)\n\nparams = [(1, 1), (0.5, 0.5), (20, 20)]\n\nfor (a, b), ax  in zip(params, axes):\n    y = pz.Beta(a, b).rv_frozen.pdf(x)\n    ax.plot(x, y)\n    ax.set_yticks([])\n    ax.set_title(f'α = {a} β = {b}')\n\n\n\n\n\ndef beta(α, β):\n    x = np.linspace(0, 1, 130)\n    plt.plot(x, pz.Beta(α, β).rv_frozen.pdf(x))\n    plt.yticks([])\n    plt.ylim(0, 6)\n\ninteract(beta,\n         α=ipyw.FloatSlider(min=0.5, max=7, step=0.5, value=2),\n         β=ipyw.FloatSlider(min=0.5, max=7, step=0.5, value=2));\n\n\n\n\n\n\n2.2.3 Definiendo el likelihood\nHabiendo definido el a priori veamos ahora el likelihood. Asumiendo que el resultado obtenido al arrojar una moneda no influye en el resultado de posteriores experimentos (es decir los experimentos son independientes entre sí) es razonable utilizar como likelihood la distribución binomial.\n\\[\np(y \\mid \\theta) = \\frac{N!}{y!(N-y)!} \\theta^y (1 - \\theta)^{N−y}\n\\]\nDonde N es la cantidad total de experimentos (monedas arrojadas al aire) e \\(y\\) es la cantidad de caras obtenidas. A los fines prácticos podríamos simplificar la igualdad anterior y convertirla en una proporcionalidad, eliminando el término \\(\\frac{N!}{y!(N-y)!}\\) ya que ese término no depende de \\(\\theta\\) que es lo que nos interesa averiguar. Por lo que podríamos establecer que:\n\\[\np(y \\mid \\theta) \\propto \\theta^y (1 - \\theta)^{N−y}\n\\]\nLa elección de esta distribución para modelar nuestro problema es razonable ya que \\(\\theta\\) es la chance de obtener una cara al arrojar una moneda y ese hecho ha ocurrido \\(y\\) veces, de la misma forma \\(1-\\theta\\) es la chance de obtener ceca lo cual ha sido observado \\(N-y\\) veces.\n\ndef binomial(n, θ):\n    plt.bar(range(n+1), pz.Binomial(n, θ).rv_frozen.pmf(range(n+1)))\n    plt.xticks(range(n+1))\n    plt.ylim(0, 1);\n\ninteract(binomial, n=ipyw.IntSlider(min=1, max=10, value=1), θ=ipyw.FloatSlider(min=0, max=1, step=0.05, value=0.5));\n\n\n\n\n\n\n2.2.4 Obteniendo el a posteriori\nSe puede demostrar que siempre que usemos como prior una función beta y como likelihood una distribución binomial obtendremos como resultado una distribución a posteriori, la cual será una beta con los siguientes parámetros:\n\\[\np(\\theta \\mid y) \\propto \\operatorname{Beta}(\\alpha_{a priori} + y, \\beta_{a priori} + N - y)\n\\]\nVeamos de donde surge este resultado, según el teorema de Bayes la distribución a posteriori es el producto del likelihood y la distribución a priori.\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n\\]\nPor lo tanto, en nuestro caso tendremos que:\n\\[\np(\\theta \\mid y) \\propto \\theta^y (1 - \\theta)^{N−y} \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n\\]\nReordenando, obtenemos que el a posteriori es:\n\\[\np(\\theta \\mid y) \\propto \\theta^{\\alpha-1+y}(1-\\theta)^{\\beta-1+N−y}\n\\]\nEsto es una distribución Beta (sin considerar la constante de normalización).\nCuando se cumple que para un cierto likelihood la forma funcional del a priori y la del a posteriori coinciden se dice que el a priori es conjugado con el likelihood. Historicamente los problemas en estadística Bayesiana estuvieron restringidos al uso de a prioris conjugados, ya que estos garantizan la tratabilidad matemática del problema, es decir garantizan que es posible obtener una expresión analítica para nuestro problema. En el próximo capítulo veremos técnicas computacionales modernas que permiten calcular la distribución a posteriori incluso cuando no se usan a prioris conjugados. Estas técnicas computacionales han permitido el resurgimiento de la estadística Bayesiana en las últimas décadas.\n\n\n2.2.5 Notación y visualización de modelos Bayesianos\nPara representar modelos en estadística Bayesiana (y en probabilidad en general) se suele utilizar la siguiente notación\n\\[\n\\begin{align}\n\\theta \\sim & \\operatorname{Beta}(\\alpha, \\beta) \\\\\nY \\sim & \\operatorname{Bin}(n=1, p=\\theta)\n\\end{align}\n\\]\nEl símbolo \\(\\sim\\) indica que la variable a la izquierda se distribuye según la distribución a la derecha. Entonces podríamos decir que \\(\\mathbf{\\theta}\\) es una variable aleatoria con distribución \\(\\operatorname{Beta}\\), y que \\(\\operatorname{Beta}\\) está definida por los parámetros \\(\\alpha\\) y \\(\\beta\\), este es nuestro a priori. En la siguiente linea tenemos el likelihood el cual está definido por una distribución binomial con parámetros \\(n=1\\) y \\(p=\\theta\\).\nGráficamente esto se puede representar usando los diagramas de Kruschke:\n\nEn el primer nivel (de arriba hacia abajo) se observa el a priori, luego el likelihood, y por último los datos. Las flechas indican la vinculación entre las partes del modelo y el signo \\(\\sim\\) la naturaleza estocástica de las variables.\n\n\n2.2.6 Obteniendo los datos\nBien, ahora que sabemos cómo calcular el a posteriori, lo único que resta es conseguir los datos. En este ejemplo los datos son sintéticos, es decir los obtuve computacionalmente mediante un generador de números (pseudo)aleatorios, pero bien podrían haber surgido de un experimento con una moneda real.\n\n\n2.2.7 Calculando el a posteriori\nEn el próximo capítulo veremos cómo usar métodos computacionales para computar un a posteriori sin necesidad de derivarlo analíticamente. Esto es lo que haremos para resolver el resto de los problemas del curso. Pero dado que ya nos tomamos el trabajo de derivar analíticamente la expresión para el a posteriori vamos a usar esa expresión. Si miran el código de la siguiente celda verán que la mayoría de las lineas se encargan de dibujar los resultados y no de calcularlos. El cálculo del a posteriori ocurre en la línea 20. Cada una de estas lineas computa el a posteriori para cada uno de los a prioris que vimos antes. El cálculo es simple, tan solo se computa el valor del a posteriori (usando la función pdf de la distribución beta provista por SciPy) para 2000 puntos igualmente espaciados entre 0 y 1 (linea 9). El loop que empieza en la linea 11 se debe a que exploraremos cómo cambian las distribuciones a posteriori para distinta cantidad de datos (n_intentos). Con un círculo negro de contorno blanco se indica el valor real de \\(\\theta\\), valor que por supuesto es desconocido en una situación real, pero conocido para mí, ya que los datos son sintéticos.\n\nplt.figure(figsize=(12, 9))\n\nn_trials = [0, 1, 2, 3, 4, 8, 16, 32, 50, 150]\ndata = [0, 1, 1, 1, 1, 4, 6, 9, 13, 48]\ntheta_real = 0.35\n\nbeta_params = [(1, 1), (0.5, 0.5), (20, 20)]\ndist = pz.Beta\nx = np.linspace(0, 1, 2000)\n\nfor idx, N in enumerate(n_trials):\n    if idx == 0:\n        plt.subplot(4, 3, 2)\n        plt.xlabel('θ')\n    else:\n        plt.subplot(4, 3, idx+3)\n        plt.xticks([])\n    y = data[idx]\n    for (a_prior, b_prior) in beta_params:\n        posterior = dist(a_prior + y, b_prior + N - y).rv_frozen.pdf(x)\n        plt.fill_between(x, 0, posterior, alpha=0.7)\n\n    plt.plot(theta_real, 0, ms=9, marker='o', mec='w', mfc='k')\n    plt.plot(0, 0, label=f'{N:4d} experimentos\\n{y:4d} caras', alpha=0)\n    plt.xlim(0, 1)\n    plt.ylim(0, 12)\n    plt.legend()\n    plt.yticks([])"
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#analizando-los-resultados",
    "href": "01_Inferencia_Bayesiana.html#analizando-los-resultados",
    "title": "2  Inferencia Bayesiana",
    "section": "2.3 Analizando los resultados",
    "text": "2.3 Analizando los resultados\nLa primer figura del panel muestra los a priori, nuestra estimación de \\(\\theta\\) dado que no hemos realizado ningún experimento. Las sucesivas nueve figuras muestran las distribuciones a posteriori y se indica la cantidad de experimentos y de caras obtenidas. Además se puede ver un círculo negro de contorno blanco en 0.35, la cual representa el valor verdadero de \\(\\theta\\). Por supuesto que en problemas reales este valor es desconocido.\nEste ejemplo es realmente ilustrativo en varios aspectos.\n\nEl resultado de un análisis Bayesiano NO es un solo valor, si no una distribución (a posteriori) de los valores plausibles de los parámetros (dado los datos y el modelo).\nLa dispersión o ancho de las curvas es una medida de la incertidumbre sobre los valores.\nEl valor más probable viene dado por la moda de la distribución (el pico de la distribución).\nAún cuando \\(\\frac{2}{1} = \\frac{8}{4}\\) son numéricamente iguales tenemos menor incertidumbre en un resultado cuando el número de experimentos es mayor.\nDada una cantidad suficiente de datos los resultados tienden a converger sin importar el a priori usado.\nLa rapidez con la que los resultados convergen varía. En este ejemplo las curvas azul y turquesa parecen converger con tan solo 8 experimentos, pero se necesitan más de 50 experimentos para que las tres curvas se muestren similares. Aún con 150 experimentos se observan ligeras diferencias.\nPartiendo de los a priori uniforme (azul) o sesgado (turquesa) y habiendo realizado un solo experimento y observado una sola cara, lo más razonable es pensar que estamos frente a una moneda con dos caras!\nLa situación cambia drásticamente al ver por primera vez una moneda caer ceca. Ahora lo más probable (dado cualquiera de los tres a prioris) es inferir que \\(\\theta=0.5\\). Los valores de \\(\\theta\\) exactamente 0 o 1 se vuelven imposibles.\nEl a priori naranja es más informativo que los otros dos (la distribución esta más concentrada), por ello se requiere de un número mas grande de experimentos para “moverlo”.\nEl a priori uniforme (azul) es lo que se conoce como no informativo. El resultado de un análisis Bayesiano usando un a priori no-informativos en general coinciden con los resultados de análisis frecuentistas (en este caso el valor esperado de \\(\\theta = \\frac{y}{N}\\)).\n\n\n2.3.1 Influencia y elección del a priori\nDe los ejemplos anteriores debería quedar claro que los a priori influencian los resultados de nuestros cálculos. Esto tiene total sentido si no fuese así no haría falta incluirlos en el análisis y todo sería más simple (aunque nos perderíamos la oportunidad de usar información previa). De los ejemplos anteriores también debería quedar claro que a medida que aumentan los datos (como las tiradas de monedas) los resultados son cada vez menos sensibles al a priori. De hecho, para una cantidad infinita de datos el a priori no tiene ningún efecto. Exactamente cuantos datos son necesarios para que el efecto del a priori sea despreciable varía según el problema y los modelos usados. En el ejemplo de la moneda se puede ver que 50 experimentos bastan para hacer que dos de los resultados sean prácticamente indistinguibles, pero hacen falta más de 150 experimentos para que los 3 resultados se vuelvan practicamente independientes del a priori. Esto es así por que los dos primeros a prioris son relativamente planos, mientras que el tercer a priori concentra casi toda la probabilidad en una región relativamente pequeña. El tercer a priori no solo considera que el valor más probable de \\(\\theta\\) es 0.5, si no que considera que la mayoría de los otros valores son muy poco probables. ¿Cómo cambiarían los resultados si hubiéramos usado como a priori \\(\\operatorname{Beta}(\\alpha=2, \\beta=2)\\)?\nLa elección de los a priori puede poner nervioso a quienes se inician en el análisis Bayesiano (o a los detractores de este paradigma). ¡El temor es que los a prioris censuren a los datos y no les permitan hablar por sí mismos! Eso está muy bien, pero el punto es que los datos no saben hablar, con suerte murmuran. Los datos solo tienen sentido a la luz de los modelos (matemáticos y mentales) usados para interpretarlos, y los a prioris son parte de esos modelos.\nHay quienes prefieren usar a priori no-informativos (también conocidos como a priori planos, vagos, o difusos). Estos a priori aportan la menor cantidad posible de información y por lo tanto tienen el menor impacto posible en el análisis. Si bien es posible usarlos, en general hay razones prácticas para no preferirlos. En este curso usaremos a priori ligeramente informativos siguendo las recomendaciones de Gelman, McElreath, Kruschke, y otros. En muchos problemas sabemos al menos algo de los valores posibles que pueden tomar nuestros parámetros, por ejemplo que solo pueden ser positivos, o que están restringidos a sumar 1 o el rango aproximado, etc. En esos casos podemos usar a prioris que introduzcan esta ligera información. En estos casos podemos pensar que la función del a priori es la de mantener las inferencias dentro de límites razonables. Estos a priori se suelen llamar regularizadores.\nPor supuesto que también es posible usar a prioris informativos (o fuertes). Hacer esto es razonable solo si contamos con información previa confiable. Esto puede ser ventajoso en casos en que los datos contengan poca información sobre el problema. Si la información no viene por el likelihood (datos), entonces puede venir por el a priori. A modo de ejemplo, en bioinformática estructural es común usar toda la información previa posible (de forma Bayesiana y no-Bayesiana) para resolver problemas. Esto es posible por la existencia de bases de datos que almacenan los resultados de cientos o miles experimentos realizados a lo largo de décadas de esfuerzo (¡No usar esta información sería casi absurdo!). En resumen, si contás con información confiable no hay razón para descartarla, menos si el argumento es algo relacionado con pretender ser objetivo (¡No hay objetividad en negar lo que se sabe!).\nHasta ahora hemos visto que es posible clasificar, aunque sea de forma vaga o aproximada, a los a priori en función de la información que contienen. Pero saber esta clasificación no necesariamente hace las cosas más simples a la hora de elegir un a priori. ¿Acaso no sería mejor eliminar los a prioris de nuestro análisis? Eso haría el asunto mucho mas simple. Bueno, el punto es que desde una perspectiva Bayesiana todos los modelos tienen a prioris, aun cuando no sean explícitos. De hecho muchos resultados de la estadística frecuentista pueden considerarse casos especiales de modelos Bayesianos usando a prioris planos. Volviendo a la figura anterior se puede ver que la moda del a posteriori para la curva azul. Coincide con la estimación (puntual) frecuentista para el valor de \\(\\theta\\)\n\\[\n\\hat \\theta = {{y} \\over {N}}\n\\]\nNotar que \\(\\hat \\theta\\) es una estimación puntual (un número) y no una distribución.\nEste ejemplo nos muestra que no es posible hacer análisis estadísticos y sacarse los a prioris de encima. Un posible corolario es que es más flexible y transparente especificar los a prioris de forma explícita que esconderlos bajo la cama. Al hacerlo ganamos mayor control sobre nuestro modelo, mayor transparencia y por el mismo precio la estimación de la incertidumbre con la que se estima cada parámetro.\nPor último, hay que recordar que el modelado estadístico (como otras formas de modelado) es un proceso iterativo e interactivo. Nada nos impide usar más de un a priori (o un likelihood) si así lo quisiéramos. Una parte importante del modelado es la de cuestionar los supuestos y los a prioris son simplemente un tipo de supuestos (como lo son los likelihoods). Si tuvieramos más de un a priori razonable podríamos realizar un análisis de sensibilidad, es decir evaluar como cambian los resultados con los a prioris, podríamos llegar a la conclusión que para un rango amplio de a prioris ¡los resultados no varían! Más adelante veremos varias herramientas para comparar distintos modelos.\nDado que los a prioris tienen un papel central en la estadística Bayesiana, seguiremos discutiéndolos a medida que vayamos viendo problemas concretos. Por lo que si esta discusión no ha aclarado todas tus dudas y seguís algo confundido, mejor mantener la calma y no preocuparse demasiado, este tema ha sido motivo de discusión y confusión durante décadas ¡y la discusión todavía continua!\n\n\n2.3.2 Cuantificando el peso del a priori\nEn general la distribución más familiar para la mayoría de las personas es la distribución Gaussiana, como esta distribución está definida por dos parámetros, la media y la dispersión de ese valor medio, suele resultarnos natural pensar las distribuciones en esos términos. Si queremos expresar la distribución beta en función de la media y la dispersión podemos hacerlo de la siguiente forma:\n\\[\\begin{align}\n\\alpha &= \\mu \\kappa \\\\\n\\beta &= (1 - \\mu) \\kappa\n\\end{align}\\]\ndonde \\(\\mu\\) es la media y \\(\\kappa\\) es un parámetro llamado concentración. Por ejemplo si \\(\\mu=0.5\\) y \\(\\kappa=40\\), tenemos que:\n\\[\\begin{align}\n\\alpha = 0.5 \\times 40 &= 20 \\\\\n\\beta = (1-0.5) \\times 40 &= 20\n\\end{align}\\]\n\\(\\kappa\\) se puede interpretar como la cantidad de experimentos si/no que realizamos dándonos como resultado la media \\(\\mu\\). Es decir el a priori no sesgado (naranja) equivale a haber arrojado una moneda 40 veces y haber obtenido como media 0.5. Es decir que si usamos ese a priori recién al observar 40 experimentos si/no, los datos tendrán el mismo peso relativo que el a priori, por debajo de este número el a priori contribuye más que los datos al resultado final y por encima menos. El a priori azul (uniforme) equivale a haber observado a la moneda caer una vez cara y otra vez ceca (\\(\\kappa = 2\\)). Cuando \\(\\kappa < 2\\), la cosa se pone un poco extraña, por ejemplo el a priori sesgado (turquesa) equivale a haber observado una sola moneda (\\(\\kappa = 1\\)) pero en una especie de (a falta de mejor analogía) ¡superposición cuántica de estados!\n\n\n2.3.3 Resumiendo el a posteriori\nEl resultado de un análisis Bayesiano es siempre una distribución de probabilidad.\nA la hora de comunicar los resultados de un análisis Bayesiano, lo más informativo es reportar la distribución completa, aunque esto no siempre es posible o deseable, por ejemplo el a posteriori de una distribución multidimensional es imposible de dibujar en papel. En general, se suele recurrir a distintas medidas que resumen el a priori, por ejemplo reportando la media de la distribución a posteriori. Algo un poco más informativo es reportar además un intervalo de credibilidad. Existen varios criterios para definir intervalos de credibilidad, el que usaremos en este curso (y que también es ampliamente usado en la literatura) es lo que se conoce como intervalo de más alta densidad y nos referiremos a él por su sigla en ingles, HDI (Highest Posterior Density interval). Un HDI es el intervalo, más corto, que contiene una porción fija de la densidad de probabilidad, generalmente el 95% (aunque otros valores como 90% o 50% son comunes). Cualquier punto dentro de este intervalo tiene mayor densidad que cualquier punto fuera del intervalo. Para una distribución unimodal, el HDI 95 es simplemente el intervalo entre los percentiles 2,5 y 97,5.\nArviZ es un paquete de Python para análisis exploratorio de modelos Bayesianos. ArviZ provee de funciones que facilitan el resumir el a posteriori. Por ejemplo plot_posterior puede ser usado para generar un gráfico con la media y HDI. En el siguiente ejemplo en vez de un a posteriori de un ejemplo real estamos usando datos generados al azar según una distribución beta.\n\nmock_posterior = pz.Beta(5, 11).rvs(size=1000)\naz.plot_posterior(mock_posterior, figsize=(8, 4));\n\n\n\n\nAhora que estamos aprendiendo que es un HDI por primera vez y antes de que automaticemos el concepto conviene aclarar un par de puntos.\n\nLa elección automática de 95% (o cualquier otro valor) es totalmente arbitraria. En principio no hay ninguna razón para pensar que describir el a posteriori con un HDI 95 sea mejor que describirlo con un HDI 98 o que no podamos usar valores como 87% o 66%. El valor de 95% es tan solo un accidente histórico. Como un sutil recordatorio de esto ArviZ usa por defecto el intervalo de 94%.\nUn intervalo de credibilidad (que es Bayesiano) no es lo mismo que un intervalo de confianza (que es frecuentista). Un intervalo de confianza es un intervalo que se define según un nivel de confianza, en general del 95%. Un intervalo de confianza se construye de tal forma que si repitiéramos infinitas veces un experimento obtendríamos que la proporción de intervalos que contienen el valor verdadero del parámetro que nos interesa coincide con el nivel de confianza estipulado. Contra-intuitivamente esto no es lo mismo que decir que un intervalo en particular tiene una probabilidad \\(x\\) de contener el parámetro (esto sería la definición de un intervalo de credibilidad, que es Bayesiano). De hecho, un intervalo de confianza en particular contiene o no contiene al valor, la teoría frecuentista no nos deja hablar de probabilidades de los parámetros, ya que estos tienen valores fijos. Si no queda clara la diferencia no te hagas problema, la diferencia entre estos dos conceptos suele ser tan difícil de entender que en la práctica estudiantes y científicos por igual interpretan los intervalos de confianza (frecuentistas) como intervalos de credibilidad (Bayesianos).\n\n\nSi bien desde la perspectiva Bayesiana podemos afirmar que un intervalo de credibilidad nos permite asegurar que la probabilidad de un parámetro está acotado en cierto rango. Siempre hay que tener presente que dicha afirmación es correcta SOLO en sentido teórico. Es decir, solo si todos los supuestos contenidos en el modelo son ciertos. Una inferencia es siempre dependiente de los datos y modelos usados."
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#distribución-predictivas",
    "href": "01_Inferencia_Bayesiana.html#distribución-predictivas",
    "title": "2  Inferencia Bayesiana",
    "section": "2.4 Distribución predictivas",
    "text": "2.4 Distribución predictivas\nSi bien el objeto central de la estadística Bayesiana es la distribución a posteriori. Existen otras distribuciones muy importantes. Una de ellas es la distribución predictiva a posteriori, otra es la distribución predictiva a priori.\n\n2.4.1 Distribución predictivas a posteriori\nEsta distribución representa las predicciones \\(\\tilde{y}\\) de un modelo una vez obtenido el a posteriori. Se calcula de la siguiente manera:\n\\[\np(\\tilde{y}  \\mid  y) = \\int p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) d\\theta\n\\]\nEs decir integramos \\(\\theta\\) de acuerdo a la distribución a posteriori.\nComputacionalmente podemos generar muestras de esta distribución según el siguiente procedimiento:\n\nElegimos un valor de \\(\\theta\\) de acuerdo a la distribución a posteriori \\(p(\\theta \\mid y)\\)\nFijamos \\(\\theta\\) en la distribución que usamos como likelihood \\(p(\\tilde{y} \\mid \\theta)\\) y generamos una muestra aleatoria\nRepetimos desde 1, tantas veces como muestras necesitemos\n\nLos datos generados son predictivos ya que son los datos que se esperaría ver por ejemplo en un futuro experimento, es decir son variables no observadas pero potencialmente observables. Como veremos en el siguiente capítulo un uso muy común para la distribución predictiva a posteriori es compararla con los datos observados y así evaluar si el posterior calculado es razonable.\n\n\n2.4.2 Distribución predictiva a priori\nAsi como es posible generar datos sintéticos desde el a posteriori. Es posible hacerlo desde el prior. En este caso la distribución se llama distribución predictiva a priori. Y representa los datos \\(p(Y^\\ast)\\) que el modelo espera ver antes de haber visto los datos. O más formalmente antes de haber sido condicionado a los datos. Se calcula como:\n\\[\np(Y^\\ast) =  \\int_{\\Theta} p(Y^\\ast \\mid \\theta) \\; p(\\theta) \\; d\\theta\n\\]\nEs importante notar que la definición es muy similar a la distribución predictiva a posteriori, solo que ahora integramos a lo largo del prior en vez del posterior.\nLos datos generados son predictivos ya que son los datos que el modelo esperara ver, es decir son datos no observados pero potencialmente observables. Como veremos en el siguiente capítulo un uso muy común para la distribución predictiva a priori es compararla con nuestro conocimiento previo y así evaluar si el modelo es capaz de generar resultados razonable, incluso antes de haber incorporado los datos.\n\n\n2.4.3 Distribución predictiva a priori y a posterior para el problema de la moneda.\nEn el caso del modelo beta-binomial es posible obtener analíticamente tanto la distribución predictiva a priori como a posteriori y estas son:\n\\[\np(Y^\\ast) \\propto \\operatorname{Beta-binomial}(n=N, \\alpha_{a priori}, \\beta_{a priori})\n\\]\n\\[\np(\\tilde{Y}  \\mid  Y)  \\propto \\operatorname{Beta-binomial}(n=N, \\alpha_{a priori} + y, \\beta_{a priori} + N - y)\n\\]\nOmitiremos la discusión de como se obtienen estas distribuciones\n\n\n2.4.4 Cuarteto Bayesiano\nEl siguiente bloque de código computa las distribuciones a priori, a posteriori, predictiva a priori y predictiva a posteriori. En vez de usar la distribución \\(\\operatorname{Beta-binomial}\\) para las distribuciones predictivas hemos optado por usar una aproximación más computacional y muestrear primero de la distribuciones beta y luego de la binomial. Esperamos que esta decisión contribuya a comprender mejor que representan estas distribuciones.\nEs importante notar que mientras la distribuciones a priori y a posteriori son distribución sobre los parámetros en un modelo, la distribución predictivas a priori y a posteriori son distribuciones sobre los datos (predichos).\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=\"row\", sharey=\"row\")\naxes = np.ravel(axes)\ndist = pz.Beta\na_prior = 1\nb_prior = 1\nN = 12\ny = 3\nx = np.linspace(0, 1, 100)\n\n\nprior = dist(a_prior, b_prior).rv_frozen.pdf(x)\naxes[0].fill_between(x, 0, prior)\naxes[0].set_title(\"Prior\")\naxes[0].set_yticks([])\n\n\nposterior = dist(a_prior + y, b_prior + N - y).rv_frozen.pdf(x)\naxes[1].fill_between(x, 0, posterior)\naxes[1].set_title(\"Posterior\")\n\n\nprior = dist(a_prior, b_prior).rvs(500)\nprior_predictive = np.hstack([pz.Binomial(n=N, p=p).rvs(N) for p in prior])\naxes[2].hist(prior_predictive, bins=range(0, N+2), rwidth=0.9, align=\"left\", density=True)\naxes[2].set_title(\"Prior predictive\")\n\nposterior = dist(a_prior + y, b_prior + N - y).rvs(500)\nprior_predictive = np.hstack([pz.Binomial(n=N, p=p).rvs(N) for p in posterior])\naxes[3].hist(prior_predictive, bins=range(0, N+2), rwidth=0.9, align=\"left\", density=True)\naxes[3].set_title(\"Posterior predictive\");\n\nfig.suptitle(\"Cuarteto Bayesiano\", fontweight=\"bold\", fontsize=16);"
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#resumen",
    "href": "01_Inferencia_Bayesiana.html#resumen",
    "title": "2  Inferencia Bayesiana",
    "section": "2.5 Resumen",
    "text": "2.5 Resumen\nEmpezamos este capítulo con una breve discusión sobre el modelado estadístico y la teoría de la probabilidad y teorema de Bayes que se deriva de ella. Luego utilizamos el problema de la moneda como una excusa para introducir aspectos básicos del modelado Bayesiano y el análisis de datos. Utilizamos este ejemplo clásico para transmitir algunas de las ideas más importantes de las estadística Bayesiana, fundamentalmente el uso de distribuciones de probabilidad para construir modelos y representar la incertidumbre. Tratamos de desmitificar el uso de los a prioris dándoles el mismo estatus epistemológico-metodológico que otros elementos que forman parte del proceso de modelado e inferencia, como el likelihood o incluso meta-preguntas, ¿Por qué me interesa este problema en particular? Concluimos el capítulo con una breve y simple descripción de cómo interpretar y comunicar los resultados de un análisis bayesiano.\nLa siguiente figura, inspirada en una figura de Sumio Watanabe resume el flujo de trabajo Bayesiano tal cual se describió en este capítulo.\n\n\nSuponemos que existe una distribución verdadera que, en general, es desconocida (ya sea en la práctica o intrínsecamente). De esta distribución se obtiene una muestra finita, ya sea haciendo un experimento, una encuesta, una observación, una simulación, etc.\nA partir de la muestra realizamos una inferencia Bayesiana obteniendo una distribución a posteriori. Esta distribución es el objeto central de la estadística Bayesiana ya que contiene toda la información sobre un problema (de acuerdo al modelo y los datos).\nUna cantidad que podemos derivar del a posteriori es la distribución predictiva a posteriori, es decir predicciones. Una forma de evaluar un modelo es comparar la distribución predictiva a posteriori con la muestra finita que obtuvimos en primer lugar.\n\nLa figura anterior es muy general y omite varios pasos, pero contiene la idea esencial que el modelado es un proceso iterativo. En los siguientes capítulo, veremos como sumar nuevos pasos, como que hacer cuando tenemos más de un modelo y profundizar sobre estos pasos y lo aprendido en este capítulo"
  },
  {
    "objectID": "01_Inferencia_Bayesiana.html#ejercicios",
    "href": "01_Inferencia_Bayesiana.html#ejercicios",
    "title": "2  Inferencia Bayesiana",
    "section": "2.6 Ejercicios",
    "text": "2.6 Ejercicios\n\nEl estadístico Bruno de Finetti declaró que “Las probabilidades no existen”, queriendo indicar que las probabilidades son solo una herramienta para cuantificar la incerteza y que no tienen existencia objetiva en sí mismas. Edwin Jaynes, físico, declaró que la teoría de probabilidad es la lógica de la ciencia. Discutí estos enunciados a la luz de lo expuesto en este y el anterior capítulo.\nUsá pz.Beta().plot_interactive() para explorar distintas combinaciones de parámetros de la distribución beta. Qué es lo que hacen los parámetros \\(\\alpha\\) y \\(\\beta\\)?\nRepetí el punto anterior pero para otra distribución\nPreliZ tiene una función llamada maxent. Explicá que hace y generá un par de ejemplos\nUsá la siguiente función para explorar diversas combinaciones de priors y likelihoods. Enunciá las conclusiones que consideres más relevantes.\n\n\ndef a_posteriori_grilla(grilla=10, a=1, b=1, caras=6, tiradas=9):\n    grid = np.linspace(0, 1, grilla)\n    prior = pz.Beta(a, b).pdf(grid)\n    likelihood = pz.Binomial(n=tiradas, p=grid).pdf(caras)\n    posterior = likelihood * prior\n    posterior /= posterior.sum()\n    _, ax = plt.subplots(1, 3, sharex=True, figsize=(16, 4))\n    ax[0].set_title('caras = {}\\ntiradas = {}'.format(caras, tiradas))\n    for i, (e, e_n) in enumerate(zip([prior, likelihood, posterior], ['a priori', 'likelihood', 'a posteriori'])):\n        ax[i].set_yticks([])\n        ax[i].plot(grid, e, 'o-', label=e_n)\n        ax[i].legend(fontsize=14)\n\n\ninteract(a_posteriori_grilla, grilla=ipyw.IntSlider(min=2, max=100, step=1, value=15), a=ipyw.FloatSlider(min=1, max=7, step=1, value=1), b=ipyw.FloatSlider(\n    min=1, max=7, step=1, value=1), caras=ipyw.IntSlider(min=0, max=20, step=1, value=6), tiradas=ipyw.IntSlider(min=0, max=20, step=1, value=9));"
  },
  {
    "objectID": "02_Programación_probabilística.html#introducción-a-pymc",
    "href": "02_Programación_probabilística.html#introducción-a-pymc",
    "title": "3  Programación probabilista",
    "section": "3.1 Introducción a PyMC",
    "text": "3.1 Introducción a PyMC\nPyMC es un paquete para programación probabilística bajo Python. PyMC es lo suficientemente madura para resolver muchos problemas estadísticos. PyMC permite crear modelos probabilísticos usando una sintaxis intuitiva y fácil de leer que es muy similar a la sintaxis usada para describir modelos probabilísticos.\nLa mayoría de las funciones de PyMC están escritas en Python. Mientras que las partes computacionalmente demandantes están escritas en NumPy y PyTensor. Pytensor es una biblioteca de Python que permite definir, optimizar y evaluar expresiones matemáticas que involucran matrices multidimensionales de manera eficiente. PyTensor es hija de Theano una librería de Python originalmente desarrollada para deep learning (que es a su vez la antecesora de TensorFlow, PyTorch, etc).\n\n3.1.1 El problema de la moneda, ahora usando PyMC y ArviZ\nA continuación revistaremos el problema de la moneda visto en el capítulo anterior, usando esta vez PyMC para definir nuestro modelo y hacer inferencia. Luego usaremos ArviZ para analizar el a posterori.\nA continuación generaremos datos sintéticos, en este caso asumiremos que conocemos el valor the \\(\\theta\\) y lo llamaremos theta_real, y luego intentaremos averiguar este valor como si no lo conociéramos. En un problema real theta_real sería desconocido y realizaríamos un proceso de inferencia precisamente para averiguar su valor.\n\nnp.random.seed(123)\nn_experimentos = 4\ntheta_real = 0.35  # en una situación real este valor es desconocido\ndatos = pz.Binomial(n=1, p=theta_real).rvs(size=n_experimentos)\ndatos\n\narray([1, 0, 0, 0])\n\n\n\n\n3.1.2 Creación del modelo\nAhora que tenemos nuestros datos es necesario especificar el modelo. Para ello usaremos una distribución beta (con parámetros \\(\\alpha=\\beta=1\\)) como a priori y la distribución de Bernoulli como likelihood. Usando la notación usual en estadística tenemos:\n\\[\\begin{align}\n\\theta &\\sim \\operatorname{Beta}(\\alpha=1, \\beta=1)\\\\\nY &\\sim \\operatorname{Bin}(n=1, p=\\theta)\n\\end{align}\\]\n\nCada uno de los elementos del array datos es un experimento de Bernoulli, es decir un experimento donde solo es posible obtener dos valores (0 o 1) si en cambio tuviera el número total de “caras” obtenidas en varios experimentos de Bernoulli podríamos modelar el likelihood como una distribución Binomial.\n\nEsto modelo se traduce casi literalmente a PyMC, veamos:\n\nwith pm.Model() as nuestro_primer_modelo:\n    θ = pm.Beta(\"θ\", alpha=1, beta=1)  # a priori\n    y = pm.Bernoulli(\"y\", p=θ, observed=datos)  # likelihood\n    # y = pm.Binomial('y',n=n_experimentos, p=θ, observed=sum(datos))\n\nEn la primer linea hemos creado un nuevo objeto llamado nuestro_primer_modelo. Este objeto contiene información sobre el modelo y las variables que lo conforman. PyMC usa el bloque with para indicar que todas las lineas que están dentro de él hacen referencia al mismo modelo (que en este caso se llama nuestro_primer_modelo).\nLa segunda linea de código, especifica el a priori, como pueden ver la sintaxis sigue de cerca a la notación matemática, la única diferencia es que el primer argumento es siempre una cadena que especifica el nombre de la variable aleatoria (el nombre es usado internamente por PyMC), este nombre siempre deberá coincidir con el nombre de la variable de Python a la que se le asigna. De no ser así el código correrá igual, pero puede conducir a errores y confusiones al analizar el modelo.\n\nEs importante recalcar que las variables de PyMC, como \\(\\theta\\), no son números sino objetos que representan distribuciones. Es decir objetos a partir de los cuales es posible calcular probabilidades y generar números aleatorios.\n\nEn la tercer linea de código se especifica el likelihood, que como verán es similar a la linea anterior con la diferencia que hemos agregado un argumento llamado observed al cual le asignamos nuestros datos. Esta es la forma de indicarle a PyMC cuales son los datos. Los datos pueden ser números, listas de Python, arrays de NumPy o data_frames de Pandas.\n\n\n3.1.3 Inferencia\nNuestro modelo ya está completamente especificado, lo único que nos resta hacer es obtener el a posteriori. En el capítulo anterior vimos como hacerlo de forma analítica, ahora lo haremos con métodos numéricos.\nEn PyMC la inferencia se realiza escribiendo las siguientes lineas:\n\nwith nuestro_primer_modelo:\n    idata = pm.sample(1000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [θ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nPrimero llamamos al objeto que definimos como nuestro modelo (nuestro_primer_modelo), indicando de esta forma que es sobre ese objeto que queremos realizar la inferencia. En la segunda linea le indicamos a PyMC que deseamos 1000 muestras. Esta linea luce inocente, pero internamente PyMC está haciendo muchas cosas por nosotros. Algunas de las cuales son detalladas en el mensaje que se imprime en pantalla.\nVeamos este mensaje:\n\nLa primer linea indica que PyMC ha asignado el método de muestreo NUTS, el cual es un muy buen método para variables continuas.\nLa segunda linea nos da información sobre cómo se inicializaron los valores de NUTS. Un detalle que por ahora no nos preocupa.\nLa tercer linea indica que PyMC correrá cuatro cadenas en paralelo, es decir generará cuatro muestras independientes del a posteriori. Esta cantidad puede ser diferente en sus computadoras ya que es determinada automáticamente en función de los procesadores disponibles (que en mi caso, 4). sample tiene un argumento chains que permite modificar este comportamiento.\nLa cuarta linea indica qué variable ha sido asignada a cual método de muestreo. En este caso la información es redundante, ya que tenemos una sola variable, pero esto no siempre es así. PyMC permite combinar métodos de muestreo, ya sea de forma automática basado en propiedades de las variables a muestrear o especificado por el usuario usando el argumento step.\nLa quinta linea es una barra de progreso con varias métricas sobre la velocidad del muestreo, que en este caso (y para referencia futura) es muy alta. También indica la cantidad de cadenas usadas y la cantidad de divergencias. Tener 0 divergencias es ideal, más adelante discutiremos la razón.\nPor último tenemos un detalle de la cantidad de muestras generadas, aunque pedimos 1000 obtuvimos 8000, la razón es que es son 1000 por cadena (4 cadenas en mi caso), es decir 4000. Todavía nos queda explicar 4000 muestras extras, estas se corresponden a 1000 por cadena y son muestras que PyMC utiliza para auto-tunear el método de muestreo. Estás muestras son luego descartadas automáticamente ya que no son muestras representativas del posterior. La cantidad de pasos que se usan para tunear el algoritmo de muestro se puede cambiar con el argumento tune de la función pm.sample(.).\n\n\n\n3.1.4 Resumiendo el a posteriori\nPor lo general, la primer tarea a realizar luego de haber realizado un muestreo es evaluar como lucen los resultados. La función plot_forestplot de ArviZ es muy útil para esta tarea.\n\naz.plot_forest(idata, combined=True, figsize=(6, 2));\n\n\n\n\nEl punto indica la media, la linea gruesa el rango intercuartil y las lineas finas el HDI 94%\n\nEs importante notar que la variable y es una variable observada, es decir conocida. Mientras que en gráfico anterior estamos dibujando solo \\(\\theta\\) que es la única variables desconocida, y por lo tanto muestreada.\n\nSi quisiéramos un resumen numérico de los resultados podemos usar:\n\naz.summary(idata, kind=\"stats\")\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      θ\n      0.332\n      0.176\n      0.035\n      0.646\n    \n  \n\n\n\n\nComo resultado obtenemos un DataFrame con los valores de la media, la desviación estándar y el intervalo HDI 94% (hdi_3 hdi_97).\nOtra forma de resumir visualmente el a posteriori es usar la función plot_posterior que viene con ArviZ, ya hemos utilizado esta distribución en el capítulo anterior para un falso a posteriori. Vamos a usarlo ahora con un posterior real. Por defecto, esta función muestra un histograma para variables discretas y KDEs para variables continuas. También obtenemos la media de la distribución (podemos preguntar por la mediana o moda usando el argumento point_estimate) y el 94% HDI como una línea negra en la parte inferior de la gráfica. Se pueden establecer diferentes valores de intervalo para el HDI con el argumento hdi_prob. Este tipo de gráfica fue presentado por John K. Kruschke en su gran libro “Doing Bayesian Data Analysis”.\n\naz.plot_posterior(idata);"
  },
  {
    "objectID": "02_Programación_probabilística.html#decisiones-basadas-en-el-posterior",
    "href": "02_Programación_probabilística.html#decisiones-basadas-en-el-posterior",
    "title": "3  Programación probabilista",
    "section": "3.2 Decisiones basadas en el posterior",
    "text": "3.2 Decisiones basadas en el posterior\nA veces describir el a posteriori no es suficiente, y es necesario tomar decisiones basadas en nuestras inferencias. Esto suele implicar reducir una estimación continua a una dicotómica: sí-no, enfermo-sano, contaminado-seguro, etc. Es posible, por ejemplo, que tengamos que decidir si la moneda está o no sesgada. Una moneda sesgada sería una que no caiga cara con probabilidad 0.5. Por lo tanto una forma de evaluar el sesgo es comparar el valor de referencia 0.5 contra el intervalo HPD. En la figura anterior, podemos ver que el HPD va de \\(\\approx 0.02\\) a \\(\\approx 0.71\\) y, por lo tanto, 0.5 está incluido en el HPD. Según el a posterioriri la moneda parece estar sesgada hacia las cecas, pero no podemos descartar por completo el valor de 0.5. Si esta conclusión nos deja sabor a poco entonces tendremos que recopilar más datos para así reducir la varianza del a posteriori o buscar información para definir un a priori más informativo.\n\n3.2.1 ROPE\nEstrictamente la probabilidad de observar el valor exacto de 0.5 es nula, además en la práctica no nos suele interesar tener precisión infinita si no que solemos tener una idea del rango de error que es tolerable o despreciable. Una posibilidad consiste en definir lo que se conoce como región de equivalencia práctica o ROPE (Region Of Practical Equivalence). Podríamos tener buenas razones para considerar que cualquier valor entre 0,45 y 0,55 es prácticamente equivalente a 0.5. No hay reglas generales para definir un ROPE ya que esta es una decisión contexto-dependiente. Para algunos problemas 0.05 podría ser mucho para otros poco, en algunos casos un rango simétrico es útil en otros es una mala idea.\nYa establecido la ROPE podemos usar las siguientes reglas para tomar una decisión:\n\nEl valor de un parámetro es considerado improbable (o rechazado) si la totalidad de la ROPE cae por fuera del HPD 94% del parámetro en cuestión.\nEl valor de un parámetro es aceptado si la ROPE contiene por completo al HPD 94% del parámetro en cuestión.\n\n\nUna ROPE es un intervalo arbitrario que se determina usando conocimiento previo y relevante sobre un tema. Cualquier valor dentro de este inervalo es considera equivalente.\n\nUsando la función plot_posterior de ArviZ, podemos graficar el posterior junto con el HPD y la ROPE.\n\naz.plot_posterior(idata, rope=[0.45, 0.55]);\n\n\n\n\nOtra herramienta que nos puede asistir en la toma de decisiones es comparar el a posteriori con un valor de referencia. La función plot_posterior también nos permite hacer esto:\n\naz.plot_posterior(idata, ref_val=0.5);\n\n\n\n\nEl valor de referencia está indicado con una linea turquesa, junto con la proporción del posterior por debajo y por arriba del valor de referencia.\nPara una discusión más detallada del uso de la ROPE pueden leer el capítulo 12 del gran libro “Doing Bayesian Data Analysis” de John Kruschke. Este capítulo también discute cómo realizar pruebas de hipótesis de forma Bayesiana y los problemas de realizar este tipo de análisis, ya sea de forma Bayesiana o no-Bayesiana.\n\n\n3.2.2 Funciones de perdida\nUna alternativa más formal al uso de las ROPEs son las Funciones de pérdida. Para poder tomar la mejor decisión posible es necesario tener la mejor descripción posible de un problema y luego una evaluación correcta de los costos y beneficios. Bajo el marco Bayesiano lo primero implica obtener una distribución a posteriori, lo segundo se puede conseguir mediante la aplicación de una función de perdida. Una función de perdida es una forma de medir cuan distinta es una estimación respecto del valor real (o de referencia) de un parámetro. Algunos ejemplos comunes son:\n\nLa perdida cuadrática \\((\\theta - \\hat \\theta)^2\\)\nLa perdida absoluta $|- | $\nLa perdida 0-1 \\(I(\\theta \\ne \\hat{\\theta})\\) siendo \\(I\\) la función indicatriz\n\nLa función de perdida (o su inversa) reciben diversos nombres según el campo de aplicación como funciones de costo, funciones objetivo, funciones de fitness (sic), funciones de utilidad, etc.\nEn la práctica generalmente desconocemos el valor correcto de \\(\\theta\\) y a duras penas tendremos un posterior adecuado, por lo tanto lo que se hace es tratar de encontrar el valor de \\(\\hat \\theta\\) que minimice el valor esperado de la función de perdida. Esto implica promediar la función de perdida sobre todo el posterior, promediamos sobre el posterior porque desconocemos el valor de \\(\\theta\\).\nEn el siguiente ejemplo tenemos dos funciones de pérdida. La función absoluta lossf_a y la cuadrática lossf_b. Evaluamos cada una de las funciones para distintos valores de \\(\\hat \\theta\\) sobre una grilla de 500 puntos y encontramos el mínimo.\n\n_, ax = plt.subplots(1)\ngrid = np.linspace(0, 1, 500)\nθ_pos = az.extract(idata, var_names=\"θ\")\nlossf_a = [np.mean(abs(i - θ_pos)) for i in grid]\nlossf_b = [np.mean((i - θ_pos) ** 2) for i in grid]\n\nfor i, (lossf, c) in enumerate(zip([lossf_a, lossf_b], [\"C0\", \"C1\"])):\n    mini = np.argmin(lossf)\n    ax.plot(grid, lossf, c)\n    ax.plot(\n        grid[mini],\n        lossf[mini],\n        \"o\",\n        color=c,\n        label=f\"función de perdida {['a','b'][i]}\",\n    )\n    pos = (np.max(lossf) - np.min(lossf)) * 0.05\n    ax.annotate(f\"{grid[mini]:.2f}\", (grid[mini], lossf[mini] + pos), color=c)\n    ax.set_yticks([])\n    ax.set_xlabel(r\"$\\hat \\theta$\")\n    ax.legend()\n\n\n\n\nLas curvas son similares entre sí e incluso los mínimos son similares, \\(\\hat{\\theta} \\approx 0.31\\) para lossf_a y \\(\\hat{\\theta} \\approx 0.33\\) para lossf_b\nLo que es interesante es que el primer valor se corresponde con la mediana del posterior y el segundo con su media.\n\nnp.median(θ_pos).item(), np.mean(θ_pos).item()\n\n(0.31254373513365896, 0.33241202565639266)\n\n\nSi bien esto no es una prueba formal, espero que haya sido un ejemplo lo suficientemente claro como para ilustrar el mensaje más importante de esta sección:\n\nDiferentes funciones de pérdida se relacionan con diferentes estimaciones puntuales\n\nPor lo tanto, si queremos ser formales al momento de computar una estimación puntual, debemos decidir qué función de costo utilizar. O a la inversa, si elegimos una estimación puntual implícitamente estamos eligiendo una función de pérdida.\nLa ventaja de elegir explícitamente una función de perdida es que podemos ajustarla a las necesidades de un problema particular, en vez de utilizar un criterio predefinido. En muchos casos el costo asociado a una toma de decisión es asimétrico, esto es común en salud pública como sucede con vacunas o con la interrupción voluntaria del embarazo; procedimientos simples, baratos y seguros que previenen una gran cantidad de inconvenientes con un bajo riesgo de complicaciones.\nDado que, en general, el a posteriori toma la forma de muestras finitas almacenadas en una computadora, es posible escribir código que refleje funciones de perdidas sin necesidad de estar acotado por la conveniencia matemática o la simplicidad. El siguiente es un ejemplo bastante pavo de esto.\n\nlossf = []\nfor i in grid:\n    f = np.cos(i) * (1 - i) + np.sin(i) * (i)\n    lossf.append(f)\n\nmini = np.argmin(lossf)\nplt.plot(grid, lossf)\nplt.plot(grid[mini], lossf[mini], \"o\")\npos = (np.max(lossf) - np.min(lossf)) * 0.05\nplt.annotate(f\"{grid[mini]:.2f}\", (grid[mini], lossf[mini] + pos))\nplt.yticks([])\nplt.xlabel(r\"$\\hat \\theta$\");\n\n\n\n\nAhora bien, en la práctica no es cierto que todo el mundo elija una estimación puntual porque realmente acuerda, o tiene presente, alguna función de perdida en particular, en general la elección es por conveniencia, o tradición. Se usa la mediana porque es más robusta que la media a valores extremos o se usa la media porque es un concepto familiar y simple de entender, o porque pensamos que tal o cual observable es realmente un promedio de algún fenómeno subyacente (como moléculas golpeándose entre sí o genes interactuando con el ambiente)."
  },
  {
    "objectID": "02_Programación_probabilística.html#modelos-multiparamétricos",
    "href": "02_Programación_probabilística.html#modelos-multiparamétricos",
    "title": "3  Programación probabilista",
    "section": "3.3 Modelos Multiparamétricos",
    "text": "3.3 Modelos Multiparamétricos\nPrácticamente todos los modelos de interés en estadística, son multiparamétricos, es decir modelos con más de un parámetro.\nSuele suceder que no todos los parámetros requeridos para construir un modelo son de interés, supongamos que quisiéramos estimar el valor medio de una distribución Gaussiana, a menos que sepamos el valor real de la desviación estándar, nuestro modelo deberá contener un parámetro para la media y uno para la desviación estándar. Los parámetros que no son de inmediato interés pero son necesarios para definir un modelo de forma completa se llaman nuisance parameters (o parámetro estorbo).\nEn estadística Bayesiana todos los parámetros tienen el mismo estatus, por lo que la diferencia entre nuisance o no nuisance no es fundamental bajo ningún concepto, sino que depende completamente de nuestras preguntas.\nEn principio podría parecer que incorporar parámetros que no nos interesan es un ejercicio de futilidad. Sin embargo, es todo lo contrario, al incorporar estos parámetros permitimos que la incertidumbre que tenemos sobre ellos se propague de forma adecuada a los resultados.\n\n3.3.1 Inferencias lumínicas\nA finales del siglo XIX Simon Newcomb realizó varios experimentos para determinar la velocidad de la luz. En uno de ellos Newcomb midió el tiempo que le tomaba a la luz recorrer 7442 metros.\nA continuación se muestra sus resultados, 66 mediciones.\n\ndatos = np.array([248.28, 248.26, 248.33, 248.24, 248.34, 247.56, 248.27, 248.16,\n                  248.4, 247.98, 248.29, 248.22, 248.24, 248.21, 248.25, 248.3,\n                  248.23, 248.29, 248.31, 248.19, 248.24, 248.2, 248.36, 248.32,\n                  248.36, 248.28, 248.25, 248.21, 248.28, 248.29, 248.37, 248.25,\n                  248.28, 248.26, 248.3, 248.32, 248.36, 248.26, 248.3, 248.22,\n                  248.36, 248.23, 248.27, 248.27, 248.28, 248.27, 248.31, 248.27,\n                  248.26, 248.33, 248.26, 248.32, 248.32, 248.24, 248.39, 248.28,\n                  248.24, 248.25, 248.32, 248.25, 248.29, 248.27, 248.28, 248.29,\n                  248.16, 248.23])\n\nSi graficamos estas medidas veremos que la distribución parece Gaussiana excepto por dos medidas inusualmente bajas.\n\nax = az.plot_kde(datos, rug=True)\nax.set_yticks([]);\n\n\n\n\nPor simplicidad vamos a suponer que los datos siguen una distribución Gaussiana, después de todo es lo que en general se esperaría, en general, al medir una misma cosa varias veces. Una distribución Gaussiana queda definida por dos parámetros, la media y la desviación estándar, como desconocemos estas dos cantidades necesitamos establecer dos a prioris uno para cada parámetro. Un modelo probabilístico razonable sería el siguiente.\n\\[\\begin{align}\n\\mu &\\sim U(l, h) \\\\\n\\sigma &\\sim \\mathcal{HN}(\\sigma_{\\sigma}) \\\\\ny &\\sim \\mathcal{N}(\\mu, \\sigma)\n\\end{align}\\]\nEs decir, \\(\\mu\\) proviene de una distribución uniforme entre los límites \\(l\\) y \\(h\\) y \\(\\sigma\\) proviene de una media-normal (half-normal) con desviación estándar \\(\\sigma_{\\sigma}\\), esta distribución es como una Gaussiana pero restringida al rango \\([0, \\infty]\\). Por último los datos \\(y\\), como dijimos anteriormente, proviene de una distribución normal, especificada por \\(\\mu\\) y \\(\\sigma\\).\nSi desconocemos por completo cuales podrían ser los valores de \\(\\mu\\) y de \\(\\sigma\\), podemos fijar valores para los a prioris que reflejen nuestra ignorancia.\nPara la distribución uniforme una opción podría ser un intervalo con límite inferior de 0 y superior de 1 segundo. El límite inferior de 0 tiene sentido ya que las velocidades no pueden ser negativas, el límite superior de un 1 segundo es un valor elevado en la escala de los datos. Otra posibilidad sería usar los datos como guía por ejemplo \\((l=datos.min() / 100, h=l+datos.min() * 100)\\). De esta forma garantizamos que el a priori contenga el rango de los datos pero que sea mucho más amplio, reflejando que no tenemos demasiado información para fijar un a priori de forma más precisa. Los Bayesianos puristas consideran usar los datos para estimar los a prioris ¡como alta traición! Ojo con las almas de cristal (¡en todo ámbito!).\nBajo ciertas condiciones los a prioris uniformes puede ser problemáticos, tanto desde el punto de vista estadístico como computacional, por lo que se recomienda evitarlos, en general se recomienda evitar a prioris con límites, como la distribución uniforme, a menos que tengamos información confiable sobre esos límites. Por ejemplo sabemos que las probabilidades están restringidas al intervalo [0, 1]. Pero no hay una buena razón para limitar la velocidad de la luz (bueno ¡no la había en los tiempos de Newcomb!).\nEn la siguiente celda podrán ver que he elegido un par de a prioris y hay otros comentados. Comparen cómo corre el modelo con los distintos a prioris, tanto en términos de los resultados como los tiempos y calidad del muestreo.\n\nwith pm.Model() as modelo_g:\n    # los a prioris\n    μ = pm.Uniform(\"μ\", 240, 250)\n    # μ = pm.Normal('μ', 240, 100) # otro a priori alternativo\n    σ = pm.HalfNormal(\"σ\", sigma=1)\n    # σ = pm.HalfNormal('σ', sigma=datos.std() * 100)\n    # el likelihood\n    y = pm.Normal(\"y\", mu=μ, sigma=σ, observed=datos)\n    idata_g = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nComo se puede ver el plot-posterior tiene ahora dos subpaneles, una por cada parámetro. Cada uno se corresponde a una variable marginal del a posteriori que en este caso es bi-dimensional.\n\naz.plot_posterior(idata_g);\n\n\n\n\nLa siguiente figura muestra la distribución a posteriori (que como ya mencionamos en bidimensional), junto con las distribuciones marginales para los parámetros \\(\\mu\\) y \\(\\sigma\\).\n\naz.plot_pair(idata_g, kind=\"kde\", marginals=True);\n\n\n\n\nUna vez computado el a posteriori podemos realizar diversos cálculos a partir de él. Uno de esos cálculos consiste en simular datos (\\(\\tilde{y}\\)). Matemáticamente lo que queremos calcular es:\n\\[\\begin{equation}\np(\\tilde{y} \\,|\\, y) = \\int p(\\tilde{y} \\,|\\, \\theta) \\, p(\\theta \\,|\\, y) \\, d\\theta\n\\end{equation}\\]\ndonde:\n\\(y\\) son los datos observados mientras que \\(\\theta\\) corresponde a los parámetros del modelo.\nSiguiendo el ejemplo de la velocidad de la luz, \\(\\theta\\) corresponde a \\(\\mu\\) y a \\(\\sigma\\). Computacionalmente podemos obtener \\(\\tilde{y}\\) de la siguiente forma:\n\nElegimos una muestra al azar de las generadas por PyMC (un valor para \\(\\mu_i\\) y \\(\\sigma_i\\))\nGeneramos un dato sintético usando el mismo likelihood que usamos en el modelo, en este caso \\(\\tilde{y_i} \\sim N(\\mu_i, \\sigma_i)\\)\nRepetimos 1 y 2 hasta obtener la cantidad requerida de muestras.\n\nUsando PyMC podemos calcular esto llamando a la función sample_ppc. El siguiente código devuelve 100 predicciones cada una de ellas de igual tamaño al de los datos.\n\nppc_g = pm.sample_posterior_predictive(idata_g, model=modelo_g)\n\nSampling: [y]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\nLos datos simulados los podemos usar para compararlos con los datos observados y de esta forma evaluar el modelo. Esto se conoce como prueba predictivas a posteriori, como ya adelantamos algo en el capítulo anterior. En la siguiente gráfica la linea negra corresponde a los datos observados mientras que las lineas azules (semitransparentes) corresponden a datos predichos por el modelo.\n\naz.plot_ppc(ppc_g, num_pp_samples=200);\n\n\n\n\nSegún la gráfica anterior ¿Cuán bueno considerás que es nuestro modelo?\n\n\n3.3.2 Modelos robustos\nUn problema con el modelo anterior es que asume una distribución normal pero tenemos dos puntos que caen muy alejados de los valores medios. Esos puntos podrían estar alejados debido a errores experimentales en la toma de esos dos datos o podría haber un error al registrarlos o al transcribirlos. Si algo de esto sucedió podríamos justificar su eliminación de nuestro conjunto de datos (dejando registro de la eliminación y de las razones por las cuales lo hicimos). Otra opción es usar el rango inter-cuartil (u otro método estadístico) para declarar esos dos puntos como datos aberrantes ¡y desterrarlos de nuestros datos! Otra opción es dejarlos pero utilizar un modelo más robusto a valores alejados de la media.\nUno de los inconvenientes al asumir normalidad, es que la media es muy sensible a valores aberrantes. La razón está en la colas de la Gaussiana, aún cuando las colas se extienden de \\(-\\infty\\) a \\(\\infty\\), la probabilidad de encontrar un valor cae rápidamente a medida que nos alejamos de la media, como se puede apreciar en la siguiente tabla que indica el porcentaje de valores que se encuentra a medida que nos alejamos de la media en unidades de desviación estándar (sd).\n\n\n\nsd\n1\n2\n3\n4\n5\n\n\n\n\n%\n68\n95\n99.7\n99.994\n99.99994\n\n\n\nUna alternativa a la distribución Gaussiana es usar una distribución t de Student, lo interesante de esta distribución es que además de estar definida por una media y una escala (análogo de la desviación estándar) está definida por un parámetro \\(\\nu\\), usualmente llamado grados de libertad, o grados de normalidad, ya que \\(\\nu\\) controla cuan pesadas son las colas de la distribución. Cuando \\(\\nu = 1\\) (la distribución se llama de Cauchy o de Lorentz) las colas son muy pesadas, el 95% de los puntos está entre -12,7 y 12,7, en cambio en una Gaussiana (con desviación estándar 1) esto ocurre entre -1,96 y 1,96. En el límite de \\(\\nu\\) tendiendo a infinito estamos en presencia de una Gaussiana. La distribución t es realmente particular, cuando \\(\\nu <= 1\\) la distribución no tiene media definida y la varianza solo está definida para valores de \\(\\nu > 2\\).\nLa siguiente figura muestra una distribución t de Student para distintos valores de \\(\\nu\\).\n\n_, ax = plt.subplots(figsize=(10, 5))\n\nx_values = np.linspace(-10, 10, 500)\nfor df in [1, 2, 5, 20, np.inf]:\n    ax = pz.StudentT(df, 0, 1).plot_pdf(support=(-7, 7))\n\n\nax.legend(loc=\"center left\", bbox_to_anchor=(0.65, 0.5));\n\n\n\n\nAhora que conocemos la distribución t de Student, podemos usarla en nuestro modelo:\n\\[\\begin{align}\n\\mu &\\sim U(l, h) \\\\\n\\sigma &\\sim \\mathcal{HN}(\\sigma_h) \\\\\n\\nu &\\sim Expon(\\lambda) \\\\\ny &\\sim StudentT(\\mu, \\sigma, \\nu)\n\\end{align}\\]\nEn algunos modelos puede ser buena idea sumar 1 a la distribución exponencial a fin de asegurarse que \\(\\nu \\ge 1\\) . En principio \\(\\nu\\) puede tomar valores de [0, \\(\\infty]\\), pero en mi experiencia valores de \\(\\nu < 1\\) pueden traer problemas durante el muestreo, ya que pueden aparecer valores demasiado alejados de la media (las colas son extremadamente gordas!). Esto puede ocurrir con modelos con datos marcadamente aberrantes, veremos un ejemplo de esto en el capítulo 4.\nGráficamente:\n\n\nwith pm.Model() as modelo_t:\n    # los a prioris\n    μ = pm.Uniform(\"μ\", 240, 250)\n    σ = pm.HalfNormal(\"σ\", sigma=100)\n    ν = pm.Exponential(\"ν\", 1 / 30)\n    # el likelihood\n    y = pm.StudentT(\"y\", mu=μ, sigma=σ, nu=ν, observed=datos)\n    idata_t = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ, ν]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\nComparemos las estimaciones entre ambos modelos\n\naz.summary(idata_g)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      μ\n      248.262\n      0.014\n      248.235\n      248.286\n      0.0\n      0.0\n      4325.0\n      3343.0\n      1.0\n    \n    \n      σ\n      0.109\n      0.010\n      0.092\n      0.128\n      0.0\n      0.0\n      3632.0\n      2723.0\n      1.0\n    \n  \n\n\n\n\n\naz.summary(idata_t)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      μ\n      248.274\n      0.006\n      248.262\n      248.286\n      0.000\n      0.000\n      3774.0\n      2893.0\n      1.0\n    \n    \n      σ\n      0.041\n      0.007\n      0.029\n      0.054\n      0.000\n      0.000\n      2315.0\n      2464.0\n      1.0\n    \n    \n      ν\n      2.578\n      0.885\n      1.156\n      4.200\n      0.018\n      0.012\n      2514.0\n      2517.0\n      1.0\n    \n  \n\n\n\n\nEn este caso, vemos que la estimación de \\(\\mu\\) es muy similar entre los dos modelos, aunque la estimación de \\(\\sigma\\), pasó de ser de ~10 a ~4. Esto es consecuencia de que la distribución t asigna menos peso a los valores alejados de la media que la distribución Gaussiana.\nHagamos un prueba predictiva a posteriori para el nuevo modelo.\n\nppc_t = pm.sample_posterior_predictive(idata_t, model=modelo_t)\n\nSampling: [y]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\n\naz.plot_ppc(ppc_t, num_pp_samples=200)\nplt.xlim(247, 250);\n\n\n\n\n¿Qué conclusión se puede sacar de comparar esta prueba predictiva a posteriori con la anterior?\n\n\n3.3.3 Accidentes mineros\nEste ejemplo está tomado del tutorial de PyMC.\nEl problema es el siguiente, tenemos un registro del número de accidentes en minas de carbón, ubicadas en el Reino Unido, que ocurrieron entre 1851 y 1962 (Jarrett, 1979). Se sospecha que la aplicación de ciertas regulaciones de seguridad tuvo como efecto una disminución en la cantidad de catástrofes. Por lo tanto nos interesa averiguar el año en que la tasa cambió y nos interesa estimar ambas tasas.\nLos datos son los siguientes, por un lado tenemos la variable accidentes que contiene la cantidad de accidentes por año y por el otro la variable años conteniendo el rango de años para los cuales tenemos datos. Si prestan atención verán que accidentes es un arreglo enmascarado (o masked array). Esto es un tipo especial de arreglo de NumPy donde cada elemento del arreglo contiene asociado un valor True o False el cual indica si el elemento debe o no ser usado durante cualquier tipo de operación. En este caso como faltan datos para dos años lo que se ha hecho es marcar esa falta de datos con un valor centinela de -999, esta es la forma de indicarle a PyMC la presencia de datos faltantes, alternativamente se pueden pasar los datos como un dataframe de Pandas conteniendo el valor especial NAN (que es el valor por defecto en Pandas para lidiar con datos faltantes).\nBien, pero para que molestarse con datos faltantes si en general es más fácil eliminarlos. una de las razones es que esto puede conducir a pérdida de información cuando por cada observación tenemos más de una variable o cantidad de interés. Por ejemplo si tenemos 50 sujetos a los que les hemos medido la presión, la temperatura y el ritmo cardíaco, pero sucede que para 4 de ellos no contamos con el datos de la presión (porque alguien se olvidó de medirlo o registrarlo, o porque el tensiómetro se rompió, o por lo que sea). Podemos eliminar esos cuatro sujetos del análisis y perder por lo tanto información sobre la presión y ritmo cardíaco, o podemos usar todos los datos disponibles y además estimar los valores de temperatura faltantes. En el contexto de la estadística Bayesiana los datos faltantes se tratan como un parámetro desconocido del modelo que puede ser estimado.\n\naccidentes = pd.Series([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n                       3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n                       2, 2, 3, 4, 2, 1, 3, np.nan, 2, 1, 1, 1, 1, 3, 0, 0,\n                       1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n                       0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n                       3, 3, 1, np.nan, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n                       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\naños = np.arange(1851, 1962)\n\n\nplt.plot(años, accidentes, \".\")\nplt.ylabel(\"Número de accidentes\")\nplt.xlabel(\"Año\");\n\n\n\n\nPara modelar los accidentes usaremos una distribución de Poisson. Como creemos que la cantidad media de accidentes es distinta antes y después de la introducción de regulaciones de seguridad usaremos dos valores de tasas medias de accidentes (\\(t_0\\) y \\(t_1\\)). Además deberemos estimar un punto de corte (\\(pc\\)) que dividirá los años para los cuales se aplica la tasa de accidentes \\(t_0\\) de los cuales se aplica la tasa \\(t_1\\):\n\\[\\begin{equation}\nA_t \\sim Poisson(tasa)\n\\end{equation}\\]\n\\[\\begin{equation}\ntasa = \\begin{cases}\nt_0, \\text{si } t \\ge pc,\\\\\nt_1, \\text{si } t \\lt pc\n\\end{cases}\n\\end{equation}\\]\nLos a prioris que usaremos serán:\n\\[\\begin{align}\nt_0 \\sim Expon(1) \\\\\nt_1 \\sim Expon(1) \\\\\npc \\sim U(A_0, A_1)\n\\end{align}\\]\nDonde la distribución uniforme es discreta y \\(A_0\\) y \\(A_1\\) corresponden al primer y último año considerado en el análisis respectivamente.\nGráficamente el modelo es:\n\nUna peculiaridad de la implementación de este modelo en PyMC es el uso de la función pm.switch (linea 10). Esta es en realidad una función de PyMC y equivale a un if else de Python. Si el primer argumento es True entonces devuelve el segundo argumento caso contrario el tercer argumento. Como resultado tenemos que tasa es un vector de longitud igual a la de años y cuyos elementos corresponden a una repetición \\(t_0\\) seguida de una repetición \\(t_1\\), la cantidad exacta de repeticiones de \\(t_0\\) y \\(t_1\\) está controlada por la condición \\(pc \\ge\\) años. De esta forma, podemos al muestrear \\(pc\\), modificar que años reciben cual tasa para el cálculo del likelihood.\n\nwith pm.Model() as modelo_cat:\n\n    pc = pm.DiscreteUniform(\"pc\", lower=años.min(), upper=años.max())\n\n    # Priors para las tasas antes y después del cambio.\n    t_0 = pm.Exponential(\"t_0\", 1)\n    t_1 = pm.Exponential(\"t_1\", 1)\n\n    # Asignamos las tasas a los años de acuerdo a pc\n    tasa = pm.Deterministic(\"tasa\", pm.math.switch(pc >= años, t_0, t_1))\n\n    acc = pm.Poisson(\"acc\", tasa, observed=accidentes)\n    idata_cat = pm.sample(1000, random_seed=1791, idata_kwargs={\"log_likelihood\": True})\n\n/home/osvaldo/anaconda3/envs/bayes/lib/python3.10/site-packages/pymc/model.py:1384: RuntimeWarning: invalid value encountered in cast\n  data = convert_observed_data(data).astype(rv_var.dtype)\n/home/osvaldo/anaconda3/envs/bayes/lib/python3.10/site-packages/pymc/model.py:1407: ImputationWarning: Data in acc contains missing values and will be automatically imputed from the sampling distribution.\n  warnings.warn(impute_message, ImputationWarning)\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n>CompoundStep\n>>Metropolis: [pc]\n>>Metropolis: [acc_missing]\n>NUTS: [t_0, t_1]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n\n\n\nidata_cat\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:            (chain: 4, draw: 1000, acc_missing_dim_0: 2,\n                        tasa_dim_0: 111, acc_dim_0: 111)\nCoordinates:\n  * chain              (chain) int64 0 1 2 3\n  * draw               (draw) int64 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * acc_missing_dim_0  (acc_missing_dim_0) int64 0 1\n  * tasa_dim_0         (tasa_dim_0) int64 0 1 2 3 4 5 ... 106 107 108 109 110\n  * acc_dim_0          (acc_dim_0) int64 0 1 2 3 4 5 ... 105 106 107 108 109 110\nData variables:\n    pc                 (chain, draw) int64 1889 1886 1886 ... 1889 1892 1892\n    acc_missing        (chain, draw, acc_missing_dim_0) int64 0 0 0 1 ... 1 3 1\n    t_0                (chain, draw) float64 3.273 3.31 2.694 ... 2.881 2.946\n    t_1                (chain, draw) float64 0.8727 0.8444 ... 1.008 0.8809\n    tasa               (chain, draw, tasa_dim_0) float64 3.273 3.273 ... 0.8809\n    acc                (chain, draw, acc_dim_0) int64 4 5 4 0 1 4 ... 0 0 1 0 1\nAttributes:\n    created_at:                 2023-04-20T21:18:39.548868\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0\n    sampling_time:              2.365055799484253\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 1000acc_missing_dim_0: 2tasa_dim_0: 111acc_dim_0: 111Coordinates: (5)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])acc_missing_dim_0(acc_missing_dim_0)int640 1array([0, 1])tasa_dim_0(tasa_dim_0)int640 1 2 3 4 5 ... 106 107 108 109 110array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110])acc_dim_0(acc_dim_0)int640 1 2 3 4 5 ... 106 107 108 109 110array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110])Data variables: (6)pc(chain, draw)int641889 1886 1886 ... 1889 1892 1892array([[1889, 1886, 1886, ..., 1888, 1887, 1888],\n       [1892, 1891, 1891, ..., 1890, 1888, 1890],\n       [1889, 1889, 1889, ..., 1890, 1890, 1890],\n       [1890, 1889, 1889, ..., 1889, 1892, 1892]])acc_missing(chain, draw, acc_missing_dim_0)int640 0 0 1 0 1 0 0 ... 2 0 2 0 2 1 3 1array([[[0, 0],\n        [0, 1],\n        [0, 1],\n        ...,\n        [3, 1],\n        [3, 1],\n        [3, 1]],\n\n       [[6, 1],\n        [6, 1],\n        [6, 3],\n        ...,\n        [2, 1],\n        [2, 1],\n        [2, 1]],\n\n       [[1, 1],\n        [1, 2],\n        [1, 0],\n        ...,\n        [4, 2],\n        [2, 2],\n        [2, 2]],\n\n       [[1, 0],\n        [1, 0],\n        [1, 0],\n        ...,\n        [2, 0],\n        [2, 1],\n        [3, 1]]])t_0(chain, draw)float643.273 3.31 2.694 ... 2.881 2.946array([[3.27300145, 3.31004382, 2.69385652, ..., 3.361489  , 3.40956827,\n        3.40956827],\n       [3.10592073, 3.36085222, 3.00534019, ..., 2.54307457, 2.51920588,\n        3.5372262 ],\n       [2.71227824, 2.71227824, 3.0856883 , ..., 2.86019813, 2.99960294,\n        2.89937231],\n       [2.86208697, 3.20730475, 2.95804455, ..., 3.36485285, 2.88077941,\n        2.94647035]])t_1(chain, draw)float640.8727 0.8444 ... 1.008 0.8809array([[0.87270139, 0.84441641, 1.0630722 , ..., 1.00885421, 1.00662329,\n        1.00662329],\n       [1.05576985, 0.91125974, 0.85414828, ..., 1.04231662, 1.05555764,\n        0.77176679],\n       [0.89805765, 0.89805765, 0.79840082, ..., 0.6720661 , 0.7872776 ,\n        0.72575752],\n       [0.75190232, 1.20079714, 0.77526223, ..., 0.73696447, 1.00761759,\n        0.88086743]])tasa(chain, draw, tasa_dim_0)float643.273 3.273 3.273 ... 0.8809 0.8809array([[[3.27300145, 3.27300145, 3.27300145, ..., 0.87270139,\n         0.87270139, 0.87270139],\n        [3.31004382, 3.31004382, 3.31004382, ..., 0.84441641,\n         0.84441641, 0.84441641],\n        [2.69385652, 2.69385652, 2.69385652, ..., 1.0630722 ,\n         1.0630722 , 1.0630722 ],\n        ...,\n        [3.361489  , 3.361489  , 3.361489  , ..., 1.00885421,\n         1.00885421, 1.00885421],\n        [3.40956827, 3.40956827, 3.40956827, ..., 1.00662329,\n         1.00662329, 1.00662329],\n        [3.40956827, 3.40956827, 3.40956827, ..., 1.00662329,\n         1.00662329, 1.00662329]],\n\n       [[3.10592073, 3.10592073, 3.10592073, ..., 1.05576985,\n         1.05576985, 1.05576985],\n        [3.36085222, 3.36085222, 3.36085222, ..., 0.91125974,\n         0.91125974, 0.91125974],\n        [3.00534019, 3.00534019, 3.00534019, ..., 0.85414828,\n         0.85414828, 0.85414828],\n...\n        [2.86019813, 2.86019813, 2.86019813, ..., 0.6720661 ,\n         0.6720661 , 0.6720661 ],\n        [2.99960294, 2.99960294, 2.99960294, ..., 0.7872776 ,\n         0.7872776 , 0.7872776 ],\n        [2.89937231, 2.89937231, 2.89937231, ..., 0.72575752,\n         0.72575752, 0.72575752]],\n\n       [[2.86208697, 2.86208697, 2.86208697, ..., 0.75190232,\n         0.75190232, 0.75190232],\n        [3.20730475, 3.20730475, 3.20730475, ..., 1.20079714,\n         1.20079714, 1.20079714],\n        [2.95804455, 2.95804455, 2.95804455, ..., 0.77526223,\n         0.77526223, 0.77526223],\n        ...,\n        [3.36485285, 3.36485285, 3.36485285, ..., 0.73696447,\n         0.73696447, 0.73696447],\n        [2.88077941, 2.88077941, 2.88077941, ..., 1.00761759,\n         1.00761759, 1.00761759],\n        [2.94647035, 2.94647035, 2.94647035, ..., 0.88086743,\n         0.88086743, 0.88086743]]])acc(chain, draw, acc_dim_0)int644 5 4 0 1 4 3 4 ... 0 0 1 0 0 1 0 1array([[[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]],\n\n       [[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]],\n\n       [[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]],\n\n       [[4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        ...,\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1],\n        [4, 5, 4, ..., 1, 0, 1]]])Indexes: (5)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))acc_missing_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='acc_missing_dim_0'))tasa_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n      dtype='int64', name='tasa_dim_0', length=111))acc_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n      dtype='int64', name='acc_dim_0', length=111))Attributes: (6)created_at :2023-04-20T21:18:39.548868arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0sampling_time :2.365055799484253tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:             (chain: 4, draw: 1000, acc_observed_dim_0: 109)\nCoordinates:\n  * chain               (chain) int64 0 1 2 3\n  * draw                (draw) int64 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\n  * acc_observed_dim_0  (acc_observed_dim_0) int64 0 1 2 3 4 ... 105 106 107 108\nData variables:\n    acc_observed        (chain, draw, acc_observed_dim_0) float64 -1.708 ... ...\nAttributes:\n    created_at:                 2023-04-20T21:18:39.746180\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0xarray.DatasetDimensions:chain: 4draw: 1000acc_observed_dim_0: 109Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])acc_observed_dim_0(acc_observed_dim_0)int640 1 2 3 4 5 ... 104 105 106 107 108array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108])Data variables: (1)acc_observed(chain, draw, acc_observed_dim_0)float64-1.708 -2.132 ... -0.8809 -1.008array([[[-1.70822553, -2.131956  , -1.70822553, ..., -1.00886322,\n         -0.87270139, -1.00886322],\n        [-1.70025194, -2.11272842, -1.70025194, ..., -1.01352594,\n         -0.84441641, -1.01352594],\n        [-1.90801508, -2.52647918, -1.90801508, ..., -1.00190918,\n         -1.0630722 , -1.00190918],\n        ...,\n        [-1.69000671, -2.08706059, -1.69000671, ..., -1.00003897,\n         -1.00885421, -1.00003897],\n        [-1.6812794 , -2.06413163, -1.6812794 , ..., -1.00002184,\n         -1.00662329, -1.00002184],\n        [-1.6812794 , -2.06413163, -1.6812794 , ..., -1.00002184,\n         -1.00662329, -1.00002184]],\n\n       [[-1.75073375, -2.22686146, -1.75073375, ..., -1.00149963,\n         -1.05576985, -1.00149963],\n        [-1.69012774, -2.08737107, -1.69012774, ..., -1.00418705,\n         -0.91125974, -1.00418705],\n        [-1.78183094, -2.29087809, -1.78183094, ..., -1.01179875,\n         -0.85414828, -1.01179875],\n...\n        [-1.83468837, -2.39323538, -1.83468837, ..., -1.06946468,\n         -0.6720661 , -1.06946468],\n        [-1.78373707, -2.29469505, -1.78373707, ..., -1.02645196,\n         -0.7872776 , -1.02645196],\n        [-1.81944907, -2.36439271, -1.81944907, ..., -1.04629684,\n         -0.72575752, -1.04629684]],\n\n       [[-1.83393653, -2.39182337, -1.83393653, ..., -1.03705118,\n         -0.75190232, -1.03705118],\n        [-1.72363481, -2.16764178, -1.72363481, ..., -1.01781152,\n         -1.20079714, -1.01781152],\n        [-1.79798468, -2.32289417, -1.79798468, ..., -1.02981618,\n         -0.77526223, -1.02981618],\n        ...,\n        [-1.68936975, -2.08542343, -1.68936975, ..., -1.04218007,\n         -0.73696447, -1.04218007],\n        [-1.8265897 , -2.37796672, -1.8265897 , ..., -1.00002887,\n         -1.00761759, -1.00002887],\n        [-1.80209233, -2.33092228, -1.80209233, ..., -1.00771557,\n         -0.88086743, -1.00771557]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))acc_observed_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n        99, 100, 101, 102, 103, 104, 105, 106, 107, 108],\n      dtype='int64', name='acc_observed_dim_0', length=109))Attributes: (4)created_at :2023-04-20T21:18:39.746180arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:                (chain: 4, draw: 1000, scaling_dim_0: 2,\n                            accept_dim_0: 2, accepted_dim_0: 2)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\n  * scaling_dim_0          (scaling_dim_0) int64 0 1\n  * accept_dim_0           (accept_dim_0) int64 0 1\n  * accepted_dim_0         (accepted_dim_0) int64 0 1\nData variables: (12/20)\n    scaling                (chain, draw, scaling_dim_0) float64 2.358 ... 2.585\n    reached_max_treedepth  (chain, draw) bool False False False ... False False\n    max_energy_error       (chain, draw) float64 0.265 0.09046 ... 0.4624\n    perf_counter_start     (chain, draw) float64 2.601e+04 ... 2.602e+04\n    energy                 (chain, draw) float64 176.4 176.6 ... 178.5 178.2\n    index_in_trajectory    (chain, draw) int64 -3 1 -2 -1 -2 -1 ... 0 2 2 2 2 -3\n    ...                     ...\n    n_steps                (chain, draw) float64 3.0 1.0 3.0 1.0 ... 3.0 3.0 3.0\n    accept                 (chain, draw, accept_dim_0) float64 0.007396 ... 0...\n    accepted               (chain, draw, accepted_dim_0) float64 0.0 0.0 ... 0.5\n    energy_error           (chain, draw) float64 -0.06509 0.09046 ... -0.104\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    step_size              (chain, draw) float64 0.8418 0.8418 ... 1.204 1.204\nAttributes:\n    created_at:                 2023-04-20T21:18:39.558212\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0\n    sampling_time:              2.365055799484253\n    tuning_steps:               1000xarray.DatasetDimensions:chain: 4draw: 1000scaling_dim_0: 2accept_dim_0: 2accepted_dim_0: 2Coordinates: (5)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])scaling_dim_0(scaling_dim_0)int640 1array([0, 1])accept_dim_0(accept_dim_0)int640 1array([0, 1])accepted_dim_0(accepted_dim_0)int640 1array([0, 1])Data variables: (20)scaling(chain, draw, scaling_dim_0)float642.358 2.438 2.358 ... 2.144 2.585array([[[2.35794769, 2.43845855],\n        [2.35794769, 2.43845855],\n        [2.35794769, 2.43845855],\n        ...,\n        [2.35794769, 2.43845855],\n        [2.35794769, 2.43845855],\n        [2.35794769, 2.43845855]],\n\n       [[1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855],\n        ...,\n        [1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855],\n        [1.9487171 , 2.58486855]],\n\n       [[2.9282    , 2.30535855],\n        [2.9282    , 2.30535855],\n        [2.9282    , 2.30535855],\n        ...,\n        [2.9282    , 2.30535855],\n        [2.9282    , 2.30535855],\n        [2.9282    , 2.30535855]],\n\n       [[2.14358881, 2.58486855],\n        [2.14358881, 2.58486855],\n        [2.14358881, 2.58486855],\n        ...,\n        [2.14358881, 2.58486855],\n        [2.14358881, 2.58486855],\n        [2.14358881, 2.58486855]]])reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])max_energy_error(chain, draw)float640.265 0.09046 ... -0.6745 0.4624array([[ 0.26504584,  0.09046142,  0.46798465, ...,  0.22325168,\n        -0.07519011,  1.3507017 ],\n       [-0.11731527,  0.6444133 ,  0.17705949, ..., -0.4767042 ,\n         0.07733244, -0.50792413],\n       [-0.46983184,  1.33427506,  0.41174189, ...,  1.70651699,\n        -0.76661906, -0.39112682],\n       [ 0.32258973,  0.35653055,  1.51618935, ...,  0.38004319,\n        -0.67445668,  0.46237617]])perf_counter_start(chain, draw)float642.601e+04 2.601e+04 ... 2.602e+04array([[26014.91047292, 26014.91148382, 26014.91232575, ...,\n        26015.87911283, 26015.87987525, 26015.88066041],\n       [26015.06253877, 26015.06348128, 26015.06439578, ...,\n        26016.17831508, 26016.17899803, 26016.17949004],\n       [26014.83830859, 26014.83938188, 26014.84018346, ...,\n        26015.825392  , 26015.826208  , 26015.82687544],\n       [26015.02727908, 26015.02858552, 26015.02973348, ...,\n        26016.12544131, 26016.12618101, 26016.12687694]])energy(chain, draw)float64176.4 176.6 178.5 ... 178.5 178.2array([[176.36555001, 176.6459368 , 178.51243729, ..., 179.84406587,\n        178.07559104, 181.37051764],\n       [178.78937655, 178.97206313, 179.92765349, ..., 178.60148204,\n        180.77395892, 179.25473734],\n       [177.06746079, 180.29252714, 177.8214689 , ..., 182.01800491,\n        179.75269857, 179.16297612],\n       [179.09410751, 178.91538598, 180.7228587 , ..., 178.93246036,\n        178.51951985, 178.20431289]])index_in_trajectory(chain, draw)int64-3 1 -2 -1 -2 -1 0 ... 0 2 2 2 2 -3array([[-3,  1, -2, ...,  2, -1,  0],\n       [-3, -2,  2, ..., -3, -1,  3],\n       [ 2,  0, -3, ..., -3,  1, -1],\n       [-3,  3, -3, ...,  2,  2, -3]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])lp(chain, draw)float64-175.4 -176.6 ... -176.9 -176.6array([[-175.41139297, -176.57599061, -177.89154262, ..., -179.02823666,\n        -177.99848681, -179.18144757],\n       [-178.65935626, -177.29025153, -179.37600142, ..., -178.44921053,\n        -180.18844401, -178.28848647],\n       [-176.46011085, -177.26077905, -175.90058188, ..., -180.86232685,\n        -177.73456282, -179.01067105],\n       [-177.68364881, -178.00208278, -176.31468939, ..., -178.20585816,\n        -176.94953554, -176.61512549]])step_size_bar(chain, draw)float641.007 1.007 1.007 ... 1.172 1.172array([[1.00729023, 1.00729023, 1.00729023, ..., 1.00729023, 1.00729023,\n        1.00729023],\n       [1.08286555, 1.08286555, 1.08286555, ..., 1.08286555, 1.08286555,\n        1.08286555],\n       [1.05223362, 1.05223362, 1.05223362, ..., 1.05223362, 1.05223362,\n        1.05223362],\n       [1.1719586 , 1.1719586 , 1.1719586 , ..., 1.1719586 , 1.1719586 ,\n        1.1719586 ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])process_time_diff(chain, draw)float640.0005549 0.0002336 ... 0.0003363array([[0.00055489, 0.00023365, 0.00047276, ..., 0.00038232, 0.0004018 ,\n        0.00020246],\n       [0.00049923, 0.00044608, 0.00043372, ..., 0.00033792, 0.00017472,\n        0.00037314],\n       [0.00054747, 0.00021153, 0.00051525, ..., 0.00040727, 0.00022437,\n        0.00045394],\n       [0.00067074, 0.00051242, 0.00061125, ..., 0.00037821, 0.00034662,\n        0.00033631]])perf_counter_diff(chain, draw)float640.0005654 0.0002338 ... 0.0003364array([[0.00056544, 0.00023377, 0.00047288, ..., 0.0003825 , 0.00040203,\n        0.00020259],\n       [0.0004992 , 0.00044613, 0.00043394, ..., 0.00033805, 0.00017483,\n        0.00037316],\n       [0.00054733, 0.0002119 , 0.00051491, ..., 0.00040753, 0.00022451,\n        0.00045401],\n       [0.00067041, 0.00051257, 0.00061126, ..., 0.00037832, 0.00034671,\n        0.00033643]])acceptance_rate(chain, draw)float640.9064 0.9135 0.7718 ... 1.0 0.8124array([[0.90639157, 0.91350957, 0.77182768, ..., 0.88353586, 0.98583535,\n        0.25905842],\n       [0.98955886, 0.76850148, 0.91835521, ..., 0.98134166, 0.9255821 ,\n        1.        ],\n       [1.        , 0.26334902, 0.82588129, ..., 0.45805148, 1.        ,\n        0.89739528],\n       [0.83775282, 0.81221993, 0.61178363, ..., 0.83165062, 1.        ,\n        0.81235333]])tree_depth(chain, draw)int642 1 2 1 2 2 2 2 ... 2 2 1 2 2 2 2 2array([[2, 1, 2, ..., 2, 2, 1],\n       [2, 2, 2, ..., 2, 1, 2],\n       [2, 1, 2, ..., 2, 1, 2],\n       [2, 2, 2, ..., 2, 2, 2]])n_steps(chain, draw)float643.0 1.0 3.0 1.0 ... 3.0 3.0 3.0 3.0array([[3., 1., 3., ..., 3., 3., 1.],\n       [3., 3., 3., ..., 3., 1., 3.],\n       [3., 1., 3., ..., 3., 1., 3.],\n       [3., 3., 3., ..., 3., 3., 3.]])accept(chain, draw, accept_dim_0)float640.007396 0.0 ... 0.1929 0.732array([[[7.39612819e-03, 0.00000000e+00],\n        [4.81757005e-01, 4.36350696e-01],\n        [1.00000000e+00, 0.00000000e+00],\n        ...,\n        [3.02994122e-02, 5.00000000e-01],\n        [3.15524455e+00, 3.78320328e-01],\n        [3.06370307e-01, 1.47077949e-01]],\n\n       [[3.74914457e-01, 7.26775219e+00],\n        [2.64087657e+00, 5.42411823e-02],\n        [1.01373740e-01, 8.18567118e-02],\n        ...,\n        [1.09606993e+00, 1.23133365e+00],\n        [2.32672273e-01, 5.45267664e-01],\n        [4.14571869e+00, 5.28322671e-01]],\n\n       [[1.00000000e+00, 5.22871476e-01],\n        [8.72268475e-02, 3.25327855e-01],\n        [4.92180078e-01, 2.47982802e+00],\n        ...,\n        [2.13478920e-01, 2.97234574e-01],\n        [6.59186005e-03, 7.33430580e-01],\n        [7.07448678e-04, 4.06632834e-03]],\n\n       [[2.13928968e-01, 1.49531966e+00],\n        [2.16730557e+00, 5.00000000e-01],\n        [1.00000000e+00, 5.00000000e-01],\n        ...,\n        [1.20579091e+00, 6.69148835e-01],\n        [7.47765382e-01, 3.68482237e-01],\n        [1.92939365e-01, 7.32034299e-01]]])accepted(chain, draw, accepted_dim_0)float640.0 0.0 1.0 0.5 ... 1.0 0.5 0.0 0.5array([[[0. , 0. ],\n        [1. , 0.5],\n        [1. , 0. ],\n        ...,\n        [1. , 0.5],\n        [1. , 0. ],\n        [1. , 0. ]],\n\n       [[1. , 1. ],\n        [1. , 0. ],\n        [0. , 0.5],\n        ...,\n        [1. , 0.5],\n        [1. , 0.5],\n        [1. , 0. ]],\n\n       [[1. , 0.5],\n        [0. , 0.5],\n        [0. , 1. ],\n        ...,\n        [0. , 0.5],\n        [0. , 0.5],\n        [0. , 0. ]],\n\n       [[1. , 0.5],\n        [1. , 0.5],\n        [1. , 0.5],\n        ...,\n        [1. , 0.5],\n        [1. , 0.5],\n        [0. , 0.5]]])energy_error(chain, draw)float64-0.06509 0.09046 ... -0.594 -0.104array([[-6.50865282e-02,  9.04614228e-02,  4.67984653e-01, ...,\n        -6.90994557e-02,  4.34232260e-02,  0.00000000e+00],\n       [ 3.18244988e-02, -6.90837604e-02,  3.89116988e-02, ...,\n         5.76026427e-02,  7.73324387e-02, -1.49912676e-01],\n       [-2.39403787e-01,  0.00000000e+00, -1.80410173e-01, ...,\n         1.15200873e+00, -7.66619061e-01,  3.67900783e-01],\n       [ 1.26164191e-01,  3.56530549e-01, -8.28935220e-05, ...,\n         3.80043189e-01, -5.94038760e-01, -1.04008828e-01]])largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])step_size(chain, draw)float640.8418 0.8418 ... 1.204 1.204array([[0.84177244, 0.84177244, 0.84177244, ..., 0.84177244, 0.84177244,\n        0.84177244],\n       [1.09823017, 1.09823017, 1.09823017, ..., 1.09823017, 1.09823017,\n        1.09823017],\n       [1.09244341, 1.09244341, 1.09244341, ..., 1.09244341, 1.09244341,\n        1.09244341],\n       [1.2040221 , 1.2040221 , 1.2040221 , ..., 1.2040221 , 1.2040221 ,\n        1.2040221 ]])Indexes: (5)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))scaling_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='scaling_dim_0'))accept_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='accept_dim_0'))accepted_dim_0PandasIndexPandasIndex(Index([0, 1], dtype='int64', name='accepted_dim_0'))Attributes: (6)created_at :2023-04-20T21:18:39.558212arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0sampling_time :2.365055799484253tuning_steps :1000\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:             (acc_observed_dim_0: 109)\nCoordinates:\n  * acc_observed_dim_0  (acc_observed_dim_0) int64 0 1 2 3 4 ... 105 106 107 108\nData variables:\n    acc_observed        (acc_observed_dim_0) int64 4 5 4 0 1 4 3 ... 1 0 0 1 0 1\nAttributes:\n    created_at:                 2023-04-20T21:18:39.562843\n    arviz_version:              0.15.1\n    inference_library:          pymc\n    inference_library_version:  5.3.0xarray.DatasetDimensions:acc_observed_dim_0: 109Coordinates: (1)acc_observed_dim_0(acc_observed_dim_0)int640 1 2 3 4 5 ... 104 105 106 107 108array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108])Data variables: (1)acc_observed(acc_observed_dim_0)int644 5 4 0 1 4 3 4 ... 0 0 1 0 0 1 0 1array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3,\n       1, 4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 1, 1, 1, 1,\n       3, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, 0,\n       1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 2, 1, 1, 1, 1, 2,\n       4, 2, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])Indexes: (1)acc_observed_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n        99, 100, 101, 102, 103, 104, 105, 106, 107, 108],\n      dtype='int64', name='acc_observed_dim_0', length=109))Attributes: (4)created_at :2023-04-20T21:18:39.562843arviz_version :0.15.1inference_library :pymcinference_library_version :5.3.0\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\nax = az.plot_posterior(idata_cat, var_names=[\"~tasa\", \"~acc\"], figsize=(12, 6));\n\n\n\n\n\naz.summary(idata_cat, var_names=[\"~tasa\", \"~acc\"])\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      pc\n      1889.971\n      2.430\n      1885.000\n      1894.000\n      0.168\n      0.119\n      224.0\n      232.0\n      1.01\n    \n    \n      acc_missing[0]\n      2.379\n      1.879\n      0.000\n      6.000\n      0.097\n      0.069\n      364.0\n      467.0\n      1.01\n    \n    \n      acc_missing[1]\n      0.931\n      0.983\n      0.000\n      3.000\n      0.038\n      0.028\n      704.0\n      827.0\n      1.00\n    \n    \n      t_0\n      3.080\n      0.286\n      2.558\n      3.625\n      0.006\n      0.004\n      2130.0\n      2528.0\n      1.00\n    \n    \n      t_1\n      0.929\n      0.118\n      0.712\n      1.151\n      0.002\n      0.002\n      3090.0\n      2802.0\n      1.00\n    \n  \n\n\n\n\n\ntasa_mean = idata_cat.posterior[\"tasa\"].mean((\"chain\", \"draw\"))\ntasa_hdi = az.hdi(idata_cat.posterior[\"tasa\"].values)\npc_hdi = az.hdi(idata_cat.posterior[\"pc\"])[\"pc\"]\n\n_, ax = plt.subplots(figsize=(10, 5), sharey=True)\nax.plot(años, accidentes, \".\")\n\nax.set_ylabel(\"Número de accidentes\")\nax.set_xlabel(\"Año\")\n\nax.vlines(\n    idata_cat.posterior[\"pc\"].mean((\"chain\", \"draw\")),\n    accidentes.min(),\n    accidentes.max(),\n    color=\"C1\",\n    lw=2,\n)\n\n\nax.fill_betweenx(\n    [accidentes.min(), accidentes.max()], pc_hdi[0], pc_hdi[1], alpha=0.3, color=\"C1\"\n)\nax.plot(años, tasa_mean, \"k\", lw=2)\nax.fill_between(años, tasa_hdi[:, 0], tasa_hdi[:, 1], alpha=0.3, color=\"k\")\n\nfaltante0 = (\n    idata_cat.posterior[\"acc_missing\"].sel(acc_missing_dim_0=0).mean((\"chain\", \"draw\"))\n)\nfaltante1 = (\n    idata_cat.posterior[\"acc_missing\"].sel(acc_missing_dim_0=1).mean((\"chain\", \"draw\"))\n)\n\nax.plot(años[np.isnan(accidentes)], [faltante0, faltante1], \"C2s\");"
  },
  {
    "objectID": "02_Programación_probabilística.html#pruebas-predictivas-a-posteriori",
    "href": "02_Programación_probabilística.html#pruebas-predictivas-a-posteriori",
    "title": "3  Programación probabilista",
    "section": "3.4 Pruebas predictivas a posteriori",
    "text": "3.4 Pruebas predictivas a posteriori\nLa prueba consiste en comparar los datos observados con los datos predichos a partir del a posteriori.\nLas pruebas predictivas a posteriori son pruebas de auto-consistencia. Este ejercicio nos permite evaluar si el modelo es razonable, la idea general no es determinar si un modelo es correcto o no ya que como dijo George Box “todos los modelos están equivocados, pero algunos son útiles”. El grado de confianza en la verosimilitud de los modelos ciertamente es distinta entre practicantes de distintas disciplinas científicas, en disciplinas como física cuando se estudian sistemas relativamente simples bajo condiciones experimentales extremadamente controladas y haciendo uso de teorías fuertes, es probable que se le asigne un alto grado de confianza a ciertos modelos. Pero esto no suele ser cierto en disciplinas como ciencias sociales o biología (aunque sospecho que la variabilidad encontrada en biología ¡es muy alta!). En el caso de contar con a prioris muy informativos la evaluación de un modelo también puede ser usado para evaluar si los propios datos son razonables, indicando que tal vez sea necesario conseguir nuevos datos o revisar como se obtuvieron los datos o como se procesaron.\nEn definitiva la principal utilidad de las pruebas predictivas a posteriori debería ser el permitirnos dar una segunda mirada, crítica, al modelo y tratar de entender la razón de discrepancias sistemáticas (si las hubiera), estas discrepancias nos pueden llevar a entender mejor los límites del modelo, abandonar el modelo por completo o tal vez mejorarlo.\nSi bien se han desarrollado métodos formales o cuantitativos para realizar pruebas predictivas a posteriori, una aproximación que suele ser más informativa y simple de interpretar es realizar gráficas, como veremos a continuación.\nUsando PyMC podemos calcular la distribución predictiva a posteriori de la siguiente forma\n\nidata_cat.extend(\n    pm.sample_posterior_predictive(idata_cat, model=modelo_cat, random_seed=1791)\n)\n\nSampling: [acc_observed]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\nSi bien es posible construir nuestras propias pruebas predictivas a posteriori, a continuación usaremos dos funciones de ArviZ.\n\n_, ax = plt.subplots(1, 2, figsize=(12, 4))\naz.plot_ppc(idata_cat, ax=ax[0])\nax[0].set_xlabel(\"acc\")\naz.plot_loo_pit(idata_cat, \"acc_observed\", ax=ax[1], use_hdi=True)\nax[1].set_yticks([]);\n\n\n\n\n\naz.plot_ppc: Por defecto esta función representa los datos observados, varias muestras de la distribución predictiva a posteriori (predicciones) y la distribución media de estas muestras. Si los datos son discretos se usan histogramas, si los datos son continuos KDEs.\naz.plot_loo_pit: Muestra la diferencia entre datos observados y predichos (linea azul), de tal forma que si no hubiera diferencia obtendríamos una distribución uniforme (linea blanca). En el eje x están los cuantiles de la distribución. Por lo que si hubiera diferencia alrededor de 0.5 esto implica diferencia alrededor de la mediana, si en cambio la diferencia estuviera entre 0 y 0.2 esto implicaría diferencias en la cola izquierda (primer 20% de la masa total de la distribución), etc. Si la curva está por encima de la linea blanca esto quiere decir que hay más observaciones que predicciones en esa región, y si la curva está por debajo lo contrario. El gráfico muestra un banda, que indica las desviaciones esperadas respecto de la distribución uniforme para el tamaño de muestra dado. Cualquier diferencia dentro de esa banda es “esperable”."
  },
  {
    "objectID": "02_Programación_probabilística.html#pruebas-predictivas-a-priori",
    "href": "02_Programación_probabilística.html#pruebas-predictivas-a-priori",
    "title": "3  Programación probabilista",
    "section": "3.5 Pruebas predictivas a priori",
    "text": "3.5 Pruebas predictivas a priori\nLas pruebas predictivas a prior son una forma de evaluar el modelo. Una vez definido un modelo Bayesiano se generan muestras a partir del mismo, pero sin condicionar en los datos \\(\\tilde{y}\\), es decir se calcula la distribución posible de datos (sintéticos) sin haber visto los datos reales.\n\\[\np(y^\\ast) =  \\int_{\\Theta} p(y^\\ast \\mid \\theta) \\; p(\\theta) \\; d\\theta\n\\]\nLos datos generados son predictivos ya que son los datos que el modelo esperara ver, es decir son datos no observados pero potencialmente observables. La prueba consiste en comparar los datos observados con el conocimiento previo que tenemos sobre el problema, ojo que NO se trata de comparar con los datos observados!\nLas pruebas predictivas a priori son pruebas de consistencia con nuestro conocimiento previo. Este ejercicio nos permite evaluar si el modelo es razonable, en el sentido de si es capaz de generar datos que concuerdan con lo que sabemos de un problema. Por ejemplo un modelo del tamaño de planetas no es muy razonable si predice planetas de escala nanométrica o incluso de unos pocos kilómetros. Es importante destacar que dado suficiente cantidad y calidad de datos un modelo de este tipo podría dar resultados razonables, una vez condicionado a esos datos. Es decir el posterior podría no incluir, o asignar probabilidades despreciables a nanoplanetas.\nEn definitiva la principal utilidad de las pruebas predictivas a prior es la de permitirnos inspeccionar críticamente un modelo y tratar de entender el comportamiento del modelo las discrepancias con el conocimiento previo nos pueden llevarnos a entender mejor los límites del modelo, abandonar el modelo por completo o tal vez mejorarlo por ejemplo usando priors más angostos u otros likelihoods.\nUsando PyMC podemos calcular la distribución predictiva a priori de la siguiente forma\n\nidata_cat.extend(pm.sample_prior_predictive(model=modelo_cat, random_seed=1791))\n\nSampling: [acc_missing, acc_observed, pc, t_0, t_1]\n\n\n\n_, ax = plt.subplots(2, 2, figsize=(10, 6), sharey=\"row\", sharex=\"col\")\n\nax[0, 0].plot(\n    años[np.isfinite(accidentes)],\n    idata_cat.prior_predictive[\"acc_observed\"].sel(draw=50).squeeze(\"chain\").T,\n    \".\",\n)\na_sample = idata_cat.prior.sel(draw=50)\ncoco = np.full_like(años, a_sample[\"t_1\"].item(), dtype=float)\ncoco[a_sample[\"pc\"] >= años] = a_sample[\"t_0\"].item()\nax[0, 0].step(años, coco)\nax[0, 0].set_ylabel(\"número de accidentes\")\n\naz.plot_dist(\n    idata_cat.prior_predictive[\"acc_observed\"].sel(draw=50), ax=ax[0, 1], rotated=True\n)\n\n\nax[1, 0].plot(\n    años[np.isfinite(accidentes)],\n    idata_cat.prior_predictive[\"acc_observed\"].squeeze(\"chain\").T,\n    \"C0.\",\n    alpha=0.05,\n)\nax[1, 0].set_ylabel(\"número de accidentes\")\nax[1, 0].set_xlabel(\"años\")\n\naz.plot_dist(idata_cat.prior_predictive[\"acc_observed\"], ax=ax[1, 1], rotated=True)\nax[1, 1].set_xlabel(\"probabilidad\");\n\n\n\n\nLa primer fila de la figura anterior muestra una muestra de la distribución predictiva a priori. A la izquierda el número de accidentes por año (puntos azules). Y la tasa media en turquesa mostrando un valor de \\(\\approx 0.3\\) antes de 1880 y 1.3 con posterioridad a esa fecha. A la derecha un histograma de la cantidad de accidentes.\nLa segunda fila muestra lo mismo pero agregado para las 500 muestras que le pedimos a PyMC. Se ve una distribución de accidentes uniforme a lo largo de los años, esto es esperable dado que hemos definido el mismo prior para ambas tasas. Además, podemos ver que nuestro modelo favorece valores relativamente bajos de accidentes por año con el 85% de la masa para valores iguales o menores a 3."
  },
  {
    "objectID": "02_Programación_probabilística.html#comparando-grupos",
    "href": "02_Programación_probabilística.html#comparando-grupos",
    "title": "3  Programación probabilista",
    "section": "3.6 Comparando grupos",
    "text": "3.6 Comparando grupos\nUna tarea común al analizar datos es comparar grupos. Podríamos estar interesados en analizar los resultados de un ensayo clínico donde se busca medir la efectividad de una droga, o la reducción de la cantidad de accidentes de tránsito al introducir un cambio en las regulaciones de tránsito, o el desempeño de estudiantes bajo diferentes aproximaciones pedagógicas, etc. Este tipo de preguntas se suele resolver en el marco de lo que se conoce como pruebas de hipótesis que busca declarar si una observación es estadísticamente significativa o no. Nosotros tomaremos una ruta alternativa.\nAl comparar grupos debemos decidir que característica(s) vamos a usar. Una característica común es la media de cada grupo. En ese caso podemos calcular la distribución a posteriori de la diferencia entre medias. Para ayudarnos a entender este posterior usaremos 3 herramientas:\n\nUn posteriorplot con un valor de referencia\nUna medida llamada d de Cohen\nLa probabilidad de superioridad\n\nEn el capítulo anterior ya vimos un ejemplo de cómo usar posteriorplot con un valor de referencia, pronto veremos otro ejemplo. Las novedades aquí son el d de Cohen y la probabilidad de superioridad, dos maneras populares de expresar el tamaño del efecto.\n\n3.6.1 d de Cohen\nUna medida muy común, al menos en ciertas disciplinas, para cuantificar el tamaño del efecto es el d de Cohen\n\\[\n\\frac{\\mu_2 - \\mu_1}{\\sqrt{\\frac{\\sigma_2^2 + \\sigma_1^2}{2}}}\n\\]\nDe acuerdo con esta expresión, el tamaño del efecto es la diferencia de las medias con respecto a la desviación estándar combinada de ambos grupos. Ya que es posible obtener una distribución a posteriori de medias y de desviaciones estándar, también es posible calcular una distribución a posteriori de los valores d de Cohen. Por supuesto, si sólo necesitamos o queremos una estimación puntual, podríamos calcular la media de esa distribución a posteriori. En general, al calcular una desviación estándar combinada, se toma en cuenta el tamaño de la muestra de cada grupo explícitamente, pero la ecuación de d de Cohen omite el tamaño de la muestra, la razón es que tomamos estos valores del posterior (por lo que ya estamos considerando la incertidumbre de las desviaciones estándar).\n\nUn d de Cohen es una forma de medir el tamaño del efecto donde la diferencia de las medias se estandariza al considerar las desviaciones estándar de ambos grupos.\n\nCohen introduce la variabilidad de cada grupo al usar sus desviaciones estándar. Esto es realmente importante, una diferencia de 1 cuando la desviación estándar es de 0.1 es muy grande en comparación con la misma diferencia cuando la desviación estándar es 10. Además, un cambio de x unidades de un grupo respecto del otro podría explicarse por cada punto desplazándose exactamente x unidades o la mitad de los puntos sin cambiar mientras la otra mitad cambia 2x unidades, y así con otras combinaciones. Por lo tanto, incluir las variaciones intrínsecas de los grupos es una forma de poner las diferencias en contexto. Re-escalar (estandarizar) las diferencias nos ayuda a dar sentido a la diferencia entre grupos y facilita evaluar si el cambio es importante, incluso cuando no estamos muy familiarizados con la escala utilizada para las mediciones.\nUn d de Cohen se puede interpretar como un Z-score. Un Z-score es la cantidad de desviaciones estándar que un valor difiere del valor medio de lo que se está observando o midiendo, puede ser positivo o negativo dependiendo de si la diferencia es por exceso o por defecto. Por lo tanto, un d de Cohen de -1.2, indica que la media de un grupo está 1.2 desviación estándar por debajo de la media del otro grupo.\nIncluso con las diferencias de medias estandarizadas, puede ser necesario tener que calibrarnos en función del contexto de un problema determinado para poder decir si un valor de d de Cohen es grande, pequeño, mediano, importante, despreciable, etc. Afortunadamente, esta calibración se puede adquirir con la práctica, a modo de ejemplo si estamos acostumbrados a realizar varios análisis para más o menos el mismo tipo de problemas, podemos acostumbrarnos a un d de Cohen de entre 0.8 y 1.2, de modo que si obtenemos un valor de 2 podría ser que estamos frente a algo importante, inusual (¡o un error!). Una alternativa es consultar con expertos en el tema.\nUna muy buena página web para explorar cómo se ven los diferentes valores de Cohen’s es http://rpsychologist.com/d3/cohend. En esa página, también encontrarán otras formas de expresar el tamaño del efecto; algunas de ellos podrían ser más intuitivas, como la probabilidad de superioridad que analizaremos a continuación.\n\n\n3.6.2 Probabilidad de superioridad\nEsta es otra forma de informar el tamaño del efecto y se define como la probabilidad que un dato tomado al azar de un grupo tenga un valor mayor que un punto tomado al azar del otro grupo. Si suponemos que los datos que estamos utilizando se distribuyen de forma Gaussiana, podemos calcular la probabilidad de superioridad a partir de la d de Cohen usando la expresión:\n\\[\\begin{equation} \\label{eq_ps}\nps = \\Phi \\left ( \\frac{\\delta}{\\sqrt{2}} \\right)\n\\end{equation}\\]\nDonde \\(\\Phi\\) es la distribución normal acumulada y \\(\\delta\\) es el d de Cohen. Podemos calcular una estimación puntual de la probabilidad de superioridad (lo que generalmente se informa) o podemos calcular la distribución a posteriori. Si no estamos de acuerdo con la suposición de normalidad, podemos descartar esta fórmula y calcularla directamente a partir del posterior sin necesidad de asumir ninguna distribución. Esta es una de las ventajas de usar métodos de muestreo para estimar el a posteriori, una vez obtenidas las muestras lo que podemos hacer con ellas es muy flexible.\n\n\n3.6.3 El conjunto de datos tips\nPara explorar el tema de esta sección, vamos a usar el conjunto de datos tips (propinas). Estos datos fueron informados por primera vez por Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics.\nQueremos estudiar el efecto del día de la semana sobre la cantidad de propinas en un restaurante. Para este ejemplo, los diferentes grupos son los días. Comencemos el análisis cargando el conjunto de datos como un DataFrame de Pandas usando solo una línea de código. Si no está familiarizado con Pandas, el comando tail se usa para mostrar las últimas filas de un DataFrame:\n\ntips = pd.read_csv(\"datos/propinas.csv\")\ntips.tail()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      239\n      29.03\n      5.92\n      Male\n      No\n      Sat\n      Dinner\n      3\n    \n    \n      240\n      27.18\n      2.00\n      Female\n      Yes\n      Sat\n      Dinner\n      2\n    \n    \n      241\n      22.67\n      2.00\n      Male\n      Yes\n      Sat\n      Dinner\n      2\n    \n    \n      242\n      17.82\n      1.75\n      Male\n      No\n      Sat\n      Dinner\n      2\n    \n    \n      243\n      18.78\n      3.00\n      Female\n      No\n      Thur\n      Dinner\n      2\n    \n  \n\n\n\n\nPara este ejemplo solo vamos a usar las columnas day y tip y vamos a usar la función plot_forest de ArviZ. Aún cuando ArviZ está pensado para análisis de modelos Bayesianos algunos de sus funciones pueden ser útiles para analizar datos.\n\naz.plot_forest(\n    tips.pivot(columns=\"day\", values=\"tip\").to_dict(\"list\"),\n    kind=\"ridgeplot\",\n    hdi_prob=1,\n    figsize=(12, 4),\n);\n\n\n\n\nA fin de simplificar el análisis vamos a crear 2 variables: * La variable categories contiene los nombres de los días (abreviados y en inglés) * La variable idx codifica los días de la semana como enteros entre 0 y 3.\n\ncategories = np.array([\"Thur\", \"Fri\", \"Sat\", \"Sun\"])\n\ntip = tips[\"tip\"].values\nidx = pd.Categorical(tips[\"day\"], categories=categories).codes\n\nEl modelo para este problema es basicamente igual a model_g, con la diferencia que \\(\\mu\\) y \\(\\sigma\\) ahora serán vectores en vez de escalares. La sintáxis de PyMC es super-útil para estos caso, en vez de usar for loops escribimos el modelo de forma vectorizada, para ello especificamos el argumento shape para los priors \\(\\mu\\) y \\(\\sigma\\) y para el likelihood usamos la variable idx para indexar de forma adecuada \\(\\mu\\) y \\(\\sigma\\) para asegurar que usamos los parámetros correctos para cada grupo. En este ejemplo un \\(\\mu\\) para jueves, otra para viernes, otra para sábado y una cuarta para domingo, y lo mismo para \\(\\sigma\\).\n    with pm.Model() as comparing_groups:\n        μ = pm.Normal('μ', mu=0, sigma=10, shape=4)\n        σ = pm.HalfNormal('σ', sigma=10, shape=4)\n\n        y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=tip)\nPyMC provee una sintaxis alternativa, la cual consisten en especificar coordenadas y dimensiones. La ventaja de esta alternativa es que permite una mejor integración con ArviZ.\nVeamos, en este ejemplo tenemos 4 valores para las medias y 4 para las desviaciones estándar, y por eso usamos shape=4. El InferenceData tendrá 4 indices 0, 1, 2, 3 correspondientes a cada uno de los 4 días. Pero es trabajo del usuario asociar esos indices numéricos con los días.\nAl usar coordenadas y dimensiones nosotros podremos usar los rótulos 'Thur', 'Fri', 'Sat', 'Sun' para referirnos a los parámetros relacionados con cada uno de estos días. ArviZ también podrá hacer uso de estos rótulos. Vamos a especificar dos coordenadas days con las dimensiones 'Thur', 'Fri', 'Sat', 'Sun' y “days_flat” que contendrá los mismo rótulos pero repetidos según el orden y longitud que corresponda con cada observación. Esto último será útil para poder obtener pruebas predictivas a posteriori para cada día.\n\ncoords = {\"days\": categories, \"days_flat\": categories[idx]}\n\nwith pm.Model(coords=coords) as comparing_groups:\n    μ = pm.HalfNormal(\"μ\", sigma=5, dims=\"days\")\n    σ = pm.HalfNormal(\"σ\", sigma=1, dims=\"days\")\n\n    y = pm.Gamma(\"y\", mu=μ[idx], sigma=σ[idx], observed=tip, dims=\"days_flat\")\n\n    idata_cg = pm.sample()\n    idata_cg.extend(pm.sample_posterior_predictive(idata_cg))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\nUna vez obtenido un a posteriori podemos hacer todos los análisis que creamos pertinentes con el. Primero hagamos una prueba predictiva a posteriori. Vemos que en general somos capaces de capturar la forma general de las distribuciones, pero hay detalles que se nos escapan. Esto puede deberse al tamaño relativamente pequeño de la muestra, a que hay otros factores además del día que tienen influencia en las propinas o una combinación de ambas. Por ahora seguiremos con el análisis considerando que el modelo es lo suficientemente bueno\n\n_, axes = plt.subplots(2, 2)\naz.plot_ppc(\n    idata_cg,\n    num_pp_samples=100,\n    coords={\"days_flat\": [categories]},\n    flatten=[],\n    ax=axes,\n);\n\n\n\n\nPodemos ver la distribución de cada uno de los parámetros haciendo\n\naz.plot_posterior(idata_cg, var_names=\"μ\", figsize=(12, 3));\n\n\n\n\nLa figura anterior es bastante informativa, por ejemplo vemos que los valores medios de las propinas difieren en solo unos pocos centavos y que para los domingos el valor es ligeramente más alto que para el resto de los días analizados.\nPero quizá consideramos que puede ser mejor mostrar los datos de otra forma. Por ejemplo podemos calcular todas las diferencias de medias a posteriori entre si. Además podríamos querer usar alguna medida del tamaño del efecto que sea popular entre nuestra audiencia, como podrían ser la probabilidad de superioridad o d de Cohen.\nCohen’s d\n\\[\n\\frac{\\mu_2 - \\mu_1}{\\sqrt{\\frac{\\sigma_1^2 + \\sigma_2^2}{2}}}\n\\]\n\nSe puede interpretar como un z-score. Cuántas desviaciones estándar una media de un grupo está por encima (o por debajo) de la media del otro grupo\nEjemplo interactivo\n\nProbabilidad de superioridad\n\nLa probabilidad que un dato tomado de un grupo sea mayor que la de un dato tomado del otro grupo.\nSi suponemos que los datos se distribuyen normalmente, entonces:\n\n\\[\n\\text{ps} = \\Phi \\left ( \\frac{\\delta}{\\sqrt{2}} \\right)\n\\]\n\\(\\Phi\\) es la cdf de una distribución normal \\(\\delta\\) es el valor del Cohen’s d.\nCon el siguiente código usamos plot_posterior para graficar todas las diferencias no triviales o redundantes. Es decir evitamos las diferencias de un día con sigo mismo y evitamos calcular ‘Fri - Thur’ si ya hemos calculado ‘Thur- Fri’. Si lo viéramos como una matriz de diferencias solo estaríamos calculando la porción triangular superior.\n\ncg_posterior = az.extract(idata_cg)\n\ndist = pz.Normal(0, 1)\n\ncomparisons = [(categories[i], categories[j]) for i in range(4) for j in range(i+1, 4)]\n\n_, axes = plt.subplots(3, 2, figsize=(13, 9), sharex=True)\n\nfor (i, j), ax in zip(comparisons, axes.ravel()):\n    means_diff = cg_posterior[\"μ\"].sel(days=i) - cg_posterior['μ'].sel(days=j)\n    \n    d_cohen = (means_diff /\n               np.sqrt((cg_posterior[\"σ\"].sel(days=i)**2 + \n                        cg_posterior[\"σ\"].sel(days=j)**2) / 2)\n              ).mean().item()\n    \n    ps = dist.cdf(d_cohen/(2**0.5))\n    az.plot_posterior(means_diff.values, ref_val=0, ax=ax)\n    ax.set_title(f\"{i} - {j}\")\n    ax.plot(0, label=f\"Cohen's d = {d_cohen:.2f}\\nProb sup = {ps:.2f}\", alpha=0)\n    ax.legend(loc=1)\n\n\n\n\nUna forma de interpretar estos resultados es comparando el valor de referencia con el intervalo HDI. De acuerdo con la figura anterior, tenemos solo un caso cuando el 94% HDI excluye el valor de referencia de cero, la diferencia en las propinas entre el jueves y el domingo. Para todos los demás ejemplos, no podemos descartar una diferencia de cero (de acuerdo con los criterios de superposición de valores de referencia de HDI). Pero incluso para ese caso, ¿es una diferencia promedio de ≈0.5 dólares lo suficientemente grande? ¿Es suficiente esa diferencia para aceptar trabajar el domingo y perder la oportunidad de pasar tiempo con familiares o amigos? ¿Es suficiente esa diferencia para justificar promediar las propinas durante los cuatro días y dar a cada mozo/a la misma cantidad de dinero de propina? Este tipo de preguntas es crucial para interpretar los datos y/o tomar decisiones, pero las respuestas no las puede ofrecer la estadística de forma automática (ni ningún otro procedimiento). La estadística solo pueden ayudar en la interpretación y/o toma de decisiones.\nNota: Dependiendo del público el gráfico anterior puede que esté demasiado “cargado”, quizá es útil para una discusión dentro del equipo de trabajo, pero para un público en general quizá convenga sacar elementos o repartir la información entre una figura y una tabla o dos figuras."
  },
  {
    "objectID": "02_Programación_probabilística.html#resumen",
    "href": "02_Programación_probabilística.html#resumen",
    "title": "3  Programación probabilista",
    "section": "3.7 Resumen",
    "text": "3.7 Resumen\nAunque la estadística Bayesiana es conceptualmente simple, los modelos probabilísticos a menudo conducen a expresiones analíticamente intratables. Durante muchos años, esta fue una gran barrera que obstaculizó la adopción amplia de métodos Bayesianos. Afortunadamente, la matemática, la física y la informática vinieron al rescate en forma de métodos numéricos capaces, al menos en principio, de resolver cualquier inferencia. La posibilidad de automatizar el proceso de inferencia ha llevado al desarrollo de los lenguajes de programación probabilista que permiten una clara separación entre la definición del modelo y la inferencia.\nPyMC es una librería de Python para programación probabilística con una sintaxis simple, intuitiva y fácil de leer que también está muy cerca de la sintaxis estadística utilizada para describir modelos probabilísticos. En este capítulo introducimos PyMC revisando el problema de la moneda que vimos en el capítulo anterior. La diferencia es que no tuvimos que derivar analíticamente la distribución a posteriori. Los modelos en PyMC se definen dentro de un bloque with; para agregar una distribución de probabilidad a un modelo, solo necesitamos escribir una línea de código. Las distribuciones se pueden combinar y se pueden usar como priors (variables no observadas) o likelihoods (variables observadas). En la sintaxis de PyMC la única diferencia entre ambas es que para esta última debemos pasar los datos usando el argumento observed. Si todo va bien las muestras generadas por PyMC serán representativas de la distribución a posteriori y por lo tanto serán una representación de las consecuencias lógicas del modelo y los datos.\nArviZ es una librería que nos ayuda a explorar los modelos definidos por PyMC (u otras librerías como PyStan, TFP, BeanMachine, etc). Una forma de usar el posterior para ayudarnos a tomar decisiones es comparando la ROPE con el intervalo HDI. También mencionamos brevemente la noción de funciones de pérdida, una aproximación formal para cuantificar los costos y beneficios asociados a la toma de decisiones. Aprendimos que las funciones de pérdida y las estimaciones puntuales están íntimamente asociadas.\nHasta este momento todos los ejemplos estuvieron basado en modelos con un solo parámetro. Sin embargo PyMC permite, en principiop, usar un número arbitrario de parámetros, esto lo ejemplificamos con un modelo Gaussiano y luego una generalización de este, el modelo t de Student. La distribución t de Student suele usarse como alternativa a la Gaussiana cuando queremos hacer inferencias robustas a valores aberrantes. Pronto veremos cómo se puede usar estos modelos como para construir regresiones lineales.\nFinalizamos comparando medias entre grupos, una tarea común en análisis de datos. Si bien esto a veces se enmarca en el contexto de las pruebas de hipótesis, tomamos otra ruta y trabajamos este problema como una inferencia del tamaño del efecto."
  },
  {
    "objectID": "02_Programación_probabilística.html#para-seguir-leyendo",
    "href": "02_Programación_probabilística.html#para-seguir-leyendo",
    "title": "3  Programación probabilista",
    "section": "3.8 Para seguir leyendo",
    "text": "3.8 Para seguir leyendo\n\nLa documentación de PyMC tiene varios ejemplos de como usar este librería y modelos de distinto tipo.\nProbabilistic Programming and Bayesian Methods for Hackers de Cameron Davidson-Pilon y varios otros contribuidores. Originalmente escrito en PyMC2 ha sido portado a PyMC\nWhile My MCMC Gently Samples. Un blog de Thomas Wiecki, desarrollador de PyMC.\nStatistical Rethinking by Richard McElreath es probablemente el mejor libro introductorio de estadística Bayesiana. El libro usa R/Stan. Pero varias personas hemos contribuido para portar el código a Python/PyMC\nDoing Bayesian Data Analysis de John K. Kruschke es otro libro introductorio bastante accesible. La mayoría de los ejemplos de la primer edición están disponibles en Python/PyMC y de la segunda edición acá."
  },
  {
    "objectID": "02_Programación_probabilística.html#ejercicios",
    "href": "02_Programación_probabilística.html#ejercicios",
    "title": "3  Programación probabilista",
    "section": "3.9 Ejercicios",
    "text": "3.9 Ejercicios\n\nUsando PyMC reproducí los resultados del primer capítulo para el problema de la moneda (use los 3 priors usados en ese capítulo).\nReemplazá la distribución beta por una uniforme en el intervalo [0, 1] ¿Cómo cambia la velocidad del muestreo? ¿Y si se usas un intervalo más ámplio, como [-3, 3]?\nPara el modelo_g. Usá una Gaussiana para la media, centrada en la media empírica. Probá modificar la desviación estándard de ese prior ¿Cuán robusto/sensible son los resultados a la elección del prior?\nLa Gaussiana es una distribución sin límites es decir es válida en el intervalo \\([-\\infty, \\infty]\\), en el ejemplo anterior la usamos para modelar datos que sabemos tienen límites ¿Qué opinas de esta elección?\nUsando los datos de la velocidad de la luz, calculá la media y desviación estándar con y sin los outilers, compará esos valores con los obtenidos con el modelo_g y con el modelo_t.\nModificá el modelo de las propinas para usar una distribución t de Student, probá usando un solo \\(\\nu\\) para los cuatro grupos y también usando un valor de \\(\\nu\\) por grupo.\nCalculá la probabilidad de superioridad a partir de las muestras del posterior (sin usar la formula de probabilidad de superioridad a partir de la d de Cohen). Comparar los resultados con los valores obtenidos a analíticamente.\nAplica al menos uno de los modelos visto en este capítulo a datos propios o de tu interés."
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#modelos-jerárquicos",
    "href": "03_Modelos_jerárquicos.html#modelos-jerárquicos",
    "title": "4  Modelado Jerárquico",
    "section": "4.1 Modelos Jerárquicos",
    "text": "4.1 Modelos Jerárquicos\nEl siguiente ejemplo está tomado del capítulo 9 del libro “Doing Bayesian Data Analysis” de John K. Kruschke. Supongamos que en vez de 1 moneda tenemos 3, supongamos además que sabemos que las tres monedas fueron echas con la misma matriz (en la misma fábrica). Para estimar el valor de \\(\\theta\\) tenemos dos opciones:\n\nestimar un valor de \\(\\theta\\) para cada moneda por separado.\njuntar las tres monedas en un mismo conjunto de datos y calcular un solo valor de \\(\\theta\\)\n\nLa ventaja de la opción 1 es que las monedas podrían diferir entre sí por lo que calcular 3 valores de \\(\\theta\\) podría ser muy informativo. La desventaja de este modelo es que hace caso omiso a la información que indica que las 3 monedas tienen un origen común, por lo que es probable que compartan características.\nLa ventaja de la opción 2 es que la cantidad de datos por parámetro aumentó, lo que reduce la incerteza. El problema es que pasamos a asumir que las 3 monedas son en realidad una, lo cual no sería problemático si las tres monedas fueran muy similares entre sí, pero esto podría no ser una buena aproximación.\nUna tercera opción es hacer algo a mitad de camino entre 1 y 2. Esto se consigue construyendo un modelo jerárquico o modelo multinivel. Este tipo de modelo nos permitirá estimar un valor de \\(\\theta\\) para cada moneda de forma tal que la estimación de cada valor de \\(\\theta\\) influencie al resto.\nEn estadística Bayesiana construir modelos jerárquicos es sencillo. A continuación veremos que un modelo jerárquico para las 3 monedas es muy similar al usado para el caso de 1 sola moneda, solo que ahora colocamos un a priori ¡sobre el a priori!\nRecordemos, el modelo del capítulo anterior era:\n\\[\\begin{align}\n\\theta &\\sim \\operatorname{Beta}(\\alpha, \\beta) \\\\\ny &\\sim \\operatorname{Bin}(n=1, p=\\theta)\n\\end{align}\\]\nEn un modelo jerárquico los argumentos de la distribución Beta (\\(\\alpha\\) y \\(\\beta\\)) no son constantes si no que son valores que proviene de alguna otra distribución. En nuestro modelo tendremos que:\n\\[\\begin{align}\n\\mu &\\sim \\operatorname{Beta}(\\alpha, \\beta) \\\\\n\\nu &\\sim \\operatorname{Gamma}(s, r) \\\\\n\\theta &\\sim \\operatorname{Beta}(\\mu, \\nu) \\\\\ny &\\sim \\operatorname{Bin}(n=1, p=\\theta)\n\\end{align}\\]\nGráficamente, tenemos:\n\nEn los modelos jerárquicos a los parámetros \\(\\mu\\) y a \\(\\kappa\\) se los llama hiper a prioris o hiperparámetros ya que son ellos quienes determinan el valor del a priori. La diferencia entre el modelo del capítulo anterior y el del presente es que ahora los valores que puede tomar \\(\\theta\\) dependen no ya de una distribución fija, como podría ser \\(\\text{Beta}(\\alpha=1\\) y \\(\\beta=1)\\), si no de una distribución que depende de los valores de \\(\\mu\\) y \\(\\kappa\\), y que estimaremos a partir de los datos. Es decir es posible estimar el a priori a partir de los datos, pero solo por que hemos introducido hiper a prioris.\nRecordarán que la distribución Beta se podía parametrizar en términos de \\(\\alpha\\) y \\(\\beta\\), pero también de \\(\\mu\\) y \\(\\kappa\\), donde \\(\\mu\\) es la media y \\(\\kappa\\) es la concentración (la inversa de la dispersión). Tenemos entonces que \\(\\mu\\) reflejará el valor promedio de 3 valores de \\(\\theta\\) y que si la proporción de caras en las tres monedas es similar entre si \\(\\kappa\\) tomara un valor más alto; mientras que si las monedas son diferentes entre si \\(\\kappa\\) tomará un valor más bajo.\n\n4.1.1 ¿Por qué la elección de los hiper a prioris?\nBueno dado que \\(\\mu\\) es la media del vector \\(\\theta\\) (y que \\(\\theta\\) solo puede tomar valores entre 0 y 1), \\(\\mu\\) queda restringida a valores entre 0 y 1 (al igual que una distribución beta), siguiendo el mismo razonamiento \\(\\kappa\\) va entre \\([0, \\infty]\\) al igual que la distribución gamma. Otras distribuciones igualmente razonables podrían haber sido:\n\n\\(\\mu \\sim U(0, 1)\\)\n\\(\\kappa \\sim \\mathcal{HN}(\\sigma=100)\\)\n\nPrimero que nada generemos algunos datos sintéticos y los pondremos de una forma que sea más simple pasárselos al modelo, esto quedará un poco más claro al la especificación del modelo.\nVamos a suponer que con cada una de las 3 monedas hicimos 10 experimentos de Bernoulli (las arrojamos al aire) y obtuvimos como resultado, para cada caso, 5 caras.\n\nN =  np.array([10, 10, 10])  # Número de experimentos por moneda\nz =  np.array([5, 5, 5]) # np.array([1, 5, 9])  # Número de caras en los Ni experimentos.\n\n# vector conteniendo los índices para cada moneda (desde 0 al número de monedas)\nmonedas = np.repeat(np.arange(len(N)), N)\n# vector con 1 para caras y 0 para cecas\ndatos = np.hstack([np.repeat([1, 0], [z[i], N[i]-z[i]]) for i in range(len(N))])\n\nComo no sabemos demasiado sobre \\(\\mu\\) y \\(\\kappa\\), vamos a elegir \\(\\mu \\sim \\operatorname{Beta}(\\alpha=2, \\beta=2)\\), lo que equivale a una distribución centrada en 0.5, pero que casi asigna la misma probabilidad a todos los valores entre 0 y 1. Y \\(\\kappa = \\operatorname{Gamma}(\\alpha=1, \\beta=0.1)\\), lo que equivale a una exponencial con media y desviación estándar de 10.\nLa especificación del modelo es igual a lo que hemos venido haciendo, la única diferencia es que en la linea 8 podemos observar que hay un argumento llamando shape. Esto nos permite especificar las dimensiones de (en este caso) \\(\\theta\\). PyMC permite escribir modelos vectorizados ahorrándonos el tener que escribir for loops. Esa es la razón por la cual en la celda superior creamos un vector monedas que usamos en la linea 10 (de la especificación del modelo) para indexar \\(\\theta\\).\n\nwith pm.Model() as modelo_j:\n    # definimos los hiperparámetros\n    μ = pm.Beta('μ', alpha=2, beta=2)\n    κ = pm.Gamma('κ', alpha=1, beta=0.1)\n    #κ = pm.Gamma('κ', mu=10, sd=10)\n    \n    # definimos el a priori\n    θ = pm.Beta('θ', alpha=μ * κ, beta=(1 - μ) * κ, shape=len(N))\n\n    # definimos el likelihood\n    y = pm.Bernoulli('y', p=θ[monedas], observed=datos)\n\n    # muestreamos\n    idata_j = pm.sample(2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, κ, θ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 5 seconds.\n\n\n\naz.plot_posterior(idata_j, figsize=(12, 5));\n\n\n\n\n\naz.summary(idata_j, kind=\"stats\")\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      μ\n      0.501\n      0.106\n      0.302\n      0.697\n    \n    \n      κ\n      15.969\n      11.649\n      0.798\n      36.936\n    \n    \n      θ[0]\n      0.500\n      0.117\n      0.285\n      0.718\n    \n    \n      θ[1]\n      0.502\n      0.117\n      0.295\n      0.735\n    \n    \n      θ[2]\n      0.503\n      0.119\n      0.280\n      0.723\n    \n  \n\n\n\n\nPodemos observar que el valor de \\(\\kappa\\) del a posteriori es mayor que del a priori. Esto es razonable ya que los experimentos con las 3 monedas han resultado idénticos, indicando que la matriz tiene un efecto importante sobre el resultado de \\(\\theta\\) para cada moneda.\n¿Qué distribución hubiéramos obtenido para \\(\\kappa\\) si las monedas hubieran mostrado distintos resultados? Probemos que hubiera pasado si:\nz = [1, 5, 9]\n\n\n4.1.2 Mirando el a posteriori desde varios lados\nEl a posteriori contiene toda la información que resulta de un análisis Bayesiano. Por lo que puede ser muy informativo analizarlo desde varios lados. Además de los gráficos que provee ArviZ como el siguiente:\n\naz.plot_pair(idata_j, marginals=True, figsize=(10, 8), kind=(\"scatter\", \"kde\"),\n             scatter_kwargs={\"alpha\":0.25},\n             point_estimate=\"median\",\n             point_estimate_kwargs={\"color\":\"gray\"},\n             point_estimate_marker_kwargs={\"color\":\"gray\"});\n\n\n\n\nPodemos analizar el a posteriori usando nuestras propias gráficas, como la siguiente.\n\n# Creamos arreglos tomando muestras del posterior\nposterior = az.extract(idata_j)\ntheta1_pos = posterior['θ'][0].values\ntheta2_pos = posterior['θ'][1].values\ntheta3_pos = posterior['θ'][2].values\nmu_pos = posterior['μ'].values\nkappa_pos = posterior['κ'].values\n\n_, ax = plt.subplots(4, 3, figsize=(12, 12))\n\n# Gráficos de dispersión de los hiper-parámetros\nax[0, 0].scatter(mu_pos, kappa_pos, marker='o', alpha=0.01)\nax[0, 0].set_xlim(0,1)\nax[0, 0].set_xlabel(r'$\\mu$')\nax[0, 0].set_ylabel(r'$\\kappa$')\n\naz.plot_posterior(mu_pos, ax=ax[0, 1], round_to=1)\nax[0, 1].set_xlabel(r'$\\mu$')\nax[0, 1].set_xlim(0,1)\n\naz.plot_posterior(kappa_pos, ax=ax[0, 2], round_to=1)\nax[0, 2].set_xlabel(r'$\\kappa$')\ncount = 1\nfor i, j in (theta1_pos, 'theta1'), (theta2_pos, 'theta2'), (theta3_pos, 'theta3'):\n    az.plot_posterior(i, ax=ax[count, 0], round_to=1)\n    ax[count, 0].set_xlabel('$\\{}$'.format(j))\n    ax[count, 0].set_xlim(0,1)\n    countb = 1\n    for k, l in (mu_pos, 'mu'), (kappa_pos, 'kappa'):\n        ax[count, countb].scatter(k, i, marker='o', alpha=0.1)\n        ax[count, countb].set_xlabel('$\\{}$'.format(l))\n        ax[count, countb].set_ylabel('$\\{}$'.format(j), rotation=0)\n        ax[count, countb].set_xlim(0)\n        ax[count, countb].set_ylim(0,1)\n        countb += 1\n    count += 1\n\n\n\n\n\n\n4.1.3 Contracción (shrinking)\nProbemos ahora con otros ejemplos (puede ser conveniente guardar las figuras obtenidas con distintos nombres).\n\nz = [1,1,1]\nz = [9,9,9]\nz = [9,1,9]\n\n¿Cuáles son los valores de \\(\\theta\\) obtenidos en cada caso? Es lo mismo el valor estimado de \\(\\theta\\) para una moneda cuando cae 1 de 10 veces caras (y las otras dos también), que cuando una moneda cae 1 de 10 veces caras y las otras dos caen 9 de 10 veces cara?\nComo podrán ver si hacen el ejercicio ¡el valor estimado \\(\\theta\\) no es el mismo! ¿Por qué sucede esto?\nPorque el modelo especifica que las monedas NO son independientes. El modelo asume que las 3 monedas provienen de una misma matriz, por lo tanto la estimación de \\(\\theta\\) para una moneda es afectada por las otras y al mismo tiempo afecta a las otras. Este fenómeno se llama contracción, la razón del nombre es que las estimaciones individuales tienden a contraerse alrededor del valor promedio de las 3 estimaciones (en nuestro modelo \\(\\mu\\)) esto se hace mas evidente para los valores aberrantes. Si todas las monedas menos una indican un valor de \\(\\theta\\) más o menos similar la que posee el valor distinto tendrá un \\(\\theta\\) mucho más cercano al valor de las demás que si la hubiéramos estimado de forma individual.\nEsto quizá pueda parecerles problemático, pero no es más que un reflejo de lo que asumimos al crear el modelo. La matriz con la que fueron echas las monedas influencia el sesgo de las mismas. Entonces, la estimación de cada elemento del vector \\(\\theta\\) debe influenciar y ser influenciado por las estimaciones de los demás elementos de \\(\\theta\\). Esto es una forma de regularización que los métodos frecuentistas deben introducir ad-hoc, pero que sin embargo ya viene incluido en un análisis Bayesiano.\nEntonces el modelo jerárquico Bayesiano que hemos construido nos dice, no solo los valores de \\(\\theta\\), sino lo valores de \\(\\mu\\) (el sesgo promedio) introducido por la matriz y los valores de \\(\\kappa\\) (cuan fuerte es el efecto de la matriz sobre los sesgos individuales de \\(\\theta\\)).\n\n\n4.1.4 Veamos otro ejemplo\nLas proteínas son moléculas formadas por 20 unidas, llamadas amino ácidos, cada amino ácido puede aparecer en una proteína 0 o más veces. Así como una melodía está definida por una sucesión de notas musicales, una proteína está definida por una sucesión de amino ácidos. Algunas variaciones de notas pueden dar como resultados pequeñas variaciones sobre la misma melodía, otras variaciones pueden resultar en melodías completamente distintas, algo similar sucede con las proteínas. Una forma de estudiar proteínas es usando resonancia magnética nuclear (la misma técnica usada para imágenes médicas). Esta técnica permite medir diversos observables, uno de ellos se llama desplazamiento químico y para simplificar diremos que podemos medir tantos desplazamientos químicos como amino ácidos tenga una proteína. Los aminoácidos son una familia de compuestos químicos por lo que tendría sentido tratarlos a todos de igual forma, pero al mismo tiempo tienen diferentes propiedades químicas, las cuales de hecho son relevantes para comprender como funcionan las proteínas! Por lo que también tiene sentido tratarlos por separado. Como ya vimos una alternativa es construir un modelo jerárquico y hacer algo a mitad de camino.\nEl siguiente conjunto de datos contiene valores de desplazamientos químicos para un conjunto de proteínas. Si inspeccionan el DataFrame cs_data verán que tiene 4 columnas:\n\nLa primera es un código que identifica a la proteína (si tienen curiosidad pueden ingresar el identificador en esta base de datos https://www.rcsb.org).\nLa segunda columna tiene el nombre del amino ácido (pueden corroborar que hay tan solo 20 nombres únicos).\nLa tercera contiene valores téoricos de desplazamientos químicos (calculados usando métodos cuánticos).\nLa cuarta tiene valores experimentales.\n\nLa motivación de este ejemplo es comparar las diferencias entre valores teóricos y experimentales, entre otras razones para evaluar la capacidad de los métodos teóricos para reproducir valores experimentales.\n\ncs_data = pd.read_csv('datos/chemical_shifts_theo_exp.csv')\ndiff = cs_data.theo - cs_data.exp\ncat_encode = pd.Categorical(cs_data['aa'])\nidx = cat_encode.codes\ncoords = {\"aa\": cat_encode.categories}\n\nPara resaltar la diferencia entre un modelo jerárquico y uno no-jerárquico vamos a construir ambos. Primero el no-jerárquico.\n\nwith pm.Model(coords=coords) as cs_nh:         \n    μ = pm.Normal('μ', mu=0, sigma=10, dims=\"aa\") \n    σ = pm.HalfNormal('σ', sigma=10, dims=\"aa\") \n \n    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) \n     \n    idata_cs_nh = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\nY ahora el jerárquico.\nEste modelo tiene un hyper-prior para la media de \\(\\mu\\) y otro para la desviación estándar de \\(\\mu\\). Para \\(\\sigma\\) no usamos un hyper-prior, es decir asumimos valores independientes. Esta es una decisión que tomé para simplificar el modelo, en principio no habría problema con usar un hyper-prior también para \\(\\sigma\\) o incluso estimar un solo valor, compartido, de \\(\\sigma\\).\n\nwith pm.Model(coords=coords) as cs_h:\n    # hyper_priors\n    μ_mu = pm.Normal('μ_mu', mu=0, sigma=10)\n    μ_sd = pm.HalfNormal('μ_sd', 10)\n\n    # priors\n    μ = pm.Normal('μ', mu=μ_mu, sigma=μ_sd, dims=\"aa\") \n    σ = pm.HalfNormal('σ', sigma=10, dims=\"aa\") \n\n    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) \n\n    idata_cs_h = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ_mu, μ_sd, μ, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\nVamos a comparar los resultados usando un plot_forest. ArviZ permite pasar más de un modelo. Esto es útil cuando queremos comparar los valores de parámetros equivalentes entre modelos como en el presente ejemplo. Noten que estamos pasando varios argumentos para obtener el gráfico, como por ejemplo combined=True que combina los resultados de todas las cadenas. Los invito a explorar el significado del resto de los parámetros.\n\naxes = az.plot_forest([idata_cs_nh, idata_cs_h], model_names=['no_jerárquico', 'jerárquico'],\n                      var_names='μ', combined=True, r_hat=False, ess=False, figsize=(10, 7),\n                      colors='cycle')\ny_lims = axes[0].get_ylim()\naxes[0].vlines(idata_cs_h.posterior['μ_mu'].mean(), *y_lims, color=\"k\", ls=\":\");\n\n\n\n\nBien, tenemos un gráfico para 40 valores medios estimados, uno por aminoácido (20) y esto duplicado ya que tenemos dos modelos. También tenemos los intervalos de credibilidad del 94% y el rango intercuartil (el intervalo que contiene el 50% central de la distribución). La línea vertical es la media parcialmente agrupada, es decir la media según el modelo jerárquico. El valor es cercano a cero, esto es parte de lo que esperaríamos ver si los valores teóricos son buenos reproduciendo los valores experimentales.\nLa parte más relevante de este gráfico es que las estimaciones del modelo jerárquico son atraidas hacia la media parcialmente agrupada o, de forma equivalente, se contraen con respecto a las estimaciones no agrupadas. Este efecto es más notorio para los grupos más alejados de la media (como 13), además la incertidumbre es igual o menor que la del modelo no jerárquico. Decimos que las estimaciones están parcialmente agrupadas porque tenemos una estimación para cada grupo, pero las estimaciones para cada grupos se restringen mutuamente mediante el hiper prior. Por lo tanto, se obtiene una situación intermedia entre tener un solo grupo, todos los aminoácidos juntos, y tener 20 grupos separados, uno por aminoácido.\nParafraseando el Zen de Python, podemos decir: hierarchical models are one honking great idea - let’s do more of those!.\nEn los próximos capítulos, seguiremos construyendo modelos jerárquicos y aprendiendo cómo usarlos para construir mejores modelos. También discutiremos cómo se relacionan los modelos jerárquicos con uno de los problemas más comunes en estadística, ciencia de datos y Machine learning el problema del overfitting/underfitting."
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#resumen",
    "href": "03_Modelos_jerárquicos.html#resumen",
    "title": "4  Modelado Jerárquico",
    "section": "4.2 Resumen",
    "text": "4.2 Resumen\nEste es un capítulo muy breve pero describe uno de los conceptos más importantes de este curso: los modelos jerárquicos. Podemos construir modelos jerárquicos cada vez que podamos identificar subgrupos en nuestros datos. En tales casos, en lugar de tratar los subgrupos como entidades separadas o ignorar los subgrupos y tratarlos como un solo gran-grupo, podemos construir un modelo para agrupar-parcialmente la información entre los grupos.\nEl principal efecto de este agrupamiento-parcial es que las estimaciones de cada subgrupo estarán sesgadas por las estimaciones del resto de los subgrupos. Este efecto se conoce como contracción y, en general, es un truco muy útil que ayuda a mejorar las inferencias haciéndolas más conservadoras (ya que cada subgrupo informa a los demás acercando el resto de las estimaciones hacia él) y más informativas, obtenemos estimaciones a nivel de subgrupo y el nivel del grupo."
  },
  {
    "objectID": "03_Modelos_jerárquicos.html#ejercicios",
    "href": "03_Modelos_jerárquicos.html#ejercicios",
    "title": "4  Modelado Jerárquico",
    "section": "4.3 Ejercicios",
    "text": "4.3 Ejercicios\n\nRepetí el ejercicio que hicimos con el model_j, pero sin la estructura jerárquica. Compará los resultados con los obtenidos de forma jerárquica.\nCreá una versión jerárquica para el ejemplo de las propinas agrupando parcialmente los días de la semana.\nAplicá al menos uno de los modelos visto en este capítulo a datos propios o de tu interés."
  },
  {
    "objectID": "04_Diagnóstico_MCMC.html#revisitando-el-teorema-de-bayes",
    "href": "04_Diagnóstico_MCMC.html#revisitando-el-teorema-de-bayes",
    "title": "5  Diagnóstico del muestreo",
    "section": "5.1 Revisitando el teorema de Bayes",
    "text": "5.1 Revisitando el teorema de Bayes\nEl teorema de Bayes, tiene una formulación que a primera vista parece muy inocente. Tan solo cuatro términos relacionados por una multiplicación y una división.\n\\[\n\\underbrace{p(\\boldsymbol{\\theta} \\mid \\boldsymbol{Y})}_{\\text{posterior}} = \\frac{\\overbrace{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})}^{\\text{likelihood}}\\; \\overbrace{p(\\boldsymbol{\\theta})}^{\\text{prior}}}{\\underbrace{{p(\\boldsymbol{Y})}}_{\\text{marginal likelihood}}}\n\\]\nPareciera que no sirve de mucho y que es fácil de calcular. Sin embargo, ambas apreciaciones son incorrectas. El resto de los capítulos se centran en mostrar contra ejemplos a la primera aseveración, así que veamos por que a veces su cálculo puede ser difícil y se requieren métodos numéricos.\nLa razón está en el cálculo del likelihood marginal. El cual toma la forma de una integral.\n\\[\n{p(\\boldsymbol{Y}) = \\int_{\\boldsymbol{\\Theta}} p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})p(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}}\n\\]\nEsta integral suele ser difícil de resolver. Veamos, esta expresión nos dice que debemos evaluar el likelihood para cada uno de los posibles valores del prior \\(\\theta\\). En la práctica esa tarea no siempre es sencilla o barata de realizar. Si \\(\\theta\\) representa un solo parámetro desconocido (como en el modelo beta-binomial) entonces solo hay que resolver una integral, pero si \\(\\theta\\) representa dos parámetros (como en el modelo Gaussiano) entonces la integral será doble. En definitiva la integral tendrá tantas dimensiones como parámetros el modelo. En general las integrales en grandes dimensiones no son simples de resolver.\nAlgo que puede ser poco intuitivo es que esto se contrapone con el cálculo de la distribución a posteriori. Para obtener una buena aproximación a la distribución a posteriori bastaría con concentrarse en las regiones donde tanto la contribución del prior como del likelihood son relativamente grandes (área gris en la siguiente figura), en general esto es lo que hacen la mayoría de los métodos numéricos. En cambio esta misma aproximación puede conducir a errores en el cálculo del likelihood marginal\n\n\n\nPara algunos problemas es posible calcular la distribución a posteriori de forma analítica. Esto ya lo vimos para el modelo beta-binomial donde la posterior es:\n\\[\np(\\theta \\mid y) \\propto \\operatorname{Beta}(\\alpha_{a priori} + y, \\beta_{a priori} + N - y)\n\\]\nPara esos casos suele ser posible también calcular el marginal likelihood de forma analítica.\nPero en general no tenemos expresiones analíticas y entonces debemos confiar en métodos numéricos."
  },
  {
    "objectID": "04_Diagnóstico_MCMC.html#calculando-la-distribución-a-posteriori",
    "href": "04_Diagnóstico_MCMC.html#calculando-la-distribución-a-posteriori",
    "title": "5  Diagnóstico del muestreo",
    "section": "5.2 Calculando la distribución a posteriori",
    "text": "5.2 Calculando la distribución a posteriori\nHay muchas formas de calcular la distribución a posteriori\n\n Conjugación \n Método de Laplace \n Aproximación de Laplace Anidada Integrada (INLA) \n Inferencia Variacional (VI) \nMarkov Chain Monte Carlo (MCMC)\n Sequential Monte Carlo \n…\n\nPor ahora solo hablaremos de los métodos MCMC ya que, por el momento, son los métodos más generales. Pero para entender de forma más simple que es lo que hacen estos métodos conviene empezar desde otro método, conocido como método de la grilla.\n\n5.2.1 Método de la grilla\nEl método de grilla es un enfoque simple de fuerza bruta. La idea central es que incluso si no somos capaces de calcular todo la distribución a posteriori, en general si somos capaces de evaluar el a priori y el likelihood punto-a-punto.\nPara un modelo con un solo parámetro el método de la grilla se puede resumir de la siguiente forma:\n\nEncuentre un intervalo razonable para el parámetro (el prior debe dar algunas pistas).\nDefina una grilla de puntos (generalmente equidistantes) en ese intervalo.\nPara cada punto de la grilla, evalúe el prior y el likelihood en ese punto y multiplique\n\nLa siguiente figura ilustra este método\n\n\n\nEl siguiente bloque de código (que ya usamos antes) implementa un método de la grilla interactivo\n\ndef a_posteriori_grilla(grilla=10, a=1, b=1, caras=6, tiradas=9):\n    grid = np.linspace(0, 1, grilla)\n    prior = pz.Beta(a, b).rv_frozen.pdf(grid)\n    likelihood = pz.Binomial(n=tiradas, p=grid).rv_frozen.pmf(caras)\n    posterior = likelihood * prior\n    posterior /= posterior.sum()\n    _, ax = plt.subplots(1, 3, sharex=True, figsize=(12, 3))\n    ax[0].set_title('caras = {}\\ntiradas = {}'.format(caras, tiradas))\n    for i, (e, e_n) in enumerate(zip([prior, likelihood, posterior], ['a priori', 'likelihood', 'a posteriori'])):\n        ax[i].set_yticks([])\n        ax[i].plot(grid, e, 'o-', label=e_n)\n        ax[i].legend()\n\n\ninteract(a_posteriori_grilla, grilla=ipyw.IntSlider(min=2, max=100, step=1, value=15), a=ipyw.FloatSlider(min=1, max=7, step=1, value=1), b=ipyw.FloatSlider(\n    min=1, max=7, step=1, value=1), caras=ipyw.IntSlider(min=0, max=20, step=1, value=6), tiradas=ipyw.IntSlider(min=0, max=20, step=1, value=9));\n\n\n\n\n\n\n\nUtilizando la función a_posteriori_grilla podemos comprobar que para obtener una mejor aproximación se puede aumentar el número de puntos de la cuadrícula. Esta estrategia puede ser útil en unas pocas dimensiones (parámetros). Pero no escala. En la siguiente figura vemos que si necesitamos 4 puntos en 1D, para mantener ese mismo grado de precisión necesitaremos 16 puntos en 2D y 64 en 3D. La velocidad con la que crecen la cantidad de evaluaciones necesarias crece demasiado rápido, una grilla de 100 en 10 dimensiones requeriría de 1e+20 puntos!\n\n\n\nComo si eso no fuera poco, la cosa es más complicada. En espacios de alta dimensión se dan una serie de fenómemos conocidos como concentración de la medida o en versión marketinera la maldición de la dimensionalidad 👻. Por ejemplo:\n\nEn una hiper-esfera casi todo el volumen está en la superficie. Es decir, si uno pelara una hiper-naranja se quedaría con hambre!\nEn un hiper-cubo la masa se concentra en las esquinas\nEn una Gaussiana hiper-dimensional casi toda la masa está lejos de la moda\n\nLa idea de estimar la distribución a posteriori evaluando, punto a punto, likelihood y prior es muy buena, pero la idea de construir una grilla predefinida solo funciona en muy bajas dimensiones.\nPero no todo está perdido, que tal si mantenemos la idea de la evaluación puntual, pero nos concentramos en las regiones que importan?\n\n\n5.2.2 Markov Chain Monte Carlo (MCMC)\nEsta es una familia muy extensa de métodos utilizados para resolver muchos problemas, entre los que se encuentra el cálculo de la distribución a posteriori. Conceptualmente se puede pensar a estos métodos como generalizaciones del método de la grilla, ya que también se basan en la posibilidad de realizar evaluaciones punto a punto del prior y likelihood. La diferencia crucial es que en vez de utilizar una grilla predefinida el método realiza evaluaciones que progresivamente se concentran en regiones de alta probabilidad. No solo eso si no que eventualmente el método devolverá muestras de forma proporcional a la probabilidad a posteriori. Es decir si una región es 3 veces más probable que otra obtendremos 3 veces más muestras de esa región que de la otra.\nA muy grandes rasgos, y dado un punto inicial arbitrario, los métodos MCMC, constan de dos pasos.\n\nGenerar un nuevo punto a partir de perturbar uno preexistente.\nAceptar o rechazar ese nuevo punto de forma probabilista y comparando la probabilidad del punto preexistente y del nuevo punto.\n\nEsta es esencialmente la receta, la forma exacta en que hacemos cada uno de estos pasos define los distintos métodos dentro de la familia MCMC. Veamos uno de los más sencillos de entender y de implementar.\n\n\n5.2.3 Metropolis-Hastings\nMetropolis-Hastings no es un algoritmo muy moderno o particularmente eficiente, pero Metropolis-Hastings es simple de entender y también proporciona una base para comprender métodos más sofisticados y poderosos.\nEl algoritmo Metropolis-Hasting se define de la siguiente manera:\n\nInicialice el valor del parámetro \\(\\boldsymbol{X}\\) en \\(x_i\\)\nUtilice una distribución de propuesta \\(q(x_{i + 1} \\mid x_i)\\) para generar un nuevo valor \\(x_{i + 1}\\)\nCalcule la probabilidad de aceptar el nuevo valor como:\n\n\\[\np_a (x_{i + 1} \\mid x_i) = \\min \\left(1, \\frac{p(x_{i + 1}) \\; q(x_i \\mid x_{i + 1})} {p(x_i) \\; q (x_{i + 1} \\mid x_i)} \\right)\n\\]\n\nSi \\(p_a > R\\) donde \\(R \\sim \\mathcal{U}(0, 1)\\), guarde el nuevo valor; de lo contrario, guarde el anterior.\nIterar de 2 a 4 hasta que se haya generado una muestra suficientemente grande\n\nEl algoritmo Metropolis es muy general y se puede usar en aplicaciones no Bayesianas, pero para la presente discusión, \\(p(x_i)\\) es la densidad del posterior evaluada en el valor del parámetro \\(x_i\\). Una forma de simplificar un poco el método es notar que si \\(q\\) es una distribución simétrica, los términos \\(q(x_i \\mid x_{i + 1})\\) y \\(q(x_{i + 1} \\mid x_i)\\) se cancelarán (conceptualmente significa que es igualmente probable que vayamos de \\(x_{i+1}\\) a \\(x_i\\) o de \\(x_{i}\\) a \\(x_{i+1}\\)), dejando solo un cociente entre el posterior evaluado en dos puntos. Este algoritmo siempre aceptará moverse de una región de baja probabilidad a una más alta y aceptará probabilísticamente moverse de una región de alta a una baja probabilidad.\n¡Otra observación importante es que el algoritmo Metropolis-Hastings no es un método de optimización! No nos importa encontrar el valor del parámetro con la máxima probabilidad, queremos explorar la distribución \\(p\\). Es decir aún si el método encuentra un máximo aún puede moverse a regiones de probabilidades más bajas.\nPara hacer las cosas más concretas, intentemos resolver el modelo Beta-Binomial.\n\\[\\begin{aligned}\n    \\theta \\sim &\\; \\text{Beta}(\\alpha, \\beta) \\\\\n    Y \\sim &\\; \\text{Bin}(n=1, p=\\theta)\n\\end{aligned}\\]\nEste modelo tiene una solución analítica. Pero supongamos que no sabemos cómo calcular el posterior y, por lo tanto, implementaremos el algoritmo Metropolis-Hastings usando Python.\n\ndef post(θ, Y, α=1, β=1):\n    if 0 <= θ <= 1:\n        prior = stats.beta(α, β).pdf(θ)\n        like  = stats.bernoulli(θ).pmf(Y).prod()\n        prob = like * prior\n    else:\n        prob = -np.inf\n    return prob\n\nTambién necesitamos datos, por lo que generaremos algunos datos falsos aleatorios para este propósito.\n\nY = stats.bernoulli(0.7).rvs(20)\n\nY finalmente ejecutamos nuestra implementación del algoritmo Metropolis-Hastings:\n\nn_iters = 1000\ncan_sd = 0.05\nα = β =  1\nθ = 0.5 \ntrace = {\"θ\":np.zeros(n_iters)}\np2 = post(θ, Y, α, β)\n\nfor iter in range(n_iters):\n    θ_can = stats.norm(θ, can_sd).rvs(1)\n    p1 = post(θ_can, Y, α, β)  \n    pa = p1 / p2\n\n    if pa > stats.uniform(0, 1).rvs(1):\n        θ = θ_can\n        p2 = p1\n\n    trace[\"θ\"][iter] = θ\n\nEn la línea 9 del bloque de código anterior generamos una propuesta muestreando una distribución Normal con desviación estándar can_sd. En la línea 10 evaluamos el posterior en el nuevo valor generado θ_can y en la línea 11 calculamos la probabilidad de aceptación. En la línea 20 guardamos un valor de θ en el array trace. Dependiendo del resultado de la comparación en la línea 13, el valor guardado será nuevo o repetiremos el anterior.\nEl primer panel de la siguiente figura muestra cada valor muestreado en cada paso, y el panel de la derecha el histograma de esos valores. El resultado parece razonable. Nada mal para unas pocas lineas de código!\n\n_, axes = plt.subplots(1,2, sharey=True)\naxes[0].plot(trace['θ'])\naxes[0].set_ylabel('θ', rotation=0, labelpad=15)\naxes[1].hist(trace['θ'], orientation=\"horizontal\", density=True)\naxes[1].set_xticks([]);\n\n\n\n\nAcá pueden ver una versión interactiva de un Metropolis-Hastings\n\n\n5.2.4 MH adaptativo\nEn teoría, y si tomaramos infinitas muestras, cualquier distribución de propuesta sería útil. Sin embargo, en la práctica la eficiencia cambia drásticamente de acuerdoa la distribución de propuesta que utilicemos. Es por ello que para obtener un MH realmente eficiente es necesario ajustar hiperparámetros como la distribución de propuesta para cada problema. Esto se puede hacer dedicando una cierta cantidad de pasos (tuning), estos pasos luego se descartan\n\nAún el RWMH adaptativo puede tener problemas para ciertas problemas\n\nParámetros muy correlacionados\nAlta dimensión (muchos parámetros)\nGeometrías complejas\n\n\nExisten otras formas de generar aún mejores propuestas\n\n\n5.2.5 Montecarlo Hamiltoniano (HMC)\nEn vez de proponer nuevos puntos al azar podemos usar una analogía física. Simulamos una particula sin fricción que se mueve por la distribución a posteriori. Esto se puede hacer si conocemos el Hamiltoniano del sistema. En términos simples, un hamiltoniano es una descripción de la energía total de un sistema físico.\n\\[\n\\underbrace{H(\\overbrace{\\mathbf{q}}^{\\text{posición}}, \\overbrace{\\mathbf{p}}^{\\text{momemtum}})}_{\\text{Hamiltoniano}}  = \\underbrace{K(\\mathbf{p}, \\mathbf{q})}_{\\text{Energía cinética}} + \\underbrace{V(\\mathbf{q})}_{\\text{Energía potencial}}\n\\]\nLa posición \\(q\\) se corresponde con los valores que puedan tomar los parámetros del modelo probabilista y la energía potencial es la probabilidad a posteriori de esos valores. El momentum, en cambio, lo sacamos de la galera. Es simplemente una variable auxiliar que nos permite calcular el hamiltoniano y “mover” el sistema.\nEntonces, a grandes rasgos un HMC tiene dos pasos que se repiten hasta obtener la cantidad de muestras necesarias:\n\nGenerar un nuevo punto a partir del hamiltoniano\nAceptar o rechazar ese nuevo punto de forma probabilista y comparando la probabilidad del punto preexistente y del nuevo punto.\n\nPor qué es buena idea usar el hamiltoniano? En un MH la propuesta es aleatoria, es como querer encontrar algo en una habitación desconocida a oscuras, hay que ir a tientas. Mientras que con el Hamiltoniano es como tener una linterna, ahora podemos ver que hay en la habitación, al menos localmente a donde apuntemos con la linterna. Veamos, una explicación un poco más matemática. Resolver el hamiltoniano implica calcular derivadas, las derivadas nos dan información sobre la curvatura de una función, por ejemplo el cálculo de la primer derivada en un punto nos dice hacia donde (de)crece una función. Si siquieramos la derivada buscando, hacia donde crece la función, eventualmente llegariamos a un máximo (asumiendo que este existe). Esto se llama maximizar una función. Al agregar el momemtum podemos hacer algo más interesante, podemos simular un trayectoria que explore la distribución a posteriori. Esto es importante en estadística Bayesiana, ya que no solo queremos el máximo, si no una descripción de toda la distribución a posteriori.\n\n\n\nUn HMC tiene varios hipeparámetros, por ejemplo para simular una trayectoria tenemos que hacerlo de a pasos discretos, mientras más pequeños los pasos más fidedigna la simulación, pero también más costosa. Otro hiperparámetro es la longitud de cada simulación si esta es muy corta demoraremos mucho tiempo en explorar la distribución a posteriori, pero si está es muy larga corremos el riesgo de volver al mismo lugar.\nEn la siguiente figura se muestran tres ejemplos. A la izquierda el paso es muy corto, por lo que la exploración no es eficiente, en el centro el paso es correcto pero la simulación demasiado larga, tanto que volvemos al punto de partida, a la derecha tanto el paso como el tiempo de simulación son adecuamos y la propuesta genera un punto alejado en el espacio de los parámetros, pero con alta probabilidad de aceptación. De hecho en este ejemplo la probabilidad de aceptación es 1, ya que la pdf es la misma para el punto de partida que para el punto final.\n\n\n\nEste es otro ejemplo, en cada caso se muestra una densidad de probabilidad que va de más probable (amarillo) a menos probable (violeta), las flechas naranjas indican la trayectoria calculada de a pasos. En en el primero caso vemos una trayectoria elíptica tan larga que vuelve al punto de partida. En el segundo ejemplo vemos que el paso no es adecuado, esto produce una simulación inestable que se manifiesta en divergencias de la trayectoria correcta. En este último caso, y como en el ejemplo anterior, vemos que tanto el paso como el tiempo de simulación son adecuamos y la propuesta genera un punto alejado en el espacio de los parámetros, pero con alta probabilidad de aceptación (1 en este caso).\n\n\n\nCuando los hiper-parámetros de un HMC son adecuados, el muestreo es muy eficiente, mucho más eficiente que un MH. Los valores de los hiper-parámetros dependen esencialmente de la geometría de la distribución a posteriori, por lo que no existe un solo conjunto de hiper-parámetros mejor que los demás. Es por ello que en la práctica estos se calculan de forma adaptativa corriendo una cantidad de pasos de HMC los cuales se utilizan para ajustar eso hiper-parámetros automáticamente y luego se descartan. NUTS (No U-Turn sampler), el sampler por defecto en PyMC es un HMC dinámico y adaptativo. El nombre proviene de una rutina del método que evita que las trayectorias den vueltas en U."
  },
  {
    "objectID": "04_Diagnóstico_MCMC.html#diagnósticos-generales",
    "href": "04_Diagnóstico_MCMC.html#diagnósticos-generales",
    "title": "5  Diagnóstico del muestreo",
    "section": "5.3 Diagnósticos generales",
    "text": "5.3 Diagnósticos generales\nAsintóticamente los MCMC ofrencen la respuesta correcta, el problema es que asintóticamente estamos todos muertos! En la práctica sea hace necesario contar con métodos de diagnóstico que permitan evaluar si el muestreo es correcto para muestras finitas. Si nos ponemos en pesimistas este es un problema sin solución, ya que es imposible demostrar que una muestra es correcta, solo podemos probar que NO lo es. Entonces lo que buscamos es poder recolectar evidencia a favor de la ausencia de problemas. Esto nos va a conducir a establecer algunos valores umbrales, es decir si el diagnositico \\(D\\) da un valor superior a \\(m\\), tenemos un problema con nuestra muestra. Esto también es problemático, ya que establecer umbrales estrictos es en general arbitrario, salvo para casos triviales. Supongamos que yo me invento un diagnóstico para la calvice. El método es simple, hay que contar pelos. Es claro que 0 pelos corresponde a un pelado y 150.000 no, ya que esto se estima como la cantidad de pelos promedio en una cabeza promedio (sea lo que eso sea). Pero que pasa si alguien tiene 122 o 4126 pelos? A continuación veremos algunos valores umbrales, es importante entonces tomarlos con pinzas.\n\n5.3.1 En la teoría confiamos\nLa teoría describe cierto comportamiento de los MCMC, muchos diagnósticos están basados en evaluar si los resultados teóricos se verifican empíricamente. Por ejmplo, la teoría de MCMC dice que:\n\nEl valor inicial es irrelevante, siempre debemos llegar al mismo resultado\nLas muestras no son realmente independientes, pero el valor de un punto solo depende del punto anterior, no hay correlaciones de largo alcance.\nSi miramos la muestra como una secuencia no deberíamos ser capaces de encontrar patrón alguno\n\nPor ej, para una muestra lo suficientemente larga, la primera porción debe ser indistinguible de la última (y la mismo cualquier otra combinación de regiones).\n\nPara un mismo problema cada muestra generada va a ser diferente de las otras, pero a los fines prácticos las muestras deberían ser indistinguibles unas de otros\n\n\n\n5.3.2 Trace plots\nEste es un gráfico muy común. Para cada parámetro graficamos su valor (eje-y) en cada iteración (eje-x). Lo esperable es no ver ningún patrón, solo ruido como en primer panel de la siguiente figura (marco turquesa).\n\n\n\nEn cambio los otros tres paneles (marco magenta) muestran problemas. De izquierda a derecha y arriba a abajo:\n\nEl segundo panel muestra que el muestreo es “pegajoso”, le toma muchos pasos a la cadena moverse de valores altos a valores bajos, es difícil predecir que sucedería si sequimos corriendo, la cadena se movería hacia arriba nuevamente, se estabilizaría en valos bajos, continuaría bajando aún más?\nEl tercer panel muestra una cadena menos “pegajosa”, pero también daría la impresión que aún no ha terminado de estabilizarse\nEl último panel, en cambio, muestra que hay una región donde el sampler se mueve bien, pero cada tanto “salta” a estados donde se queda atascado. Quizá esto se deba a una distribución a posteriori multimodal o dificultades en el sampler para explorar regiones con distinta curvatura.\n\nComo ya vimos por defecto PyMC corre más de una cadena, por lo que un traceplot ideal debería verse como esto:\n\n\n\nArviZ permite graficar trace-plots usando la función az.plot_trace(). Por defecto obtenemos el trace a la derecha y un KDE (para variables continuas) y un histograma (para discretas) a la izquierda\n\n\n\n\n\n5.3.3 Rank plots\nLos trace plots son muy comunes, pero existe una alternativa más moderna llamada rank plots. La idea básica es la siguiente. Para un parámetro tomamos todas las cadenas y ordenamos los valores de menor a mayor y les asignamos un rango es decir al valor más bajo le ponemos 1, al que sigue 0 y así hasta llegar a un número que será igual a la candidad de muestras totales (cantidad de cadenas multiplicado por la cantidad de muestras por cadena). Luego reagrupamos los rankings según las cadenas que les dieron origen y para cada cadena hacemos un histograma. Si las cadenas fuesen indistinguibles esperariamos que los histogramas sean uniformes. Ya que no hay razón para que una cadena tenga más rankings bajos (o medios o altos) que el resto.\nLa siguiente figura muestra 4 ejemplos, donde solo uno (marco cyan) no muestra problemas\n\n\n\nEn ArviZ los rank plots se pueden obtener con la función az.plot_rank o pasando un argumento a plot_trace az.plot_trace(⋅, kind=\"rank_bars\")\n\n\n5.3.4 \\(\\hat R\\) (R sombrero)\nLos gráficos suelen ser útiles para descrubir patrones, pero aveces queremos números, por ejmplo al evaluar rápidamente mucho parámetros. \\(\\hat R\\) es la respuesta a la pregunta. Lograron las cadenas mezclarse adecuadamente? Pero también me gusta pensalo como el jurado en un concurso de trace (o rank) plots. La versión implementada en ArviZ hace varias cosas debajo del capot, pero la idea central es que compara la varianza entre cadenas con la varianza dentro de cada cadena.\n\n\n\nIdealmente $R =$1, en la práctica \\(\\hat R \\lessapprox 1.01\\) son considerados seguros y en la primer fase de modelado valores más altos como \\(\\hat R \\approx 1.1\\) pueden estár bien.\nUsando ArviZ podemos obtener \\(\\hat R\\) usando az.rhat(⋅), az.summary(⋅) y `az.plot_forest(⋅, r_hat=True)``\n\n\n5.3.5 Gráfico de autocorrelación\nIdealmente, una muestra debe ser independiente e idénticamente distribuida (iid). Por definición, las muestras MCMC están correlacionadas. En la práctica, queremos muestras con baja autocorrelación. En ArviZ obtenemos este gráfico con la función az.plot_autocorr()\n\ncadenas_defectuosas = {\"cadenas_defectuosas\": np.linspace(0, 1, 1000).reshape(2, -1)}\naz.plot_autocorr(cadenas_defectuosas);\n\n\n\n\n\ncadenas_defectuosas = {\"cadenas_defectuosas\": np.linspace(0, 1, 1000).reshape(2, -1)}\naz.plot_autocorr(cadenas_defectuosas);\n\n\n\n\n\ncadenas_adecuadas = {\"cadena_adecuadas\": pz.Uniform(0, 1).rvs(size=(2, 500))}\naz.plot_autocorr(cadenas_adecuadas);\n\n\n\n\n\n\n5.3.6 Tamaño de muestra efectivo (ESS)\nComo las muestras de un MCMC están correlacionadas la cantidad de información “útil” es menor que una muestra del mismo tamaño pero iid.\n \n\n\n\nPodemos estimar el tamaño de muestra efectivo (ESS), es decir, el tamaño de una muestra con la cantidad equivalente de información pero sin autocorrelación. Esto es útil para determinar si la muestra que tenemos es lo suficientemente grande. Se recomienta que el ESS sea superior a 100 por cadena. Es decir para para 4 cadenas queremos un mínimo de 400.\nCon ArviZ podemos obtenerlo az.ess(⋅), az.summary(⋅) y `az.plot_forest(⋅, ess=True)``\n\npd.concat((az.ess(cadenas_defectuosas).to_pandas(),\n           az.ess(cadenas_adecuadas).to_pandas()))\n\ncadenas_defectuosas      2.282878\ncadena_adecuadas       910.058723\ndtype: float64\n\n\nVemos que az.summary(⋅) devuelve dos valores de ESS, ess_bulk y ess_tail. Esto se debe a que, distintas regiones del espacio de los parámetros pueden tener distinto valor de ESS, ya que no todas las regiones son muestreadas con la misma eficiencia. Intuitivamente uno puede pensar que al muestrear una distribución como una Gaussiana es más fácil obtener mejor calidad de muestra alrededor de la media que de las colas, simplemente por que tenemos más muestras de esa región.\n\npd.concat([az.summary(cadenas_adecuadas, kind=\"diagnostics\"),\n           az.summary(cadenas_defectuosas, kind=\"diagnostics\")])\n\n\n\n\n\n  \n    \n      \n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      cadena_adecuadas\n      0.010\n      0.007\n      910.0\n      988.0\n      1.00\n    \n    \n      cadenas_defectuosas\n      0.198\n      0.165\n      2.0\n      11.0\n      3.05\n    \n  \n\n\n\n\nSi las muestras de MCMC las vamos a usar para calcular valores centrales como medias o medianas entonces tenemos que asegurarnos que el ess_bulk sea lo suficientemente algo, en cambio, si queremos calcular intervalos como un HDI 95% hay que asegurarse que ess_tail sea adecuado.\nArviZ ofrece varias funciones vinculadas al ESS. Por ejemplo si queremos evaluar el desempeño del sampler para varias regiones al mismo tiempo podemos usar az.plot_ess.\n\n_, axes = plt.subplots(1, 2, figsize=(10,4), sharey=True)\naz.plot_ess(cadenas_adecuadas, ax=axes[0])\naz.plot_ess(cadenas_defectuosas, ax=axes[1]);\n\n\n\n\nUna forma simple de aumentar el ESS es aumentar la cantidad de muestras, pero podría darse el caso que el ESS crezca muy lento con el número de muestras, por lo que aún si aumentaramos 10 veces la cantidad de muestras estaríamos por debajo de lo requerido. Una forma de estimar “cuanto nos falta” es usar az.plot_ess(⋅, kind=\"evolution\"). Este gráfico nos muestra como fue cambiando el ESS en muestra muestra, lo que nos permite hacer proyecciones. En el siguiente ejemplo vemos que para cadenas_adecuadas el ESS crece lineamente con el número de muestras mientras que para cadenas_defectuosas no crece para nada. Este último caso no hay esperanzas de mejorar el ESS simplemente aumentando la cantidad de muestras.\n\n_, axes = plt.subplots(1, 2, figsize=(10,4), sharey=True)\naz.plot_ess(cadenas_adecuadas, kind=\"evolution\", ax=axes[0])\naz.plot_ess(cadenas_defectuosas,  kind=\"evolution\", ax=axes[1]);\n\n\n\n\n\n\n5.3.7 Error estándard del Monte Carlo (MCSE)\nUna ventaja del ESS es que no tiene escala, da igual si un parámetro varía entre 0.1 y 0.2 y otro entre -2000 y 5000, un ESS de 400 tiene el mismo significado en ambos casos. En modelos con muchos parámetros rápidamente podemos indentificar cuales parámetros son más problemáticos. Sin embargo, a la hora de reportar resultados no es muy informativo saber si el ESS fue de 1372 o 1501. En cambio nos gustaría saber el orden del error que estamos cometiendo al aproximar la distribución a posterori. Esa información la da el error estándard del Monte Carlo (MCSE). Al igual que el ESS, el MCSE tiene en cuenta la autocorrelación de las muestras. Este error debe estar por debajo de la precisión deseada en nuestros resultados. Es decir si para un parámetro el MCSE es 0.1, no tiene sentido reportar que la media de ese parámetro es 3.15. Ya que tranquilamente el valor correcto podría estar entre 3.4 y 2.8.\nUna de las cantidades devueltas por summary es mc_error."
  },
  {
    "objectID": "04_Diagnóstico_MCMC.html#diagnóstico-de-algoritmos-basados-en-gradiente",
    "href": "04_Diagnóstico_MCMC.html#diagnóstico-de-algoritmos-basados-en-gradiente",
    "title": "5  Diagnóstico del muestreo",
    "section": "5.4 Diagnóstico de algoritmos basados en gradiente",
    "text": "5.4 Diagnóstico de algoritmos basados en gradiente\nDebido a su funcionamiento interno, algoritmos como NUTS ofrecen algunas pruebas específicas que no están disponibles para otros métodos. Generalmente estas pruebas son muy sensibles\nPara ejemplificar esto vamos a cargar dos InferenceData de modelos pre-calculados. Los detalles de como se generaron estos idata no es relevante por el momento. Solo diremos que son dos modelos que son matemáticamente equivalente pero parametrizados de formas distintas. En este caso la parametrización afecta la eficiencia del sampler. El modelo centrado es muestreado de forma más eficiente que el modelo no centrado.\n\nidata_cm = az.load_arviz_data(\"centered_eight\")\nidata_ncm = az.load_arviz_data(\"non_centered_eight\")\n\n\n5.4.1 Energía de transición vs energía marginal\nPodemos pensar en un Monte Carlo Hamiltoniano como un proceso de dos pasos * Un muestreo determinista (siguiendo el hamiltoniano) * Una caminata aleatorio en el espacio del momentum\nSi la distribución de la energía de transición es similar a la distribución de la energía marginal, entonces NUTS es capaz de generar muestras de la distribución marginal de la energía que sean casi independientes entre transiciones. Esto lo podemos evaluar visualmente y numéricamente (Bayesian Fraction of Missing Information)\n\n_, axes = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(12, 4), constrained_layout=True)\n\nfor ax, idata, nombre in zip(axes.ravel(), (idata_cm, idata_ncm), (\"centrado\", \"no centrado\")):\n    az.plot_energy(idata, ax=ax)\n    ax.set_title(nombre)\n\n\n\n\n\n\n5.4.2 Divergencias\nUna ventaja de NUTS es que falla con el estilo. Esto sucede por ejemplo al intentar pasar de regiones de baja curvatura a regiones de alta curvatura. En estos casos las trayectorias numéricas pueden divergir. En esencia esto sucede por que en esos casos no existe un único conjunto de hiper-parámetros que permita el muestreo eficiente de ambas regiones. Por lo que una de la regiones es muestreada adecuandamente y cuando el sampler se mueve hacia la otra región falla. Las trayectorias numéricas divergentes son identificadores extremadamente sensibles de vecindarios patológicos.\nEl siguiente ejemplo muestra dos cosas el modelo no centrado muestra varias divergencias (círculos turquesas) agrupados en una región. En el modelo centrado, que no tiene divergencias, se puede ver que alrededor de esa misma región hay muestras para valores más pequeños de tau. Es decir el modelo no centrado falla en muestrear una región, pero al menos avisa que está teniendo problemas en muestrear esa región!\n\n_, axes = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5), constrained_layout=True)\n\n\nfor ax, idata, nombre in zip(axes.ravel(), (idata_cm, idata_ncm), (\"centrado\", \"no_centrado\")):\n    az.plot_pair(idata, var_names=['theta', 'tau'], coords={'school':\"Choate\"}, kind='scatter',\n                 divergences=True, divergences_kwargs={'color':'C1'},\n                 ax=ax)\n    ax.set_title(nombre)\n\n\n\n\n\naz.plot_parallel(idata_cm, figsize=(12, 4));"
  },
  {
    "objectID": "04_Diagnóstico_MCMC.html#qué-hacer-cuando-los-diagnósticos-no-dan-bien",
    "href": "04_Diagnóstico_MCMC.html#qué-hacer-cuando-los-diagnósticos-no-dan-bien",
    "title": "5  Diagnóstico del muestreo",
    "section": "5.5 Qué hacer cuando los diagnósticos no dan bien?",
    "text": "5.5 Qué hacer cuando los diagnósticos no dan bien?\n\n\n Más muestras o más pasos de tuning. Esto solo suele ser útil cuando los problemas son menores\n Burn-in. Software moderno como PyMC utiliza una cantidad de muestras para ajustar los hiper-parámetros de los métodos de muestreo. Por defecto esas muestras son eliminadas, por lo que en general no es necesario hacer Burn-in manualmente. \n Cambiar el método de muestreo! \nReparametrizar el modelo\n Mejorar las distribuciones a priori \n\nEl teorema popular de la estadística computacional: Cuando tienes problemas computacionales, a menudo hay un problema con tu modelo. La recomendación NO es cambiar la distribución a priori para mejorar la calidad del muestreo. La recomendación es que si el muestreo es malo, quizá el modelo también lo sea. En ese caso, podemos pensar en mejorar el modelo, una forma de mejorarlo es usar conocimiento previo para mejorar las distribuciones a priori.\n\nAlgunos modelos pueden expresarse en más de una forma, todas matemáticamente equivalentes. En esos casos, algunas parametrizaciones pueden ser más eficientes que otras. Por ejemplo, como veremos más adelante con modelos lineales jerárquicos.\nEn el caso de las divergencias, estas suelen eliminarse aumentando la tasa de aceptación (pm.sample(..., target_accept=x) x>0.8)\nLeer los mensajes de advertencia y sugerencias de PyMC! ;-)"
  },
  {
    "objectID": "04_Diagnóstico_MCMC.html#ejercicios",
    "href": "04_Diagnóstico_MCMC.html#ejercicios",
    "title": "5  Diagnóstico del muestreo",
    "section": "5.6 Ejercicios",
    "text": "5.6 Ejercicios\n\nExplicá en tus propias palabras que es el ESS, el \\(\\hat R\\) y el MCSE. ¿Qué información nos dan?\n¿Qué significa que el \\(\\hat R\\) sea 1.01? ¿Qué significa que sea 1.5? ¿Qué significa que sea 1.9?\n¿Qué significa que el ESS sea 100? ¿Qué significa que sea 1000? ¿Qué significa que sea 10000?\n¿Qué significa que el MCSE sea 0.1? ¿Qué significa que sea 0.5? ¿Qué significa que sea 1?\n¿Qué significa que el MCSE sea 0.1 para un parámetro cuya media es 3.15? ¿Qué significa que sea 0.5? ¿Qué significa que sea 1?\n¿Qué significa que el MCSE sea 0.1 para un parámetro cuya media e"
  },
  {
    "objectID": "04_Diagnóstico_MCMC.html#para-seguir-leyendo",
    "href": "04_Diagnóstico_MCMC.html#para-seguir-leyendo",
    "title": "5  Diagnóstico del muestreo",
    "section": "5.7 Para seguir leyendo",
    "text": "5.7 Para seguir leyendo\nExploratory Analysis of Bayesian Models Trabajo en Progreso!\nA Conceptual Introduction to Hamiltonian Monte Carlo\nRank-normalization, folding, and localization\nComputing Bayes: Bayesian Computation from 1763 to the 21st Century."
  },
  {
    "objectID": "05_Regresión_lineal.html",
    "href": "05_Regresión_lineal.html",
    "title": "6  Regresión Lineal",
    "section": "",
    "text": "7 Regresión lineal\nSupongamos que tenemos una variable \\(X\\), y a partir de esta queremos predecir o modelar una variable \\(Y\\). Además, estás variables se encuentran apareadas ${(x_1,y_1), (x_2,y_2),(x_n,y_n)} $. En el caso más simple \\(X\\) e \\(Y\\) son variables aleatorias continuas y unidimensionales, usando Python las representaríamos usando arrays de dimensión 1 de tipo flotante.\nLas variable \\(Y\\) suele recibir distintos nombres como variable dependiente, predicha o respuesta, mientras que \\(X\\) recibe nombres como variable independiente, predictora o de entrada. En Machine learning es común hablar de features en vez de variables y es común pensar que una regresión lineal es un ejemplo de aprendizaje supervisado.\nCuando tenemos más de una variable independiente es común representarla como una matriz \\(\\boldsymbol{X}\\) (usualmente llamada matriz de diseño), donde por lo general las columnas representan distintos tipos de variables (o features) y las filas distintas observaciones, instancias , sujetos, etc. Este tipo de modelo se llama regresión lineal múltiple o regresión lineal multivariable y es quizá el caso más común de regresión lineal. El nombre regresión lineal multivariada debería reservarse a casos en que tenemos más de una variable respuesta, aunque es muy común en literatura que estos términos se usen de forma intercambiable.\nAlgunos ejemplos donde se pueden usar modelos de regresión lineal:\nHabiendo ya discutido algunas ideas generales sobre regresión lineal veamos cómo es que este modelo se construye. Podemos describir una relación lineal usando la siguiente expresión:\n\\[y_i = \\alpha + x_i \\beta  \\tag{3.1}\\]\nSegún esta expresión cada observación \\(y_i\\) se obtiene a partir de multiplicar \\(x_i\\) por un coeficiente \\(\\beta\\) y luego se le suma el coeficiente \\(\\alpha\\).\nEl parámetro \\(\\beta\\) controla la pendiente en la relación lineal, podemos interpretarlo como el cambio en la variable \\(Y\\) por cambio de unidad en la variable \\(X\\). El parámetro \\(\\alpha\\) se conoce como intercepto u ordenada al origen, y podemos interpretarlo como el valor de \\(y_i\\) cuando \\(x_i = 0\\). Gráficamente, \\(\\alpha\\) indica el valor de \\(y_i\\) donde la línea intercepta el eje y.\nUn método muy popular para encontrar los parámetros para un modelo lineal se conoce como ajuste por mínimos cuadrados. Este método devuelve un valor para \\(\\alpha\\) y uno para \\(\\beta\\) de tal forma que esos valores sean los que minimizan el error cuadrático medio entre los \\(y\\) observados y predichos. Es decir obtenemos una sola linea recta, la “mejor” según este criterio (hay otros).\nNosotros vamos a seguir una ruta diferente, para ello vamos a reformular la expresión 3.1 en términos probabilístas:\n\\[Y \\sim \\mathcal{N}(\\mu=\\alpha + X \\beta, \\epsilon) \\tag{3.2}\\]\nEs decir \\(Y\\) es una variable aleatoria distribuida según una Gaussiana con media \\(\\alpha + X \\beta\\) desviación estándar \\(\\epsilon\\). Desde esta perspectiva una regresión lineal es una extensión de un modelo Gaussiano donde en vez de estimar la media de forma directa la calculamos como una una función lineal de las variables predictoras.\nComo desconocemos los valores de los parámetros \\(\\alpha\\), \\(\\beta\\) y \\(\\epsilon\\) debemos asignarles distribuciones a priori. Si usáramos a prioris planos entonces el valor máximo a posteriori (la moda del posterior) sería el mismo que el encontrado usando mínimos cuadrados y el mismo que usando maximum likelihood. En general es posible hacer algo mejor que esto. Una elección razonable y genérica para los a prioris sería:\n\\[\n\\alpha \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha) \\\\\n\\beta \\sim \\mathcal{N}(\\mu_\\beta, \\sigma_\\beta) \\\\\n\\epsilon \\sim \\mathcal{HN}(\\sigma_\\epsilon) \\tag{3.3}\n\\]\nSi no tenemos una idea muy clara sobre qué valores deberían tener los a prioris podemos fijar los valores de \\(\\sigma_{\\alpha}\\), \\(\\sigma_{\\beta}\\) o \\(\\sigma_{\\epsilon}\\) de forma tal que sean grandes dada la escala de los datos. En general es más fácil tener una idea de los valores que \\(\\beta\\) puede tomar por sobre los de \\(\\alpha\\), por ejemplo solemos saber si la pendiente es positiva o negativa. Usar datos estandarizados suele ser útil para elegir a prioris ligeramente informativos que funciona para un amplio rango de problemas.\nPara parámetros como \\(\\epsilon\\), que están restringidos a los positivos, es común el uso de a prioris como la media Gaussiana (como en 3.3), algunas alternativas son la distribución uniforme y la media-Cauchy, mientras que la media-gaussiana y la media-Cauchy funcionan bien como a prioris generales, la distribución uniforme no suele ser buena idea, en general no es buena idea usar distribuciones restringidas a un rango salvo que sepamos que los parámetros realmente están restringidos a ese rango. La distribución gamma se puede usar para definir a prioris más informativos para \\(\\epsilon\\), especialmente si la definimos usando la media y desviación estándar, PyMC permite definir una distribución Gamma usando dos parametrizaciones alternativas.\nUsando diagramas de Krusche podemos representar una regresión lineal de la siguiente forma:\n\\(\\mu\\) está definida usando el símbolo \\(=\\), en vez de \\(\\sim\\), esto se debe a que una vez conocidos \\(\\alpha\\) y \\(\\beta\\) el valor de \\(\\mu\\) queda completamente determinado. Llamamos a este tipo de variables deterministas.\nAhora necesitamos los datos para alimentar el modelo. Una vez más, vamos a confiar en un conjunto de datos sintéticos. Una ventaja de un conjunto de datos sintético es que conocemos los valores correctos de los parámetros y, por lo tanto, podemos verificar si podemos recuperarlos con nuestros modelos.\nEscribir este modelo en PyMC es bastante directo, la única diferencia con los modelos anteriores es que ahora hemos especificado a la variable \\(\\mu\\) como una variable determinista. Una variable determinista tendrá una distribución a posteriori ya que es función de al menos una variable estocástica. Si en PyMC especificamos un variable como determinista esta será incluida en el idata. Alternativamente podríamos haber escrito:\nEsto es igualmente válido, la única diferencia es que en este caso la variable \\(\\mu\\) no estaría incluida en el trace\n\\[\ny_i \\sim Normal(\\mu, \\sigma) \\\\\n\\mu \\sim Normal(0, 10) \\\\\n\\sigma \\sim HalfNormal(25)\n\\]\n\\[\ny \\sim Normal(\\mu, \\epsilon) \\\\\n\\mu = \\alpha + \\beta x \\\\\n\\alpha \\sim Normal(0, 10) \\\\\n\\beta \\sim Normal(0, 1) \\\\\n\\epsilon \\sim HalfNormal(25) \\\\\n\\]\nCompare los resultados usando funciones de ArviZ como plot_trace y plot_pairs. Centre la variable \\(x\\) y repita el ejercicio ¿Qué opina?"
  },
  {
    "objectID": "05_Regresión_lineal.html#modelos-lineales-y-autocorrelación",
    "href": "05_Regresión_lineal.html#modelos-lineales-y-autocorrelación",
    "title": "6  Regresión Lineal",
    "section": "7.1 Modelos lineales y autocorrelación",
    "text": "7.1 Modelos lineales y autocorrelación\nEn un modelo lineal los parámetros \\(\\alpha\\) y \\(\\beta\\) están correlacionados. Esto se puede ver en la siguiente figura:\n\naz.plot_pair(idata_g, var_names='~μ', scatter_kwargs={'alpha':0.5});\n\n\n\n\nEsta correlación es una consecuencia directa de nuestras suposiciones. En general al hacer una regresión lineal Bayesiana las lineas que ajustan los datos pasan aproximadamente por la media de \\(X\\) y la media de \\(Y\\), además el aumento en la pendiente significa la disminución de la ordenada al origen y viceversa. Esto provoca que el posterior para \\(\\alpha\\) y \\(\\beta\\) sea un espacio muy diagonal. Esto puede ser problemático para métodos como Metropolis-Hastings y, en menor medida para NUTS.\nUn método simple para eliminar la correlación entre \\(\\alpha\\) y \\(\\beta\\) consiste en centrar la variable \\(X\\), para esto calculamos su media y se la restamos a cada valor, obteniendo así \\(x'\\). Como resultado la media de \\(x'\\) será 0, si usamos \\(x'\\) como variable dependiente \\(\\alpha\\) deberá estar alrededor de 0 y además las lineas que sean solución al problema pivotearán alrededor de 0 por lo que los cambios de \\(\\beta\\) tendrán poco efecto en los valores de \\(\\alpha\\) esto provoca que el posterior para \\(\\alpha\\) y \\(\\beta\\) sea más circular y menos correlacionado. Esto lo pueden comprobar ustedes mismos si vuelven a correr el modelo anterior, pero esta vez centrando los datos.\nCentrar datos no es solo un truco computacional, también puede ser un truco estadístico que ayuda a interpretar los resultados. \\(\\alpha\\) es el valor de $y_i $ cuando \\(x_i = 0\\). Para muchos problemas, esta interpretación no tiene ningún sentido. Por ejemplo, para cantidades tales como la altura o el peso, los valores de cero no tienen sentido. En cambio, al centrar las variables, \\(\\alpha\\) se convierte en el valor de \\(y_i\\) para el valor medio de \\(x\\). Para algunos problemas, puede ser útil estimar \\(\\alpha\\) precisamente porque no es factible medir experimentalmente el valor de \\(x_i = 0\\) y, por lo tanto, \\(\\alpha\\) puede proporcionarnos información valiosa, pero las extrapolaciones tienen sus advertencias, así que ¡tene cuidado cuando haces esto!\nEs posible que deseemos informar los parámetros estimados en términos de los datos centrados o en términos de datos descentrados, la decisión dependerá del problema y de la audiencia. Si necesitamos informar los parámetros como si hubiesen sido determinados en la escala original, podemos hacer lo siguiente para devolverlos a esa escala:\n\\[\\alpha = \\alpha' - \\beta' \\bar x \\tag{3.5}\\]\nEsta corrección es el resultado del siguiente razonamiento algebraico:\n\\[\ny =\\alpha' + \\beta'x' + \\epsilon \\\\\ny =\\alpha' + \\beta'(x - \\bar x) + \\epsilon \\\\\ny =\\alpha' - \\beta' \\bar x + \\beta'  x + \\epsilon \\\\ \\tag{3.6}\n\\]\nLuego se deduce que la ecuación 3.5 es verdadera y también:\n\\[\\beta = \\beta' \\tag{3.7}\\]\nOtra transformación que puede ser útil es estandarizar los datos. Esta transformación es una práctica común para los modelos de regresión lineal tanto en estadística y machine learning, ya que muchos algoritmos se comportan mejor cuando los datos están estandarizados. Esta transformación se logra al centrar los datos y dividirlos por la desviación estándar. Matemáticamente tenemos:\n\\[\nx' = \\frac{x - \\bar x}{x_{sd}} \\\\\ny' = \\frac{y - \\bar y}{y_{sd}} \\tag{3.8}\n\\]\nUna ventaja de la estandarización de los datos es que siempre podemos usar los mismos priors débilmente informativos, sin tener que pensar en la escala de los datos. Para datos estandarizados, la intersección siempre será alrededor de 0 y la pendiente estará restringida al intervalo [-1, 1]. Estandarizar los datos nos permite hablar en términos de Z-score, es decir, en unidades de desviaciones estándar. Si alguien dice que el valor de un parámetro es -1.3 unidades de Z-score, sabemos automáticamente que el valor en cuestión es 1.3 desviaciones estándar por debajo del valor de la media (aún cuando no sepamos cual es el valor de la media). Un cambio en una unidad Z-score es un cambio en una desviación estándar cualquiera sea la escala original de los datos. Los Z-zcore también son muy útiles cuando se trabaja con muchas variables; ya que tener todas las variables en una misma escala puede simplificar la interpretación de los datos."
  },
  {
    "objectID": "05_Regresión_lineal.html#interpretando-y-visualizando-el-posterior",
    "href": "05_Regresión_lineal.html#interpretando-y-visualizando-el-posterior",
    "title": "6  Regresión Lineal",
    "section": "7.2 Interpretando y visualizando el posterior",
    "text": "7.2 Interpretando y visualizando el posterior\nComo ya hemos visto, podemos explorar el posterior usando funciones de ArviZ como plot_trace y summary, o podemos usar nuestras propias funciones. Para una regresión lineal, podría ser útil dibujar la línea promedio que ajusta los datos junto con los valores promedio de \\(\\alpha\\) y \\(\\beta\\). Para reflejar la incertidumbre contenida en la distribución a posteriori, podemos usar líneas semitransparentes muestreadas de esta distribución:\n\nplt.plot(x, y, 'C0.')\n\n\nα_m = idata_g.posterior['α'].mean((\"chain\", \"draw\")).item()\nβ_m = idata_g.posterior['β'].mean((\"chain\", \"draw\")).item()\n\nfew_samples = az.extract(idata_g, num_samples=50)\nplt.plot(x, few_samples['α'].values + few_samples['β'].values *  x[:,np.newaxis], c='C1', alpha=0.25);\n\nplt.plot(x, α_m + β_m * x, c='k',\n         label=f'y = {α_m:.2f} + {β_m:.2f} * x')\n\nplt.xlabel('x')\nplt.ylabel('y', rotation=0, labelpad=10)\nplt.legend();\n\n\n\n\nEn la figura anterior se puede ver que la incertidumbre es menor en el medio, aunque no se reduce a un solo punto, es decir, la distribución a posteriori es compatible con las líneas que no pasan exactamente por la media de los datos, como ya hemos mencionado.\nUna alternativa a muestrear lineas de la distribución a posteriori es dibujar una banda semitransparente que represente un intervalo HDI de \\(\\mu\\). Al haber definido la variable \\(\\mu\\) como determinista en el modelo, podemos hacer esto de forma sencilla:\n\nplt.plot(x, y, 'C0.')\n\nplt.plot(x, α_m + β_m * x, c='k',\n         label='y = {:.2f} + {:.2f} * x'.format(α_m, β_m))\n\naz.plot_hdi(x, idata_g.posterior['μ'], color='C1')\n\nplt.xlabel('x')\nplt.ylabel('y', rotation=0, labelpad=10);\n\n\n\n\nUna tercera opción es representar el HDI de la distribución predictiva a posteriori, es decir la distribución de datos predichos. En la siguiente figura sea usa un gris más oscuro para el HDI 50% y un gris más claro para el HDI 94%. Para poder hacer el siguiente gráfico necesitamos, primero, obtener las muestras predictivas posteriores. Lo cual es fácil usando PyMC con la función sample_ppc:\n\nidata_g.extend(pm.sample_posterior_predictive(idata_g, model=model_g))\n\nSampling: [y_pred]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\nY ahora si la figura\n\nplt.plot(x, y, 'b.')\nplt.plot(x, α_m + β_m * x, c='k', label='y = {:.2f} + {:.2f} * x'.format(α_m, β_m))\n\naz.plot_hdi(x, idata_g.posterior_predictive['y_pred'], color='C1')\naz.plot_hdi(x, idata_g.posterior_predictive['y_pred'], hdi_prob=0.5, color='C1')\n\nplt.xlabel('x')\nplt.ylabel('y', rotation=0);"
  },
  {
    "objectID": "05_Regresión_lineal.html#regresión-lineal-robusta",
    "href": "05_Regresión_lineal.html#regresión-lineal-robusta",
    "title": "6  Regresión Lineal",
    "section": "7.3 Regresión lineal robusta",
    "text": "7.3 Regresión lineal robusta\nAsumir que los datos siguen una distribución gaussiana es perfectamente razonable en muchas situaciones. Al asumir Gaussianidad, no necesariamente estamos aceptando que los datos son gaussianos; en cambio, estamos diciendo que es una aproximación razonable para un problema determinado. Como vimos en el capítulo anterior, a veces esta suposición gaussiana falla, por ejemplo, en presencia de valores aberrantes. Aprendimos que el uso de la distribución t de Student es una forma de tratar de manera efectiva con valores atípicos y obtener una inferencia más robusta. La misma idea se puede aplicar a la regresión lineal y para ejemplificarla vamos a utilizar un conjunto de datos muy simple: el tercer grupo de datos del cuarteto de Anscombe\n\nans = pd.read_csv('datos/anscombe.csv')\nx_3 = ans[ans.group == 'III']['x'].values\ny_3 = ans[ans.group == 'III']['y'].values\nx_3 = x_3 - x_3.mean()\n\nY ahora veamos cómo luce este pequeño conjunto de datos:\n\n_, ax = plt.subplots(1,2, figsize=(10,5), sharey=True)\n\nax[0].plot(x_3, y_3, 'C0o')\nax[0].set_xlabel('x')\nax[0].set_ylabel('y', rotation=0, labelpad=15)\nax[1].set_xticks([])\naz.plot_kde(y_3, ax=ax[1], rug=True, rotated=True);\n\n\n\n\nAhora vamos a reescribir el modelo_g esta vez usando una distribución t de Student en lugar de una Gaussiana. Este cambio también introduce la necesidad de especificar el valor de \\(\\nu\\), el parámetro de normalidad. Si no recuerdas la función de este parámetro, consultá el capítulo anterior antes de continuar.\nEn el siguiente modelo estamos usando una distribución exponencial desplazada, para evitar valores de \\(\\nu\\) cercanos a cero. La exponencial no desplazada pone demasiado peso en valores cercanos a cero y esto puede traer algunos problemas. En el casos del tercer conjunto de datos de Anscombe lo problemático deriva de que es posible ajustar una recta de forma perfecta (si obviamos el dato aberrante). Como regla general los priors usados en este curso suelen ser buenos valores por defecto, pero nada más que eso. Otra distribución a priori comunmente usada para \\(\\nu\\) es gamma(2, 0.1) o gamma(mu=20, sd=15).\n\nwith pm.Model() as model_t:\n    α = pm.Normal('α', y_3.mean(), 1)\n    β = pm.Normal('β', 0, 1)\n    ϵ = pm.HalfNormal('ϵ', 5)\n    ν_ = pm.Exponential('ν_', 1/29)\n    ν = pm.Deterministic('ν', ν_ + 1)\n    \n    y_pred = pm.StudentT('y_pred', mu=α + β * x_3,\n                         sigma=ϵ, nu=ν, observed=y_3)\n    \n    idata_t = pm.sample(2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β, ϵ, ν_]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:05<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 6 seconds.\n\n\n\nvar_names = ['~ν_']\naz.plot_trace(idata_t, var_names=var_names, kind=\"rank_vlines\", combined=True);\n\n\n\n\nEn la siguiente gráfica podemos ver el ajuste robusto, según model_t, y el ajuste no robusto de acuerdo con la función linregress de SciPy (esta función realiza una regresión por mínimos cuadrados). Como ejercicio, puede intentar agregar a esta gráfica la mejor línea obtenida usando model_g.\n\nbeta_c, alpha_c = stats.linregress(x_3, y_3)[:2]\n\nplt.plot(x_3, (alpha_c + beta_c * x_3), label='no-robusto')\nplt.plot(x_3, y_3, 'ko')\nalpha_m = idata_t.posterior['α'].mean().item()\nbeta_m = idata_t.posterior['β'].mean().item()\nplt.plot(x_3, alpha_m + beta_m * x_3, label='robusto')\n\nplt.xlabel('x')\nplt.ylabel('y', rotation=0, labelpad=15)\nplt.legend(loc=2);\n\n\n\n\nLa figura anterior se puede explicar porque una distribución de t, con sus colas más pesadas, es capaz de dar menos importancia a los puntos que están alejados del grupo principal de datos. En cambio el ajuste no robusto se esfuerza por incluir a todos los puntos. SI bien este es un conjunto muy particular de datos el mensaje es válido para datos más complejos y reales\nAntes de continuar tómese un momento para contemplar los valores de los parámetros (estoy omitiendo los parámetros intermedios ya que no es de interés directo).\nAntes de continuar tómemonos un momento para contemplar los valores de los parámetros según la distribución a posteriori.\n\naz.summary(idata_t, var_names)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      α\n      7.114\n      0.001\n      7.112\n      7.117\n      0.000\n      0.000\n      5019.0\n      4365.0\n      1.0\n    \n    \n      β\n      0.345\n      0.000\n      0.345\n      0.346\n      0.000\n      0.000\n      4709.0\n      3331.0\n      1.0\n    \n    \n      ϵ\n      0.003\n      0.002\n      0.001\n      0.006\n      0.000\n      0.000\n      1527.0\n      581.0\n      1.0\n    \n    \n      ν\n      1.205\n      0.199\n      1.000\n      1.581\n      0.003\n      0.002\n      1752.0\n      614.0\n      1.0\n    \n  \n\n\n\n\nComo pueden ver, los valores de \\(\\alpha\\), \\(\\beta\\) y \\(\\epsilon\\) practicamente no tienen variación (sd=0), incluso \\(\\epsilon\\) es prácticamente 0. Esto es totalmente razonable dado que estamos ajustando una línea recta a un perfecto conjunto alineado de puntos (si ignoramos el punto atípico)."
  },
  {
    "objectID": "05_Regresión_lineal.html#regresión-lineal-jerárquica",
    "href": "05_Regresión_lineal.html#regresión-lineal-jerárquica",
    "title": "6  Regresión Lineal",
    "section": "7.4 Regresión lineal jerárquica",
    "text": "7.4 Regresión lineal jerárquica\nEn el capítulo anterior, aprendimos los rudimentos de los modelos jerárquicos. Este mismo concepto se puede aplicar a las regresiones lineales. Esto permite que los modelos realicen inferencias a nivel de subgrupo y a nivel global. Como ya vimos, esto se hace incluyendo hiperpriors.\nVamos a crear ocho grupos de datos relacionados, incluido uno con un solo dato\n\nN = 20\nM = 8\nidx = np.repeat(range(M-1), N)\nidx = np.append(idx, 7)\nnp.random.seed(314)\n\nalfa_real = np.random.normal(2.5, 0.5, size=M)\nbeta_real = np.random.beta(6, 1, size=M)\neps_real = np.random.normal(0, 0.5, size=len(idx))\n\ny_m = np.zeros(len(idx))\nx_m = np.random.normal(10, 1, len(idx))\ny_m = alfa_real[idx] + beta_real[idx] * x_m  + eps_real\n\n_, ax = plt.subplots(2, 4, figsize=(10,5), sharex=True, sharey=True)\nax = np.ravel(ax)\nj, k = 0, N\nfor i in range(M):\n    ax[i].scatter(x_m[j:k], y_m[j:k])\n    ax[i].set_xlabel('$x_{}$'.format(i))\n    ax[i].set_ylabel('$y_{}$'.format(i), rotation=0, labelpad=15)\n    ax[i].set_xlim(6, 15)\n    ax[i].set_ylim(7, 17)\n    j += N\n    k += N\n\n\n\n\nVamos a centrar los datos antes de pasárselos al modelo.\n\nx_centered = x_m - x_m.mean()\n\n\nwith pm.Model() as hierarchical_model:\n    # hyper-priors\n    α_μ_tmp = pm.Normal('α_μ_tmp', mu=0, sigma=10)\n    α_σ_tmp = pm.HalfNormal('α_σ_tmp', 10)\n    β_μ = pm.Normal('β_μ', mu=0, sigma=10)\n    β_σ = pm.HalfNormal('β_σ', sigma=10)\n\n    # priors\n    α_tmp = pm.Normal('α_tmp', mu=α_μ_tmp, sigma=α_σ_tmp, shape=M)\n    β = pm.Normal('β', mu=β_μ, sigma=β_σ, shape=M)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n    ν = pm.Exponential('ν', 1/30)\n\n    y_pred = pm.StudentT('y_pred', mu=α_tmp[idx] + β[idx] * x_centered, sigma=ϵ, nu=ν, observed=y_m)\n\n    α = pm.Deterministic('α', α_tmp - β * x_m.mean()) \n\n    idata_hm = pm.sample(1000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α_μ_tmp, α_σ_tmp, β_μ, β_σ, α_tmp, β, ϵ, ν]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:08<00:00 Sampling 4 chains, 127 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 9 seconds.\n\n\n\naz.plot_forest(idata_hm, var_names=['α', 'β'], figsize=(10, 4), combined=True, r_hat=False, ess=False);\n\n\n\n\nDibujemos las líneas ajustadas, para cada uno de los ocho grupos.\n\n_, ax = plt.subplots(2, 4, figsize=(12, 4), sharex=True, sharey=True)\nax = np.ravel(ax)\nj, k = 0, N\nx_range = np.linspace(x_m.min(), x_m.max(), 10)\nposterior = az.extract(idata_hm)\n\nfor i in range(M):\n    ax[i].scatter(x_m[j:k], y_m[j:k])\n    ax[i].set_xlabel('$x_{}$'.format(i))\n    ax[i].set_ylabel('$y_{}$'.format(i), labelpad=10, rotation=0)\n    alfas = posterior['α'].sel(α_dim_0=i)\n    betas = posterior['β'].sel(β_dim_0=i)\n    alfa_m = alfas.mean(\"sample\").item()\n    beta_m = betas.mean(\"sample\").item()\n    ax[i].plot(x_range, alfa_m + beta_m * x_range, c='k')\n    az.plot_hdi(x_range, alfas + betas * xr.DataArray(x_range).transpose(), ax=ax[i])\n    plt.xlim(x_m.min()-1, x_m.max()+1)\n    plt.ylim(y_m.min()-1, y_m.max()+1)\n    j += N\n    k += N"
  },
  {
    "objectID": "05_Regresión_lineal.html#regresión-lineal-multiple",
    "href": "05_Regresión_lineal.html#regresión-lineal-multiple",
    "title": "6  Regresión Lineal",
    "section": "7.5 Regresión lineal Multiple",
    "text": "7.5 Regresión lineal Multiple\nHasta ahora hemos estado trabajando con una variable dependiente y una variable independiente, sin embargo no es inusual tener varias variables independientes que queremos incluir en nuestro modelo. Algunos ejemplos podrían ser:\n\nCalidad percibida del vino (dependiente) y acidez, densidad, nivel de alcohol, azúcar residual y contenido de sulfatos (variables independientes)\nCalificaciones promedio de los estudiantes (dependientes) e ingresos familiares, distancia de la casa a la escuela y educación de la madre (variable categórica)\n\nPodemos extender fácilmente el modelo de regresión lineal simple para tratar con más de una variable independiente. Llamamos a este modelo regresión lineal múltiple, que no debe confundirse con la regresión lineal multivariada, que corresponde con el caso de múltiples variables dependientes.\nEn una regresión lineal múltiple modelamos la media de la variable dependiente como:\n\\[\\mu = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 \\dots + \\beta_m x_m \\tag{3.15}\\]\nLa expresión 3.15 se parece a una regresión polinomial (ecuación 3.12), pero no es exactamente lo mismo. Para la regresión lineal múltiple tenemos diferentes variables independientes en lugar de potencias sucesivas de la misma variable independiente. Desde el punto de vista de la regresión lineal múltiple, podemos decir que una regresión polinomial es como una regresión lineal múltiple pero con variables inventadas.\nUsando la notación de álgebra lineal, podemos escribir una versión más corta:\n\\[\\mu = \\alpha + X \\beta \\tag{3.16}\\]\nDonde \\(\\beta\\) es un vector de coeficientes de longitud \\(m\\), es decir, el número de variables dependientes. La variable \\(X\\) es una matriz de tamaño \\(m \\times n\\) si \\(n\\) es el número de observaciones y \\(m\\) es el número de variables independientes. Si estás un poco oxidado con el álgebra lineal, puedes consultar el artículo de Wikipedia sobre el producto escalar entre dos vectores y su generalización a la multiplicación de matrices. Básicamente lo que necesitamos saber por el momento es que estamos usando una forma más corta y conveniente de escribir nuestro modelo:\n\\[X \\beta = \\sum_{i=1}^n \\beta_ix_i = \\beta_1 x_1 + \\beta_2 x_2 \\dots + \\beta_m x_m \\tag{3.17} \\]\nUsando el modelo de regresión lineal simple, encontramos una línea recta que (con suerte) explica nuestros datos. Bajo el modelo de regresión lineal múltiple, encontramos, en cambio, un hiperplano de dimensión \\(m\\). Por lo tanto, el modelo de regresión lineal múltiple es esencialmente el mismo modelo de regresión lineal simple, la única diferencia es que ahora $$ es un vector y \\(X\\) es una matriz.\nVamos a definir nuestros datos:\n\nnp.random.seed(314)\nN = 100\nalpha_real = 2.5\nbeta_real = [0.9, 1.5]\neps_real = np.random.normal(0, 0.5, size=N)\n\nX = np.array([np.random.normal(i, j, N) for i,j in zip([10, 2], [1, 1.5])]).T\nX_mean = X.mean(axis=0, keepdims=True)\nX_centered = X - X_mean\ny = alpha_real + np.dot(X, beta_real) + eps_real\n\nA continuación vamos a definir una función que realiza tres gráficos de dispersión, dos entre cada variable independiente y la variable dependiente y el último entre ambas variables dependientes. Usaremos esta función para ayudarnos durante el resto del capítulo:\n\ndef scatter_plot(x, y):\n    plt.figure(figsize=(10, 10))\n    for idx, x_i in enumerate(x.T):\n        plt.subplot(2, 2, idx+1)\n        plt.scatter(x_i, y)\n        plt.xlabel('x_{}'.format(idx+1))\n        plt.ylabel('y', rotation=0)\n\n    plt.subplot(2, 2, idx+2)\n    plt.scatter(x[:,0], x[:,1])\n    plt.xlabel('x_{}'.format(idx))\n    plt.ylabel('x_{}'.format(idx+1), rotation=0)\n\nscatter_plot(X_centered, y);\n\n\n\n\nAhora sí vamos a definir, en PyMC3, un modelo adecuado para la regresión lineal múltiple. Como es de esperar el código luce muy similar al modelo de regresión lineal simple. Las principales diferencias son:\n\nLa variable \\(\\beta\\) es Gaussiana con shape = 2, es decir una pendiente por cada variable independiente.\nLa variable \\(\\mu\\) la definimos usando la función pm.math.dot()\n\nSi estás familiarizado con NumPy, probablemente sepas que NumPy también incluye una función para multiplicar matrices np.dot y desde Python 3.5 (y desde NumPy 1.10) existe un nuevo operador para multiplicar matrices @. Sin embargo, aquí usamos la función de PyMC3, que no es más que un alias para el operador de multiplicación de matrices de Theano. Necesitamos hacer esto porque la variable \\(\\beta\\) es un tensor de Theano y no un array de NumPy.\n\nwith pm.Model() as model_mlr:\n    α_tmp = pm.Normal('α_tmp', mu=0, sigma=10)\n    β = pm.Normal('β', mu=0, sigma=1, shape=2)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α_tmp + pm.math.dot(X_centered, β)\n\n    α = pm.Deterministic('α', α_tmp - pm.math.dot(X_mean, β)) \n\n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n\n    trace_mlr = pm.sample(2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α_tmp, β, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 3 seconds.\n\n\n\nvar_names = ['α', 'β','ϵ']\naz.plot_trace(trace_mlr, var_names);\n\n\n\n\n\naz.summary(trace_mlr, var_names)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      α[0]\n      1.850\n      0.452\n      1.013\n      2.704\n      0.004\n      0.003\n      10727.0\n      6419.0\n      1.0\n    \n    \n      β[0]\n      0.969\n      0.044\n      0.884\n      1.047\n      0.000\n      0.000\n      11149.0\n      6629.0\n      1.0\n    \n    \n      β[1]\n      1.470\n      0.032\n      1.414\n      1.533\n      0.000\n      0.000\n      12941.0\n      6680.0\n      1.0\n    \n    \n      ϵ\n      0.474\n      0.034\n      0.412\n      0.537\n      0.000\n      0.000\n      10672.0\n      6604.0\n      1.0\n    \n  \n\n\n\n\nComo podemos ver, nuestro modelo es capaz de recuperar los valores correctos (verificalo comparando contra los valores utilizados para generar los datos sintéticos).\nEn las siguientes secciones, nos centraremos en algunas precauciones que debemos tomar al analizar los resultados de un modelo de regresión múltiple, especialmente la interpretación de las pendientes. El principal mensaje de la siguiente sección es que en una regresión lineal múltiple, cada uno de los parámetros solo tiene sentido en el contexto del resto de los parámetros."
  },
  {
    "objectID": "05_Regresión_lineal.html#variables-de-confusión-y-variables-redundantes",
    "href": "05_Regresión_lineal.html#variables-de-confusión-y-variables-redundantes",
    "title": "6  Regresión Lineal",
    "section": "7.6 Variables de confusión y variables redundantes",
    "text": "7.6 Variables de confusión y variables redundantes\nImagina la siguiente situación. Tenemos una variable \\(z\\) correlacionada con la variable predictora \\(x\\) y, al mismo tiempo, con la variable dependiente \\(y\\). Supongamos que la variable \\(z\\) es la responsable de causar \\(x\\) e \\(y\\). Por ejemplo, \\(z\\) podría ser la revolución industrial (¡una variable realmente compleja!), \\(x\\) el número de piratas, e \\(y\\) la concentración de \\(CO_2\\). Este ejemplo debería ser muy familiar para lectores Pastafariano. Si omitimos \\(z\\) de nuestro análisis, podríamos terminar con una buena relación lineal entre \\(x\\) e \\(y\\), incluso podríamos predecir \\(x\\) a partir de \\(y\\). Sin embargo, si nuestro interés radica en minimizar el calentamiento global, podríamos pasar por alto lo que realmente está sucediendo con el mecanismo subyacente que relaciona estas variables.\nLo que intento decir es la conocida expresión correlación no implica causalidad. Una razón por la que esto no es necesariamente cierto es que podemos estar omitiendo la variable \\(z\\) de nuestro análisis. Cuando esto sucede, llamamos a \\(z\\) variable de confusión o factor de confusión. En muchos escenarios reales \\(z\\) es fácil de perder de vista. Tal vez no la medimos o no estaba presente en el conjunto de datos que nos enviaron, o ni siquiera pensamos que \\(z\\) podría estar relacionado con nuestro problema. No tomar en cuenta las variables de confusión en un análisis podría llevarnos a establecer correlaciones falsas. Esto siempre es un problema cuando tratamos de explicar algo y también puede ser problemático cuando tratamos de predecir algo sin preocuparnos por comprender el mecanismo subyacente. Comprender el mecanismo nos ayuda a traducir lo que hemos aprendido a situaciones nuevas; las predicciones ciegas no siempre tienen buena transferibilidad. Por ejemplo, la cantidad de zapatillas producidas en Argentina podría utilizarse como un indicador simple para estimar la fortaleza de su economía, pero podría ser un pésimo predictor para otros países con una matriz de producción o un contexto cultural diferente.\nA continuación vamos a usar datos sintéticos para explorar un poco el concepto de variable de confusión. El siguiente código simula una variable de confusión como \\(x_1\\). Esta variable tiene influencia tanto en \\(x_2\\) como en \\(y\\):\n\nnp.random.seed(42)\nN = 100\nx_1 = np.random.normal(size=N)\nx_2 = x_1 + np.random.normal(size=N, scale=1)\n#x_2 = x_1 + np.random.normal(size=N, scale=0.01)  \ny = x_1 + np.random.normal(size=N)\nX = np.vstack((x_1, x_2)).T\n\nDebido a como generamos los datos estos ya están centrados como se puede ver en la siguiente figura:\n\nscatter_plot(X, y);\n\n\n\n\nAhora vamos a construir tres modelos relacionados, el primero m_x1x2, es un modelo de regresión lineal con dos variables independientes \\(x_1\\) y \\(x_2\\) (apilados en la variable \\(X\\)). El segundo modelo, m_x1, es una regresión lineal simple para y el tercero, m_x2, una regresión de línea simple para \\(x_2\\):\n\nwith pm.Model() as m_x1x2:\n    α = pm.Normal('α', mu=0, sigma=10)\n    β1 = pm.Normal('β1', mu=0, sigma=10)\n    β2 = pm.Normal('β2', mu=0, sigma=10)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α + β1 * X[:,0] + β2 * X[:,1] \n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n \n    idata_x1x2 = pm.sample(2000)\n    \n    \nwith pm.Model() as m_x1:\n    α = pm.Normal('α', mu=0, sigma=10)\n    β1 = pm.Normal('β1', mu=0, sigma=10)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α + β1 * X[:,0]\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n \n    idata_x1 = pm.sample(2000)\n    \nwith pm.Model() as m_x2:\n    α = pm.Normal('α', mu=0, sigma=10)\n    β2 = pm.Normal('β2', mu=0, sigma=10)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α + β2 * X[:,1]\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n \n    idata_x2 = pm.sample(2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β1, β2, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 3 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β1, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 2 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β2, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 2 seconds.\n\n\nA continuación usamos un forestplot a fin de comparar los valores de \\(\\beta\\) para estos 3 modelos.\n\nax = az.plot_forest([idata_x1x2, idata_x1, idata_x2], model_names=['m_x1x2', 'm_x1', 'm_x2'], var_names=['β1', 'β2'],\n                    r_hat=False, ess=False, combined=True, colors='cycle',\n                    kind=\"ridgeplot\",\n                    figsize=(9, 4));\n\n\n\n\nComo podemos ver \\(\\beta_2\\) para el modelo m_x1x2 es alrededor de cero, lo que indica una contribución casi nula de la variable \\(x_2\\) para explicar \\(y\\). Esto es interesante porque ya sabemos que la variable realmente importante es \\(x_1\\) (como se puede verificar en la generación de datos sintéticos). También hay que notar, y esto es realmente importante, \\(\\beta_2\\) para el modelo m_x2 es alrededor de 0.55, es decir es más grande que para el modelo m_x1x2. En otras palabras el poder de \\(x_2\\) para predecir \\(y\\) se reduce cuando tenemos en cuenta \\(x_1\\), ya que la información en \\(x_1\\) es redundante dado \\(x_2\\)."
  },
  {
    "objectID": "05_Regresión_lineal.html#multicolinearidad-o-cuando-la-correlación-es-demasiado-alta",
    "href": "05_Regresión_lineal.html#multicolinearidad-o-cuando-la-correlación-es-demasiado-alta",
    "title": "6  Regresión Lineal",
    "section": "7.7 Multicolinearidad o cuando la correlación es demasiado alta",
    "text": "7.7 Multicolinearidad o cuando la correlación es demasiado alta\nLlevemos el ejemplo anterior a un extremo y veamos qué sucede cuando dos variables están altamente correlacionadas. Para estudiar este problema y sus consecuencias para la inferencia, utilizaremos los mismos datos y modelos sintéticos que antes, pero ahora aumentaremos el grado de correlación entre \\(x_1\\) y \\(x_2\\) al reducir la cantidad de ruido Gaussiano que agregamos a \\(x_1\\) para obtener \\(x_2\\).\n\n# Este es el mismo código de una celdas antes pero con una valor más bajo de `scale`\nnp.random.seed(42)\nN = 100\nx_1 = np.random.normal(size=N)\nx_2 = x_1 + np.random.normal(size=N, scale=0.01)  \ny = x_1 + np.random.normal(size=N)\nX = np.vstack((x_1, x_2)).T\n\nEl cambio en la generación de datos, de la celda anterior, prácticamente equivale a sumar 0 a \\(x_1\\), por lo tanto ambas variables son iguales para todo propósito práctico. A continuación, pueden intentar variar los valores de la desviación estándar y usar valores menos extremos, pero por ahora queremos que el efecto sea bien claro. En el siguiente gráfico se puede ver que ahora el diagrama de dispersión para \\(x_1\\) y \\(x_2\\) es prácticamente una línea recta con una pendiente alrededor de 1:\n\nscatter_plot(X, y)\n\n\n\n\nEjecutamos una regresión lineal múltiple:\n\nwith pm.Model() as model_red :\n    α = pm.Normal('α', mu=0, sigma=10)\n    β = pm.Normal('β', mu=0, sigma=10, shape=2)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α + pm.math.dot(X, β)\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n \n    idata_red = pm.sample(2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 01:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 63 seconds.\n\n\nY verificamos los resultados de los parámetros con un forestplot:\n\naz.plot_forest(idata_red, var_names=['β'], r_hat=0, ess=0, combined=1, figsize=(8, 2));\n\n\n\n\nLos valores de los HDI para \\(\\beta\\) son sospechosamente amplios, podemos obtener una idea de lo que está sucediendo con un diagrama de dispersión de los coeficientes \\(\\beta\\)\n\naz.plot_pair(idata_red, var_names=['β'], scatter_kwargs={'alpha':0.05});\n\n\n\n\nLa distribución de los coeficientes \\(\\beta\\) es una diagonal realmente estrecha. Cuando un coeficiente \\(\\beta\\) sube, el otro debe bajar. Ambos están efectivamente correlacionados. Esto es solo una consecuencia del modelo y los datos. Según nuestro modelo, la media \\(\\mu\\) es:\n\\[ \\mu = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 \\tag{3.19} \\]\nSi suponemos que \\(x_1\\) y \\(x_2\\) no son prácticamente equivalentes, sino matemáticamente idénticos, podemos volver a escribir el modelo como:\n\\[ \\mu = \\alpha + (\\beta_1 + \\beta_2) x \\tag{3.20} \\]\nResulta que es la suma \\(\\beta_1 + \\beta_2\\) y no sus valores separados, lo que afecta \\(\\mu\\). Podemos hacer que \\(\\beta_1\\) sea más pequeño y más pequeño siempre que hagamos que \\(\\beta_2\\) sea más y más grande. Los resultados nos están diciendo que como prácticamente NO tenemos dos variables \\(x\\), prácticamente NO tenemos dos parámetros \\(\\beta\\). Decimos entonces que el modelo está indeterminado (o de forma equivalente, los datos no pueden restringir los parámetros en el modelo). En nuestro ejemplo, hay dos razones por las cuales \\(\\beta\\) no se mueve libremente en el intervalo \\((-\\infty, \\infty)\\). Primero, ambas variables son casi las mismas, pero no son exactamente iguales, en segundo lugar, y lo más importante, los a prioris actuan como restricciones de los valores plausibles que \\(\\beta\\) puede tomar.\nHay un par de cosas que notar de este ejemplo. En primer lugar, el a posteriori es tan solo la consecuencia lógica de los datos y el modelo, por lo tanto no hay nada de malo en obtener distribuciones tan amplias para \\(\\beta\\), C’est la vie. En segundo lugar, podemos confiar en este modelo para hacer predicciones ya que los valores predichos por el modelo están de acuerdo con los datos, es decir el modelo captura los datos muy bien. En tercer lugar, este puede no ser un modelo muy bueno para comprender nuestro problema. Puede ser más inteligente simplemente eliminar una de las variables del modelo. Terminaremos teniendo un modelo que predice los datos igual que antes, pero con una interpretación más simple.\nEn cualquier conjunto de datos real, van a existir variables (parcialmente) correlacionadas. Esto sucede por al menos dos razones: la existencia de correlaciones espurias y quizá lo más relevante; al estudiar un problema tendemos a recolectar información que consideramos relevante pero que puede ser parcialmente redundante. Por ejemplo, la cantidad de radiación solar, la temperatura y las precipitaciones son factores que influyen para predecir el rinde de un cultivo, y son variables que tienen a estar correlacionadas por ejemplo si el verano es la temporada de mayores lluvias.\n¿Qué tan fuerte deben correlacionarse dos o más variables para convertirse en un problema? Bueno se suele considerar que a partir de 0.9845. No, mentira! Desafortunadamente, la estadística es una disciplina con muy pocos números mágicos. Siempre es posible hacer una matriz de correlación antes de ejecutar cualquier modelo y verificar las variables con una alta correlación de, digamos por encima de 0.9 o más. Sin embargo, el problema con este enfoque es que lo que realmente importa no son las correlaciones por pares que podemos observar en una matriz de correlación, sino la correlación de las variables dentro de un modelo, y como ya vimos, las variables se comportan de forma diferente cuando están aisladas que cuando se relacionan dentro de un modelo. Dos o más variables pueden aumentar o disminuir su correlación cuando se colocan en el contexto de otras variables en un modelo de regresión múltiple. Como siempre, una inspección cuidadosa de la distribución a posteriori junto con una aproximación iterativa y crítica del modelado, son muy recomendables y pueden ayudarnos a detectar problemas y comprender los datos y los modelos.\nSolo como una guía rápida (a tomar con pinzas). ¿Qué deberíamos hacer si encontramos variables altamente correlacionadas?\n\nSi la correlación es realmente alta, podemos eliminar una de las variables del análisis; dado que ambas variables tienen información similar, cual eliminamos suele ser a menudo irrelevante. Podemos eliminar variables basadas en nuestra conveniencia, por ejemplo eliminar variables menos conocidas o más difíciles de interpretar o las más costosas de medir.\nOtra posibilidad es crear una nueva variable promediando las variables redundantes. Una versión más sofisticada es usar un algoritmo de reducción de variables como un análisis de componentes principales (PCA). El problema con PCA es que las variables resultantes son combinaciones lineales de las originales que ofuscan, en general, la interpretabilidad de los resultados.\nOtra solución es utilizar a prioris más fuertes para restringir los valores plausibles que puede adoptar el coeficiente, en este contexto los a prioris se usan como regularizadores de la inferencia (algo que discutiremos más adelante)."
  },
  {
    "objectID": "05_Regresión_lineal.html#variables-de-efecto-de-enmascaramiento",
    "href": "05_Regresión_lineal.html#variables-de-efecto-de-enmascaramiento",
    "title": "6  Regresión Lineal",
    "section": "7.8 Variables de efecto de enmascaramiento",
    "text": "7.8 Variables de efecto de enmascaramiento\nOtro ejemplo de cómo las variables contribuyen a un resultado es el caso de las variables que enmascaran. Vamos a crear dos variables independientes (\\(x_1\\) y \\(x_2\\)), las cuales están positivamente correlacionadas entre sí y están correlacionadas con \\(y\\), pero en direcciones opuestas \\(x_1\\) está correlacionada positivamente y \\(x_2\\) correlacionada negativamente.\n\nnp.random.seed(42)\nN = 126\nr = 0.8\nx_1 = np.random.normal(size=N)\nx_2 = np.random.normal(x_1, scale=(1 - r ** 2) ** 0.5)\ny = np.random.normal(x_1 - x_2)\nX = np.vstack((x_1, x_2)).T\n\n\nscatter_plot(X, y);\n\n\n\n\nComo hicimos antes, vamos a construir 3 modelos relacionados, el primero m_x1x2, es un modelo de regresión lineal con dos variables independientes \\(x_1\\) y \\(x_2\\) (apilados en la variable X). El segundo modelo, m_x1, es una regresión lineal simple para \\(x_1\\) y el tercero, m_x2, una regresión lineal simple para \\(x_2\\).\n\nwith pm.Model() as m_x1x2:\n    α = pm.Normal('α', mu=0, sigma=10)\n    β1 = pm.Normal('β1', mu=0, sigma=10)\n    β2 = pm.Normal('β2', mu=0, sigma=10)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α + β1 * X[:,0] + β2 * X[:,1] \n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n \n    trace_x1x2 = pm.sample(1000)\n    \n    \nwith pm.Model() as m_x1:\n    α = pm.Normal('α', mu=0, sigma=10)\n    β1 = pm.Normal('β1', mu=0, sigma=10)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α + β1 * X[:,0]\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n \n    trace_x1 = pm.sample(1000)\n    \nwith pm.Model() as m_x2:\n    α = pm.Normal('α', mu=0, sigma=10)\n    β2 = pm.Normal('β2', mu=0, sigma=10)\n    ϵ = pm.HalfCauchy('ϵ', 5)\n\n    μ = α + β2 * X[:,1]\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n \n    trace_x2 = pm.sample(1000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β1, β2, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β1, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β2, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\nEchemos un vistazo a los parámetros \\(\\beta\\) usando un forestplot para compararlos en un solo diagrama.\n\naz.plot_forest([trace_x1x2, trace_x1, trace_x2], model_names=['m_x1x2', 'm_x1', 'm_x2'],\n               var_names=['β1', 'β2'], r_hat=False, ess=False, combined=True, \n               kind=\"ridgeplot\", colors='cycle',\n               figsize=(10, 4));\n\n\n\n\nDe acuerdo a la distribución a posteriori, los valores de \\(\\beta\\) para m_x1x2 están cerca de 1 y -1 (como se esperaba, de acuerdo a la forma en que generamos los datos). Para el modelo de regresión lineal simple, es decir, cuando estudiamos cada variable por separado, podemos ver que los valores de \\(\\beta\\) son más cercanos a cero, lo que indica un efecto más débil.\nLo que sucede es que \\(x_1\\) está correlacionado con \\(x_2\\), cuando \\(x_1\\) aumenta \\(x_2\\) también aumenta. Además \\(y\\) aumenta, \\(x_1\\) también aumenta, pero \\(x_2\\) disminuye. Como resultado de este arreglo particular, obtenemos una cancelación parcial de los efectos a menos que incluyamos ambas variables en la misma regresión lineal. El modelo de regresión lineal puede desenredar estos efectos porque el modelo está aprendiendo, para cada punto de datos, cuál es la contribución de \\(x_1\\) a \\(y\\) dado un valor de \\(x_2\\), y al revés por \\(x_2\\)."
  },
  {
    "objectID": "05_Regresión_lineal.html#resumen",
    "href": "05_Regresión_lineal.html#resumen",
    "title": "6  Regresión Lineal",
    "section": "7.9 Resumen",
    "text": "7.9 Resumen\nUna regresión lineal simple es un modelo que puede usarse para predecir y/o explicar una variable desde otra. Desde una perspectiva probabilista, un modelo de regresión lineal es una extensión del modelo Gaussiano donde la media no se estima directamente, sino que se calcula como una función lineal de una variable de predicción y algunos parámetros adicionales. Si bien la distribución gaussiana es la opción más común para la variable dependiente, somos libres de elegir otras distribuciones. Una alternativa especialmente útil cuando se trata de posibles valores atípicos, es la distribución t de Student. Otra forma útil de expandir un modelo de regresión lineal es haciendo una versión jerárquica de él. Esto es muy simple de lograr con PyMC y obtenemos los beneficios del shrinkage."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html",
    "href": "06_Generalizando_modelos_lineales.html",
    "title": "7  Generalizando modelos lineales",
    "section": "",
    "text": "8 Regresión logística múltiple\nDe manera similar a la regresión lineal múltiple, la regresión logística múltiple consiste en utilizar más de una variable independiente. Intentemos combinar la longitud del sépalo y el ancho del sépalo. Recuerda que necesitamos preprocesar un poco los datos."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#regresión-polinomial",
    "href": "06_Generalizando_modelos_lineales.html#regresión-polinomial",
    "title": "7  Generalizando modelos lineales",
    "section": "7.1 Regresión polinomial",
    "text": "7.1 Regresión polinomial\nAhora vamos a aprender cómo ajustar curvas usando una regresión lineal. Una manera de ajustar curvas usando un modelo de regresión lineal es construyendo un polinomio como este:\n\\[\\mu = \\beta_0 x^0 + \\beta_1 x^1  \\dots + \\beta_m x^m \\tag{3.12} \\]\nSi prestamos atención, podemos ver que este polinomio esconde un modelo lineal simple. De hecho si hacemos que \\(\\beta_n = 0\\) para \\(n \\gt 1\\) obtendremos:\n\\[\\mu = \\beta_0 + \\beta_1 x^1 \\tag{3.13} \\]\nQue no es otra cosa que la ecuación de una recta. Una regresión polinomial sigue siendo una regresión lineal, ya que la linearidad del modelo está relacionada con la forma en que los parámetros entran en el modelo y no con las variables. Probemos construyendo una regresión polinomial de grado 2.\n\\[\\mu = \\beta_0 + \\beta_1 x^1 + \\beta_2 x^2 \\tag{3.14} \\]\nEl tercer término controla la curvatura de la relación como veremos a continuación.\nComo un conjunto de datos, vamos a utilizar el segundo grupo del cuarteto de Anscombe.\n\nans = pd.read_csv('datos/anscombe.csv')\nx_2 = ans[ans.group == 'II']['x'].values\ny_2 = ans[ans.group == 'II']['y'].values\nx_2 = x_2 - x_2.mean()\ny_2 = y_2 - y_2.mean()\n\nplt.scatter(x_2, y_2)\nplt.xlabel('x')\nplt.ylabel('y', rotation=0);\n\n\n\n\n\nwith pm.Model() as model_poly:\n    α = pm.Normal('α', mu=y_2.mean(), sigma=1)\n    β1 = pm.Normal('β1', mu=0, sigma=1)\n    β2 = pm.Normal('β2', mu=0, sigma=1)\n    ϵ = pm.HalfNormal('ϵ', 10)\n\n    mu = α + β1 * x_2 + β2 * x_2**2\n    \n    y_pred = pm.Normal('y_pred', mu=mu, sigma=ϵ, observed=y_2)\n    \n    idata_poly = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β1, β2, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\nOnce again, we are going to omit some checks and summaries and just plot the results, a nice curved line fitting the data almost with no errors. Take into account the minimalistic nature of the dataset.\n\naz.plot_trace(idata_poly);\n\n\n\n\n\naz.summary(idata_poly)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      α\n      1.267\n      0.001\n      1.265\n      1.269\n      0.0\n      0.0\n      1857.0\n      1551.0\n      1.0\n    \n    \n      β1\n      0.500\n      0.000\n      0.500\n      0.500\n      0.0\n      0.0\n      2638.0\n      1874.0\n      1.0\n    \n    \n      β2\n      -0.127\n      0.000\n      -0.127\n      -0.127\n      0.0\n      0.0\n      1911.0\n      1786.0\n      1.0\n    \n    \n      ϵ\n      0.002\n      0.001\n      0.001\n      0.003\n      0.0\n      0.0\n      1433.0\n      1578.0\n      1.0\n    \n  \n\n\n\n\n\nx_p = np.linspace(-6, 6)\npost_mean = idata_poly.posterior.mean((\"chain\", \"draw\"))\n\ny_p = post_mean['α'].item() + post_mean['β1'].item() * x_p + post_mean['β2'].item() * x_p**2\nplt.scatter(x_2, y_2)\nplt.xlabel('x',)\nplt.ylabel('y', rotation=0)\nplt.plot(x_p, y_p, c='C1');\n\n\n\n\n\n7.1.1 Interpretando los parámetros de una regresión polinomial\nUno de los problemas de la regresión polinómica es la interpretación de sus parámetros. Si queremos saber cómo cambia \\(y\\) por unidad de cambio de \\(x\\), no podemos simplemente verificar el valor de \\(\\beta_1\\), ya que \\(\\beta_2\\), y los coeficientes más altos (de estar presentes), tendrán un efecto en dicha cantidad. Entonces los coeficientes \\(\\beta\\) ya no son pendientes, son otra cosa. En el ejemplo anterior, \\(\\beta_1\\) es positivo y, por lo tanto, la curva comienza con una pendiente positiva, pero \\(\\beta_2\\) es negativo y, por lo tanto, después de un tiempo, la línea comienza a curvarse hacia abajo. Es como si tuviéramos dos fuerzas en juego, una empujando la línea hacia arriba y la otra hacia abajo. La interacción depende del valor de \\(x\\). Cuando \\(x \\lessapprox 11\\) (en la escala original, o 2 en la escala centrada), la contribución dominante proviene de \\(\\beta_1\\), y cuando \\(x \\gtrapprox 11\\), entonces \\(\\beta_2\\) domina.\nEl principal problema de interpretar los parámetros en modelos polinomiales, es que en general los parámetros no se traducen a cantidades que tengan sentido a la luz de nuestro conocimiento de dominio. Es decir no podemos relacionarlos con la tasa metabólica de una célula, la energía emitida por una galaxia o el número de habitaciones en una casa. Los parámetros terminan siendo simplemente perillas que podemos manipular para mejorar el ajuste pero sin un significado claro. En la práctica, la mayoría de la gente suele estar de acuerdo en que los polinomios de orden superior a dos o tres generalmente no son modelos muy útiles y se prefieren alternativas, quizá como splines o los Procesos Gaussianos.\nEn este trabajo se propone una versión interpretable (y no lineal) de un polinomio de grado 2.\n\\[\n\\alpha_y - (\\alpha_y - \\alpha_0) \\left(\\frac{x_i}{\\alpha_x} -1\\right)^2\n\\] * \\(\\alpha_0\\) : intercepto, valor de \\(Y\\) cuando \\(x=0\\) * \\(\\alpha_x\\) : valor de \\(x_i\\) que maximiza/minimiza \\(Y\\) * \\(\\alpha_y\\) : valor máximo/mínimo de \\(Y\\)"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#varianza-variable",
    "href": "06_Generalizando_modelos_lineales.html#varianza-variable",
    "title": "7  Generalizando modelos lineales",
    "section": "7.2 Varianza variable",
    "text": "7.2 Varianza variable\nHemos estado usando el modelo lineal para modelar la media de una distribución, dejando la varianza de lado. En caso que consideremos que el supuesto de varianza constante no tiene sentido podemos considerar la varianza como una función (lineal) de la variable dependiente.\nLa Organización Mundial de la Salud y otras instituciones de salud de todo el mundo recopilan datos para recién nacidos y adultos mayores y diseñan estándares de gráficos de crecimiento. Estas tablas son un componente esencial del conjunto de herramientas pediátricas y también como una medida del bienestar general de las poblaciones con el fin de formular políticas de salud, planificar intervenciones y controlar su eficacia. Un ejemplo de tales datos son la longitud (alturas) de las niñas recién nacidas en función de la edad (en meses):\n\ndata = pd.read_csv('datos/babies.csv')\ndata.plot.scatter('Meses', 'Longitud');\n\n\n\n\nPara modelar estos datos, presentaremos 3 elementos nuevos en comparación con los modelos anteriores:\n\n\\(\\epsilon\\) ahora es una función lineal de \\(x\\), y para hacer esto agregamos dos nuevos parámetros \\(\\gamma\\) y \\(\\delta\\), estos son análogos directos de \\(\\alpha\\) y \\(\\beta\\).\nEl modelo lineal para la media es una función de \\(\\sqrt{x}\\), esto es solo un truco simple para ajustar un modelo lineal a una curva.\nHemos definido una variable compartida x_shared, esto nos permitirá cambiar los valores de la variable \\(x\\) (Meses en este ejemplo) sin la necesidad de volver a muestrear el modelo. Por qué hacemos estos será evidente pronto si tienen un poco de paciencia.\n\n\nwith pm.Model() as model_vv:\n    x_shared = pm.MutableData(\"x_shared\", data.Meses.values.astype(float))\n    α = pm.Normal('α', sigma=10)\n    β = pm.Normal('β', sigma=10)\n    γ = pm.HalfNormal('γ', sigma=10)\n    δ = pm.HalfNormal('δ', sigma=10)\n\n\n    μ = pm.Deterministic('μ', α + β * x_shared**0.5)\n    ϵ = pm.Deterministic('ϵ', γ + δ * x_shared)\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=data.Longitud)\n    \n    idata_vv = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β, γ, δ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\nLa siguiente figura muestra el resultado de nuestro modelo. La media de es \\(\\mu\\) representada con una curva negra, y dos bandas turquesa semitransparentes representan 1 y 2 desviaciones estándar.\n\n_, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(data.Meses, data.Longitud, 'C0.', alpha=0.1);\n\nposterior = az.extract(idata_vv)\n\nμ_m = posterior['μ'].mean(\"sample\").values\nϵ_m = posterior['ϵ'].mean(\"sample\").values\n\naxes[0].plot(data.Meses, μ_m, c='k')\naxes[0].fill_between(data.Meses, μ_m + 1 * ϵ_m, μ_m - 1 * ϵ_m, alpha=0.6, color='C1')\naxes[0].fill_between(data.Meses, μ_m + 2 * ϵ_m, μ_m - 2 * ϵ_m, alpha=0.4, color='C1')\n\naxes[0].set_xlabel('Meses')\naxes[0].set_ylabel('Longitud');\n\n\naxes[1].plot(data.Meses, ϵ_m)\naxes[1].set_xlabel('Meses');\naxes[1].set_ylabel(r'$\\bar ϵ$', rotation=0);\n\n\n\n\nAhora que tenemos ajustado el modelo podríamos querer averiguar cómo se compara la longitud de una niña en particular con el modelo. Una forma de responder a esta pregunta es preguntarle al modelo por la distribución de la variable longitud para bebas de digamos de 0.5 meses. Usando PyMC3 podemos hacer estas preguntas con la función sample_ppc, ya que esto arrojará muestras de \\(\\tilde y\\) es decir los valores predichos considerando la incertidumbre de los parámetros. El único problema es que, por defecto, esta función devolverá valores de \\(\\tilde y\\) para los valores observados de \\(x\\) y de 0,5 meses (el valor que me importa) no es parte de los datos originales. La manera más fácil de obtener predicciones para valores no observados es definir una variable compartida \\(x\\) (como parte del modelo) y luego actualizar el valor de la variable compartida justo antes del muestreo de la distribución predictiva a posteriori.\n\nwith model_vv:\n    pm.set_data({\"x_shared\": [0.5]})\n    ppc = pm.sample_posterior_predictive(idata_vv)\n    y_ppc = ppc.posterior_predictive['y_pred'].stack(sample=(\"chain\", \"draw\"))\n\nSampling: [y_pred]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00<00:00]\n    \n    \n\n\nAhora podemos graficar la distribución esperada de las longitudes para las bebas con 2 semanas de vida y calcular cantidades adicionales, por ejemplo, el percentil de un niño para su longitud:\n\nref = 52.5\ngrid, pdf = az.stats.density_utils._kde_linear(y_ppc.values)\nplt.plot(grid, pdf)\npercentile = int((y_ppc <= ref).mean() * 100)\nplt.fill_between(grid[grid < ref], pdf[grid < ref], label='percentil = {:2d}'.format(percentile))\nplt.xlabel('longitud')\nplt.yticks([])\nplt.legend();"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#modelos-lineales-generalizados",
    "href": "06_Generalizando_modelos_lineales.html#modelos-lineales-generalizados",
    "title": "7  Generalizando modelos lineales",
    "section": "7.3 Modelos lineales generalizados",
    "text": "7.3 Modelos lineales generalizados\nHasta el momento hemos asumido que la variable dependiente puede variar, a priori, sin restricciones y por ello usamos como likelihood una Gaussiana (o una generalización de esta como lo es la t de Student). Pero que podemos hacer si la variable respuesta está restringida a ser positiva, o discreta, o en el intervalo [0, 1], etc. Usando software como PyMC bastaría con cambiar el likelihood de una Gaussiana a una Poisson (o lo que haga falta), pero si solo hacemos eso tendremos problemas, ya que la combinación lineal de parámetros podría dar valores fuera del rango permitido (por ej negativos). Para subsanar este problema podemos aplicar una función a la combinación lineal de variables de entrada, algo como:\n\\[\\mu = f(\\alpha + X \\beta) \\tag{4.1}\\]\ndonde \\(f\\) es lo que se conoce como función inversa de enlace. Hay una gran variedad de funciones inversas de enlace que podemos elegir, probablemente la más simple sea la función identidad. Esta es una función que devuelve el mismo valor utilizado como argumento. Todos los modelos del capítulo anterior usaron la función de identidad, y por simplicidad simplemente la omitimos. La función de identidad puede no ser muy útil en sí misma, pero nos permite pensar en varios modelos diferentes de una manera unificada.\n\n¿Por qué llamamos a \\(f\\) función inversa de enlace en lugar de llamarla simplemente función de enlace? La razón es histórica. Tradicionalmente las personas aplican funciones al otro lado de la ecuación \\(4.1\\), y llaman a esas funciones funciones de enlace, por lo tanto, para evitar confusiones, nos apegaremos al término función inveras de enlace.\n\nVeamos algunos ejemplos concretos de modelos lineales generalizados"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#regresión-logística",
    "href": "06_Generalizando_modelos_lineales.html#regresión-logística",
    "title": "7  Generalizando modelos lineales",
    "section": "7.4 Regresión logística",
    "text": "7.4 Regresión logística\nLa regresión logistica es la generalización del modelo de regresión que vimos en el capítulo pasado para cuando la variable dependiente es binaria. Esta generalización se logra en dos pasos. Primero reemplazamos \\(f\\) en \\(4.1\\) por la función logística:\n\\[ \\text{logística}(z) = \\frac{1}{1 + e^{-z}} \\tag{4.2}\\]\nUsamos esta función por que una de sus propiedades es que no importa el valor del argumento \\(z\\), el resultado siempre será un valor en el intervalo [0-1]. La función logística es conocida también como función sigmoide, por su aspecto típico de S como se puede ver al ejecutar la siguiente celda:\n\nz = np.linspace(-6, 6)\nlogística = 1 / (1 + np.exp(-z))\nplt.plot(z, logística)\nplt.xlabel('z')\nplt.ylabel('logística(z)');\n\n\n\n\nEl segundo paso consiste en usar como likelihood una distribución binomial y no una Gaussiana. De esta forma el modelo queda expresado como:\n\\[\n\\theta = logistic(\\alpha + x\\beta) \\\\\ny = \\text{Bern}(\\theta) \\tag{4.3}\n\\]\nEsto modelo se puede motivar de la siguiente forma. Si nuestros datos son binarios \\(y \\in \\{0, 1\\}\\), como con el ejemplo de la moneda o el diagnóstico, vemos que tiene sentido usar una distribución bernoulli. Esta distribución está parametrizada por un único parámetro en el intervalo [0, 1], el cual puede ser generado desde un modelo lineal siempre y cuando los valores generados por el modelo lineal sean comprimidos al intervalo [0, 1], algo que puede ser obtenido al emplear una función logística.\nUsando un diagrama de Kruschke una regresión logística con priors Gaussianos:"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#el-conjunto-de-datos-del-iris",
    "href": "06_Generalizando_modelos_lineales.html#el-conjunto-de-datos-del-iris",
    "title": "7  Generalizando modelos lineales",
    "section": "7.5 El conjunto de datos del Iris",
    "text": "7.5 El conjunto de datos del Iris\nVamos a aplicar una regresión logística al conjunto de datos Iris. Este es un conjunto de datos clásico que contiene información sobre flores de 3 especies estrechamente relacionadas: setosa, virginica y versicolor. Estas serán nuestras variables dependientes, las clases que queremos predecir. Tenemos 50 individuos de cada especie y para cada individuo el conjunto de datos contiene cuatro variables (o features) que vamos a usar como variables independientes. Estas son el largo del pétalo, el ancho del pétalo, el largo del sépalo y el ancho del sépalo. Por si se lo están preguntando, los sépalos son hojas modificadas, cuya función está generalmente relacionada con la protección de las flores en la yema.\nPodemos cargar un DataFrame con el conjunto de datos del iris haciendo:\n\niris = pd.read_csv('datos/iris.csv')\niris.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\nAhora graficaremos las 3 especies versus la longitud del sépalo usando la función stripplot de seaborn:\n\nsns.stripplot(x=\"species\", y=\"sepal_length\", data=iris, hue=\"species\", jitter=True, legend=False);\n\n\n\n\nObserve en la figura 4.2 que en el eje y se representan una variable continua mientras que en el eje x la variable es categórica. La dispersión (o jitter) de los puntos a lo largo del eje x no tiene ningún significado, y es solo un truco para evitar que todos los puntos colapsen en una sola línea (pueden probar pasando el argumento jitter=False). Por lo tanto lo único que importa al leer el eje x es la pertenencia de los puntos a las clases setosa, versicolor o virginica.\nOtra forma de inspeccionar los datos es haciendo una matriz de dispersión con la función pairplot. En la figura 4.3 podemos ver una matriz de \\(4 \\times 4\\), ya que tenemos 4 variables independientes (o features). La matriz es simétrica con los triángulos superior e inferior conteniendo la misma información. En la diagonal principal en vez de tener una gráfico de dispersión de una variable contra si misma (lo cual no es informativo) tenemos un KDE de cada feature para cada especie (o clase). Cada especie está representada usando un color particular.\n\nsns.pairplot(iris, hue='species', plot_kws={\"legend\":False});\n\n/home/osvaldo/anaconda3/envs/bayes/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\nAntes de continuar, tómese un tiempo para estudiar las gráficas anteriores y familiarizarse con el conjunto de datos y cómo se relacionan las variables dependientes y las independientes.\n\n7.5.1 El modelo logístico aplicado al conjunto de datos del iris.\nVamos a comenzar con la regresión logística más simple posible: dos clases, setosa y versicolor, y solo una variable independiente, la longitud del sépalo. Como se hace normalmente, vamos a codificar las variables categóricas setosa y versicolor con los números 0 y 1. Usando Pandas podemos hacer:\n\ndf = iris.query(\"species == ('setosa', 'versicolor')\")\ny_0 = pd.Categorical(df['species']).codes\nx_n = 'sepal_length' \nx_0 = df[x_n].values\nx_c = x_0 - x_0.mean()\n\nAl igual que con otros modelos lineales, centrar los datos puede ayudar con el muestreo. Ahora que tenemos los datos en el formato adecuado, finalmente podemos construir el modelo con PyMC.\nObserve cómo la primera parte del siguiente modelo se asemeja a un modelo de regresión lineal. Este modelo tiene dos variables deterministas: θ ybd. θ es la salida de la función logística aplicada a la variable μ y bd es límite de decisión (el cual explicaremos más adelante).Otro punto que vale la pena mencionar es que en lugar de escribir explícitamente la función logística estamos usando pm.math.sigmoid (esto es un alias para una función de Theano).\n\nwith pm.Model() as modelo_0:\n    α = pm.Normal('α', mu=0, sigma=10)\n    β = pm.Normal('β', mu=0, sigma=10)\n    \n    μ = α + pm.math.dot(x_c, β)    \n    θ = pm.Deterministic('θ', pm.math.sigmoid(μ))\n    bd = pm.Deterministic('bd', -α/β)\n    \n    yl = pm.Bernoulli('yl', p=θ, observed=y_0)\n\n    idata_0 = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nComo es habitual, también mostramos el summary del posterior. Más adelante, compararemos el valor que obtengamos para el límite de decisión con un valor calculado utilizando otro método.\n\naz.plot_trace(idata_0, var_names='~θ');\n\n\n\n\n\naz.summary(idata_0, var_names='~θ')\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      α\n      0.300\n      0.332\n      -0.322\n      0.907\n      0.006\n      0.005\n      3040.0\n      2833.0\n      1.0\n    \n    \n      β\n      5.376\n      1.064\n      3.511\n      7.416\n      0.020\n      0.014\n      3087.0\n      2711.0\n      1.0\n    \n    \n      bd\n      -0.055\n      0.061\n      -0.169\n      0.060\n      0.001\n      0.001\n      3104.0\n      2881.0\n      1.0\n    \n  \n\n\n\n\nAhora vamos a graficar los datos junto con la curva sigmoide ajustada:\n\n_, ax = plt.subplots(figsize=(10, 6))\n\npost_0 = az.extract(idata_0)\n\ntheta = post_0['θ'].mean(\"sample\")\nidx = np.argsort(x_c)\nax.plot(x_c[idx], theta[idx], color='C8', lw=3)\nax.vlines(post_0['bd'].mean(\"sample\"), 0, 1, color='k')\nbd_hdi = az.hdi(post_0.unstack())[\"bd\"]\nax.fill_betweenx([0, 1], bd_hdi[0], bd_hdi[1], color='k', alpha=0.5)\n\nax.scatter(x_c, np.random.normal(y_0, 0.02), marker='.', color=[f'C{x}' for x in y_0])\ntheta_hdi = az.hdi(post_0.unstack())['θ'][idx]\nax.fill_between(x_c[idx], theta_hdi[:,0], theta_hdi[:,1], color='C8', alpha=0.5)\n\nax.set_xlabel(x_n)\nax.set_ylabel('θ', rotation=0, labelpad=20)\n# use original scale for xticks\nlocs, _ = plt.xticks() \nax.set_xticks(locs, np.round(locs + x_0.mean(), 1));\n\n\n\n\nLa figura anterior muestra la longitud del sépalo para las especies (setosa = 0, versicolor = 1). Para mitigar la superposición de los datos, hemos agregado ruido (jitter) a las variable-respuesta que es binaria. Una línea púrpura en forma de S representa el valor medio de \\(\\theta\\). Esta línea se puede interpretar como la probabilidad que una flor sea versicolor dado el valor de la longitud del sépalo. La banda púrpura semitransparente es el intervalo del 94% de HDI. Esta figura nos muestra que podemos interpretar la regresión logística como una forma de combinar variables linealmente a fin de obtener una probabilidad para variables binarias.\nAlternativamente podemos usar una regresión logística para clasificar, esto lo podemos hacer discretizando el valor de probabilidad obtenido. El caso más común es asignar la clase 1 si la probabilidad es mayor a 0.5 y asignar la clase 0 en caso contrario. En la figura 4.4 hemos graficado este límite de decisión usando una línea vertical negra junto con su 94% HDI (la banda gris). De acuerdo con el límite de decisión, los valores \\(x_i\\) (longitud del sépalo en este caso) a la izquierda corresponden a la clase 0 (setosa) y los valores a la derecha a la clase 1 (versicolor).\nEl límite de decisión se define como el valor de \\(x_i\\), para el cual \\(y = 0.5\\). Y resulta ser $- $, como podemos comprobar a continuación:\nA partir de la definición del modelo tenemos la relación:\n\\[\\theta = logistic(\\alpha + x \\beta) \\tag{4.4}\\]\nY a partir de la definición de la función logística tenemos que $= 0.5 $, cuando el argumento de la regresión logística es 0, es decir:\n\\[0.5 = logística(\\alpha + x_i \\beta) \\Leftrightarrow 0 = \\alpha + x_i \\beta \\tag{4.5}\\]\nReordenando 4.5, encontramos que el valor de \\(x_i\\), para el cual, \\(\\theta = 0.5\\) corresponde a la expresión:\n\\[x_i = - \\frac{\\alpha}{\\beta} \\tag{4.6}\\]\nResumiendo los puntos más importantes hasta el momento:\n\nEl valor de \\(\\theta\\) es, en términos generales, $p(y= 1 x) $. En este sentido, la regresión logística es en realidad una regresión, solo que estamos regresionando la probabilidad de que un punto de datos pertenezca a la clase 1, dada una combinación lineal de características.\nEstamos modelando la media de una variable dicotómica, es decir, un número en el intervalo [0-1]. Luego, introducimos una regla para convertir esta probabilidad en una asignación de dos clases. En este caso, si $p(y = 1) >= 0.5 $ asignamos clase 1, de lo contrario clase 0.\nNo hay nada especial en el valor 0.5, aparte de que es el número en el medio entre 0 y 1. Podemos argumentar que este límite solo es razonable si estamos de acuerdo en cometer un error en una u otra dirección. En otras palabras, si es lo mismo para nosotros clasificar erróneamente una setosa como versicolor o una versicolor como setosa. Resulta que este no es siempre el caso, y el costo asociado a la clasificación errónea no tiene por qué ser simétrico, como recordarán del capítulo 2 cuando analizamos las funciones de pérdida."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#el-límite-de-decisión",
    "href": "06_Generalizando_modelos_lineales.html#el-límite-de-decisión",
    "title": "7  Generalizando modelos lineales",
    "section": "8.1 El límite de decisión",
    "text": "8.1 El límite de decisión\nNo dudes en omitir esta sección y pasar directamente a la implementación del modelo si no estás demasiado interesado en cómo podemos obtener el límite de decisión.\nDesde el modelo, tenemos:\n\\[\\theta = logística(\\alpha + \\beta_1 x_1 + \\beta_2 x_2) \\tag{4.7}\\]\nY a partir de la definición de la función logística, tenemos que \\(\\theta = 0.5\\), cuando el argumento de la regresión logística es cero, es decir:\n\\[ 0.5 = logística(\\alpha + \\beta_1x_1 + \\beta_2x_2) \\Leftrightarrow 0 = \\alpha + \\beta_1x_1 + \\beta_2x_2 \\tag {4.8}\\]\nReordenando, encontramos el valor de \\(x_2\\) para el cual \\(\\theta = 0.5\\) el cual corresponde a la expresión:\n\\[ x_2 = -\\frac{\\alpha}{\\beta_2} + \\left (-\\frac{\\beta_1}{\\beta_2} x_1 \\right) \\tag {4.9}\\]\nEsta expresión para el límite de decisión tiene la misma forma matemática que la ecuación de una línea, siendo el primer término el intercepto y el segundo la pendiente. Los paréntesis se utilizan para mayor claridad y podemos omitirlos si queremos. Que el límite sea una línea es totalmente razonable, ¿no es así? Si tenemos una sola variable, tenemos datos unidimensionales y podemos dividirla en dos grupos usando un punto; si tenemos dos variables, tenemos un espacio de datos bidimensional y podemos separarlo usando una línea; para las tres dimensiones, el límite será un plano y para dimensiones más altas hablaremos genéricamente acerca de los hiperplanos. Bueno, en realidad siempre podemos hablar de hyperplanos n-dimensionales."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#implementando-el-modelo",
    "href": "06_Generalizando_modelos_lineales.html#implementando-el-modelo",
    "title": "7  Generalizando modelos lineales",
    "section": "8.2 Implementando el modelo",
    "text": "8.2 Implementando el modelo\nPara escribir el modelo de regresión logística múltiple utilizando PyMC, aprovechamos sus capacidades de vectorización, lo que nos permite introducir solo modificaciones menores respecto del modelo logístico simple:\n\nwith pm.Model() as modelo_1: \n    α = pm.Normal('α', mu=0, sigma=10) \n    β = pm.Normal('β', mu=0, sigma=2, shape=len(x_n)) \n     \n    μ = α + pm.math.dot(x_1, β) \n    θ = pm.Deterministic('θ', pm.math.sigmoid(μ)) \n    bd = pm.Deterministic('bd', -α/β[1] - β[0]/β[1] * x_1[:,0])\n     \n    yl = pm.Bernoulli('yl', p=θ, observed=y_1) \n \n    idata_1 = pm.sample(2000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:10<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 10 seconds.\n\n\n\nvarnames = ['α', 'β'] \naz.plot_forest(idata_1, var_names=varnames, figsize=(10, 3));\n\n\n\n\nComo hicimos para una única variable predictiva, vamos a graficar los datos y el límite de decisión.\n\n_, ax = plt.subplots(figsize=(10, 6))\n\nidx = np.argsort(x_1[:,0]) \n\nbd = idata_1.posterior['bd'].mean((\"chain\", \"draw\"))[idx] \nplt.scatter(x_1[:,0], x_1[:,1], c=[f'C{x}' for x in y_0]) \nplt.plot(x_1[:,0][idx], bd, color='k'); \n \naz.plot_hdi(x_1[:,0], idata_1.posterior['bd'], color=\"C8\")\n    \nax.set_xlabel(x_n[0]) \nax.set_ylabel(x_n[1]);\n\n\n\n\nEl límite de decisión es una línea recta, como ya hemos visto. No se confunda con el aspecto curvo de la banda del 94% de HDI. La curvatura aparente es el resultado de tener múltiples líneas que giran alrededor de una región central (aproximadamente alrededor de la media de x y la media de y)."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#interpretación-de-los-coeficientes-de-una-regresión-logística",
    "href": "06_Generalizando_modelos_lineales.html#interpretación-de-los-coeficientes-de-una-regresión-logística",
    "title": "7  Generalizando modelos lineales",
    "section": "8.3 Interpretación de los coeficientes de una regresión logística",
    "text": "8.3 Interpretación de los coeficientes de una regresión logística\nDebemos tener cuidado al interpretar los coeficientes \\(\\beta\\) de una regresión logística. La interpretación no es tan sencilla como con los modelos lineales en el capítulo anterior. La función logística introduce una no linearidad, que debemos tener en cuenta. Si \\(\\beta\\) es positivo, aumentar \\(x\\) aumentará \\(p(y = 1)\\) en cierta cantidad, pero la cantidad no es una función lineal de \\(x\\), es en cambio una función no-lineal de \\(x\\). Podemos visualizar este hecho en la figura 4.4, en lugar de una línea con una pendiente constante, tenemos una línea en forma de S con una pendiente que cambia en función de \\(x\\). Un poco de álgebra nos puede dar una idea de cuánto cambia \\(p(y=1)\\) con \\(\\beta\\):\nEl modelo logístico básico es:\n\\[\\theta = logistic (\\alpha + X \\beta) \\tag{4.11} \\]\nEl inverso de la logística es la función logit, que es:\n\\[ logit(z) = log \\left (\\frac{z}{1-z} \\right) \\tag{4.12}\\]\nPor lo tanto, si tomamos la primera ecuación en esta sección y aplicamos la función logit a ambos términos, obtenemos:\n\\[ logit(\\theta) = \\alpha + X \\beta \\tag{4.13}\\]\nO equivalente:\n\\[ log \\left (\\frac{\\theta} {1-\\theta} \\right) = \\alpha + X \\beta \\tag {4.14}\\]\nRecuerden que \\(\\theta\\) en nuestro modelo era la probabilidad de $y = 1 $, por lo tanto:\n\\[ log \\left(\\frac {p(y = 1)} {1-p (y = 1)} \\right) = \\alpha + X \\beta \\tag {4.15} \\]\nLa cantidad \\[\\frac{p (y = 1)} {1-p (y = 1)}\\] se conoce como odds. Los odds a favor se definen como la relación entre la probabilidad de éxito y la probabilidad de no éxito. Mientras que la probabilidad de obtener 2 tirando un dado es 1/6, los odds para el mismo evento son \\(\\frac{1/6}{5/6} \\simeq 0.2\\) o dicho de otra forma 1 evento favorable frente a 5 eventos desfavorables. Los odds suelen ser utilizadas por los apostadores ya que proporcionan una herramienta más intuitiva que las probabilidades en bruto cuando se piensa en la forma correcta de apostar.\n\nEn una regresión logística, el coeficiente \\(\\beta\\) codifica el aumento en unidades de log-odds por unidad de aumento de la variable \\(x\\).\n\nLa transformación de probabilidad a odds es una transformación monotónica, lo que significa que las probabilidades aumentan a medida que aumenta la probabilidad. Mientras que las probabilidades están restringidas al intervalo \\([0, 1]\\), los odds viven en el intervalo \\([0, \\infty]\\). El logaritmo es otra transformación monótonica y los log-odds están en el intervalo \\([-\\infty, \\infty]\\). La figura 4.6 muestra cómo la probabilidad está relacionada con los odds y los log-odds.\n\nprobability = np.linspace(0.01, 1, 100)\nodds = probability / (1 - probability)\n\n_, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax1.plot(probability, odds, 'C0')\nax2.plot(probability, np.log(odds), 'C2')\n\nax1.set_xlabel('probabilidad')\nax1.set_ylabel('odds', color='C0')\nax2.set_ylabel('log-odds', color='C2');\n\n/tmp/ipykernel_16695/2677285095.py:2: RuntimeWarning: divide by zero encountered in divide\n  odds = probability / (1 - probability)\n\n\n\n\n\nPor lo tanto, los valores de los coeficientes proporcionados por summary están en la escala log-odds.\n\ndf = az.summary(idata_1, var_names=['α', 'β'])\ndf\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      α\n      -9.019\n      4.745\n      -18.156\n      -0.685\n      0.089\n      0.066\n      2918.0\n      3083.0\n      1.0\n    \n    \n      β[0]\n      4.642\n      0.909\n      3.036\n      6.413\n      0.018\n      0.013\n      2672.0\n      2638.0\n      1.0\n    \n    \n      β[1]\n      -5.179\n      0.980\n      -6.955\n      -3.279\n      0.018\n      0.013\n      2907.0\n      2779.0\n      1.0\n    \n  \n\n\n\n\nUna forma muy empírica de entender los modelos es cambiar los parámetros y ver qué sucede. En el siguiente bloque de código, calculamos las log-odds en favor de versicolor como \\(\\text {log_odds_versicolor_i} = \\alpha + beta_1 x1 + \\beta_2 x2\\), y luego la probabilidad de versicolor con la función logística. Luego repetimos el cálculo arreglando \\(x_2\\) y aumentando \\(x_1\\) en 1.\n\nx_1 = 4.5  # sepal_length\nx_2 = 3   # sepal_width\n\nlog_odds_versicolor_i = (df['mean'] * [1, x_1, x_2]).sum()\nprobability_versicolor_i = logistic(log_odds_versicolor_i)\n\n\nlog_odds_versicolor_f = (df['mean'] * [1, x_1 + 1, x_2]).sum()\nprobability_versicolor_f = logistic(log_odds_versicolor_f)\n\n(f'{log_odds_versicolor_f - log_odds_versicolor_i:.2f}', \n f'{probability_versicolor_f - probability_versicolor_i:.2f}')\n\n('4.64', '0.70')\n\n\nSi ejecutas el código, encontrarás que el aumento en las log-odds es de \\(\\approx 4.7\\), que es exactamente el valor de \\(\\beta_0\\) (verifique el summary para trace_1). Esto está en línea con nuestro hallazgo anterior que muestra que los coeficientes \\(\\beta\\) indican el aumento en unidades log-odds por incremento unitario de la variable \\(x\\). El aumento en la probabilidad es \\(\\approx 0.70\\)."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#trabajando-con-variables-correlacionadas",
    "href": "06_Generalizando_modelos_lineales.html#trabajando-con-variables-correlacionadas",
    "title": "7  Generalizando modelos lineales",
    "section": "8.4 Trabajando con variables correlacionadas",
    "text": "8.4 Trabajando con variables correlacionadas\nSabemos por el capítulo anterior que trabajar con variables muy correlacionadas puede traernos problemas. Las variables correlacionadas se traducen en combinaciones más amplias de coeficientes que explican los datos o, desde el punto de vista complementario, variables correlacioadas tienen menos poder para restringir los modelos. Un problema similar ocurre cuando las clases se vuelven perfectamente separables, es decir, no hay superposición entre clases dada la combinación lineal de variables en nuestro modelo. Podemos visualizar un ejemplo de esto al usar el conjunto de datos iris con el modelo_1, pero esta vez utilizando las variables ancho de pétalo y largo de pétalo. Encontraras que los coeficientes \\(\\beta\\) son más amplios que antes y también el 94% HDI (banda gris en la figura 4.5) es mucho más amplia. La figura 4.7 muestra un heatmap para las variables sepal_length y sepal_width (usadas en el primer ejemplo) la correlación no es tan alta como la correlación entre las variables petal_length y petal_width (usada en el segundo ejemplo).\n\ncorr = iris[iris['species'] != 'virginica'].corr() \nmask = np.tri(*corr.shape).T \nsns.heatmap(corr.abs(), mask=mask, annot=True, cmap='viridis');\n\n\n\n\nPara generar la figura 4.7, hemos utilizado una máscara que elimina el triángulo superior y los elementos diagonales del heatmap, ya que estos son poco informativos o redundantes. Observe también que hemos graficado el valor absoluto de la correlación, ya que en este momento no nos importa el signo de la correlación entre las variables, solo su fuerza.\nUna solución cuando se trabaja con variables (altamente) correlacionadas, es simplemente eliminar una (o más de una) de las variables correlacionadas. Otra opción es poner más información en el a priori, esto se puede lograr con a prioris informativos si es que contamos con información previa útil, o más general utilizando a prioris ligeramente informativos. Andrew Gelman y el equipo de Stan recomiendan usar el siguiente a priori al realizar una regresión logística:\n\\[ \\beta \\sim Student t (0, \\nu, sd) \\tag {4.10}\\]\ndonde sd se elije de forma que informe débilmente sobre los valores esperados para la escala. Se sugiere que el parámetro de normalidad \\(\\nu\\) sea alrededor de 3-7. Lo que dice este a priori es que esperamos que el coeficiente sea pequeño, pero ponemos colas pesadas porque esto nos lleva a un modelo más robusto que el uso de una distribución gaussiana."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#tratando-con-clases-desequilibradas",
    "href": "06_Generalizando_modelos_lineales.html#tratando-con-clases-desequilibradas",
    "title": "7  Generalizando modelos lineales",
    "section": "8.5 Tratando con clases desequilibradas",
    "text": "8.5 Tratando con clases desequilibradas\nEl conjunto de datos del iris está completamente equilibrado; en el sentido de que cada categoría tiene exactamente el mismo número de observaciones. Tenemos 50 setosas, 50 versicolores, y 50 virgininas. Por el contrario, muchos conjuntos de datos constan de datos no balanceados, es decir, hay muchos más datos de una clase que de la otra. Cuando esto sucede, la regresión logística puede generar problemas, es decir, el límite no se puede determinar con la misma precisión que cuando el conjunto de datos está más equilibrado.\nPara ver un ejemplo de este comportamiento, vamos a usar el conjunto de datos del iris y vamos a eliminar arbitrariamente algunos puntos de datos de la clase setosa:\n\ndf = iris.query(\"species == ('setosa', 'versicolor')\") \ndf = df[45:]  \ny_3 = pd.Categorical(df['species']).codes \nx_n = ['sepal_length', 'sepal_width'] \nx_3 = df[x_n].values\n\nY ahora ejecutamos una regresión logística múltiple, tal cual hicimos antes.\n\nwith pm.Model() as modelo_3: \n    α = pm.Normal('α', mu=0, sigma=10) \n    β = pm.Normal('β', mu=0, sigma=2, shape=len(x_n)) \n     \n    μ = α + pm.math.dot(x_3, β) \n    θ = pm.math.sigmoid(μ)\n    bd = pm.Deterministic('bd', -α/β[1] - β[0]/β[1] * x_3[:,0]) \n     \n    yl = pm.Bernoulli('yl', p=θ, observed=y_3) \n \n    idata_3 = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:05<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 6 seconds.\n\n\nEl límite de decisión se desplaza hacia la clase menos abundante y la incertidumbre es más grande que antes. Este es el comportamiento típico de un modelo logístico para datos no balanceados. ¡Pero espera un minuto! Bien podrías argumentar que te estoy engañando ya que la mayor incertidumbre es en realidad el producto de tener menos datos y no solo menos setosas que versicolores. Este es un punto totalmente válido, pero si realizas el ejercicio 2 podrás verificar que lo que explica esta gráfica son los datos desequilibrados.\n\nidx = np.argsort(x_3[:,0]) \nbd = idata_3.posterior['bd'].mean((\"chain\", \"draw\"))[idx] \nplt.scatter(x_3[:,0], x_3[:,1], c= [f'C{x}' for x in y_3]) \nplt.plot(x_3[:,0][idx], bd, color='C8'); \n \nbd_hdi = az.hdi(idata_3.posterior)['bd'][idx] \nplt.fill_between(x_3[:,0][idx], bd_hdi[:,0], bd_hdi[:,1], color='C8', alpha=0.5); \n \nplt.xlabel(x_n[0]) \nplt.ylabel(x_n[1]);\n\n\n\n\n¿Qué hacer si encontramos datos desequilibrados? Bueno, la solución obvia es obtener un conjunto de datos con aproximadamente la misma cantidad por clase. Este es un punto a tener en cuenta al recopilar o generar los datos. Si no tenés control sobre el conjunto de datos, debes tener cuidado al interpretar los resultados para datos no balanceados. Verifique la incertidumbre del modelo y ejecute algunas verificaciones predictivas posteriores para ver si los resultados son útiles para usted. Otra opción sería utilizar priors más informativos y/o ejecutar un modelo alternativo como se explica más adelante en este capítulo."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#regresión-softmax-o-multinomial",
    "href": "06_Generalizando_modelos_lineales.html#regresión-softmax-o-multinomial",
    "title": "7  Generalizando modelos lineales",
    "section": "8.6 Regresión softmax (o multinomial)",
    "text": "8.6 Regresión softmax (o multinomial)\nUna forma de generalizar la regresión logística a más de dos clases es con la regresión softmax. Necesitamos introducir 2 cambios con respecto a la regresión logística, primero reemplazamos la función logística con la función softmax:\n\\[softmax (\\mu_i) = \\frac {exp (\\mu_i)} {\\sum exp (\\mu_k)} \\tag{4.16}\\]\nEn palabras, para obtener la salida de la función softmax para el i-esimo elemento de un vector \\(\\mu\\), tomamos la exponencial del valor i-esimo dividido por la suma de todos los valores del vector \\(\\mu\\) exponenciados.\nLa función softmax garantiza que obtendremos valores positivos que suman 1. La función softmax se reduce a la función logística cuando \\(k=2\\). Como nota al margen, la función softmax tiene la misma forma que la distribución de Boltzmann, distribución central en la mecánica estadística, una rama muy poderosa de la física que se ocupa de la descripción probabilística de los sistemas atómicos y moleculares. La distribución de Boltzmann (y a veces la función softmax) incluye un parámetro llamado temperatura (T) que divide \\(\\mu\\); cuando $ T $ la distribución de probabilidad se vuelve plana y todos los estados son igualmente probables, y cuando \\(T \\rightarrow 0\\) solo se llena el estado más probable y, por lo tanto, el softmax se comporta como la función máximo.\nEl segundo cambio en la regresión softmax es que reemplazamos la distribución de Bernoulli por la distribución categórica. La distribución categórica es la generalización de Bernoulli a más de dos resultados. Además, como la distribución de Bernoulli (tirada de una sola moneda) es un caso especial de la Binomial (tiradas de \\(n\\) monedas), la categórica (tirada de un dado de \\(k\\) caras) es un caso especial de la distribución multinomial (\\(n\\) tiradas de un dado de \\(k\\) caras).\nk-diagram\nPara ejemplificar la regresión de softmax, continuaremos trabajando con el conjunto de datos iris, solo que esta vez usaremos sus 3 clases (setosa, versicolor y virginica) y sus cuatro características (largo sépalo, ancho sépalo, longitud del pétalo y ancho del pétalo). También vamos a estandarizar los datos, ya que esto ayudará a que el sampler se ejecute de manera más eficiente (también podríamos centrar los datos):\n\ny_s = pd.Categorical(iris['species']).codes\nx_n = iris.columns[:-1]\nx_s = iris[x_n].values\nx_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)\n\nEl código de PyMC refleja los pocos cambios entre el modelo logístico y el modelo softmax. Presta atención a los valores de shape para los coeficientes $$ y \\(\\beta\\). En el siguiente código usamos la función softmax de Theano. Hemos utilizado la expresión import theano.tensor as tt, que es la convención utilizada por los desarrolladores de PyMC:\n\nwith pm.Model() as modelo_s:\n    α = pm.Normal('α', mu=0, sigma=5, shape=3)\n    β = pm.Normal('β', mu=0, sigma=5, shape=(4,3))\n    μ = pm.Deterministic('μ', α + pm.math.dot(x_s, β))\n    θ = pm.math.softmax(μ)\n    yl = pm.Categorical('yl', p=θ, observed=y_s)\n    idata_s = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:27<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 28 seconds.\n\n\n\naz.plot_forest(idata_s, var_names=['α', 'β'], figsize=(10, 4), combined=True);\n\n\n\n\n¿Qué tan bien funciona nuestro modelo? Averigüemos cuántos casos podemos predecir correctamente. En el siguiente código, solo usamos la media de los parámetros para calcular la probabilidad de que cada punto de datos pertenezca a cada una de las tres clases, luego asignamos la clase usando la función argmax. Y comparamos el resultado con los valores observados:\n\ndata_pred = idata_s.posterior['μ'].mean((\"chain\", \"draw\"))\n\ny_pred = [np.exp(point)/np.sum(np.exp(point), axis=0) for point in data_pred]\n\nf'{np.sum(y_s == np.argmax(y_pred, axis=1)) / len(y_s):.2f}'\n\n'0.95'\n\n\nEl resultado es que clasificamos correctamente \\(\\approx 95 \\%\\) de los datos. Ese es realmente un muy buen trabajo. Sin embargo, una verdadera prueba para evaluar el rendimiento de nuestro modelo sería verificarlo con un conjunto de datos no usado para ajustar al modelo. De lo contrario, es posible que estemos sobreestimando la capacidad real del modelo para generalizar a otros datos.\nEs posible que hayas notado que las distribuciones marginales de cada parámetro son muy amplias. Este es el mismo problema de no identificabilidad que ya hemos encontrado para los datos correlacionados en otros modelos de regresión o con clases perfectamente separables. En este caso, el ancho posterior se debe a la condición de que todas las probabilidades deben sumar 1. Dada esta condición, estamos usando más parámetros de los que necesitamos para especificar completamente el modelo. En términos simples, si tenés 10 números que suman 1, solo necesitás darme 9 de ellos; el otro puedo calcularlo. Esto es precisamente lo que está pasando con este problema. Una solución es fijar los parámetros extra a algún valor, por ejemplo, cero. El siguiente código muestra cómo lograr esto usando PyMC:\n\nwith pm.Model() as modelo_sf:\n    α = pm.Normal('α', mu=0, sigma=2, shape=2)\n    β = pm.Normal('β', mu=0, sigma=2, shape=(4,2))\n    α_f = pm.math.concatenate([[0] ,α])\n    β_f = pm.math.concatenate([np.zeros((4,1)) , β], axis=1)\n    μ = α_f + pm.math.dot(x_s, β_f)\n    θ = pm.math.softmax(μ)\n    yl = pm.Categorical('yl', p=θ, observed=y_s)\n    idata_sf = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:08<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 8 seconds.\n\n\n\naz.plot_forest(idata_sf, var_names=['α', 'β'], figsize=(10, 3), combined=True);"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#linear-discriminant-analysis-lda",
    "href": "06_Generalizando_modelos_lineales.html#linear-discriminant-analysis-lda",
    "title": "7  Generalizando modelos lineales",
    "section": "8.7 Linear discriminant analysis (LDA)",
    "text": "8.7 Linear discriminant analysis (LDA)\nHasta ahora hemos discutido la regresión logística y algunas extensiones de la misma. En todos estos casos, calculamos $p(y x) $, es decir, la probabilidad que una clase \\(y\\) teniendo como dato una o más variables \\(x\\), luego usamos un umbral o límite para convertir la probabilidad computada en un límite discreto lo que nos permite asignar clases.\nEste enfoque no es único. Una alternativa es modelar primero \\(p(x \\mid y)\\). No vamos a entrar en mucho detalle aquí sobre este tipo de modelos para clasificación, pero vamos a ver un ejemplo que ilustra la idea central de este tipo de modelo. Lo haremos para dos clases y una sola variable, exactamente como el primer modelo que construimos en este capítulo, es más usaremos los mismos datos.\nEn el siguiente código se puede ver que ahora el límite de decisión se define como el promedio entre las medias de las Gaussianas. Este modelo es equivalente a lo que se conoce como análisis discriminante lineal (Linear Discriminar Analysis).\n\nwith pm.Model() as modelo_lda:\n    μ = pm.Normal('μ', mu=0, sigma=10, shape=2)\n    σ = pm.HalfNormal('σ', 10)\n    setosa = pm.Normal('setosa', mu=μ[0], sigma=σ, observed=x_0[:50])\n    versicolor = pm.Normal('versicolor', mu=μ[1], sigma=σ, observed=x_0[50:])\n    bd = pm.Deterministic('bd', (μ[0] + μ[1]) / 2)\n    idata_lda = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [μ, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\n\n\nAhora vamos a generar una figura que muestra las dos clases (setosa = 0 yversicolor = 1) contra los valores de la longitud del sépalo, y también el límite de decisión como una línea turquesa y el intervalo del 94% de HDI como una banda turquesa semitransparente.\n\nplt.axvline(idata_lda.posterior['bd'].mean((\"chain\", \"draw\")), ymax=1, color='C1')\nbd_hdi = az.hdi(idata_lda.posterior)['bd'].values\nplt.fill_betweenx([0, 1], bd_hdi[0], bd_hdi[1], color='C1', alpha=0.5)\n\nplt.plot(x_0, np.random.normal(y_0, 0.02), '.', color='k')\nplt.ylabel('θ', rotation=0)\nplt.xlabel('sepal_length');\n\n\n\n\nComo habrá notado, la figura 4.9 es bastante similar a la figura 4.4. Verifique también los valores de la decisión de límite en el siguiente summary:\n\naz.summary(idata_lda)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      μ[0]\n      5.005\n      0.064\n      4.889\n      5.133\n      0.001\n      0.001\n      6084.0\n      3292.0\n      1.0\n    \n    \n      μ[1]\n      5.936\n      0.064\n      5.821\n      6.059\n      0.001\n      0.001\n      5652.0\n      3249.0\n      1.0\n    \n    \n      σ\n      0.447\n      0.032\n      0.392\n      0.510\n      0.000\n      0.000\n      5429.0\n      3241.0\n      1.0\n    \n    \n      bd\n      5.471\n      0.046\n      5.384\n      5.554\n      0.001\n      0.000\n      5770.0\n      2905.0\n      1.0\n    \n  \n\n\n\n\nTanto el modelo LDA como la regresión logística proporcionan resultados similares. El modelo discriminante lineal puede extenderse a más de una característica al modelar las clases como Gaussianas multivariadas. Además, es posible relajar el supuesto de que las clases comparten una varianza común (o covarianza). Esto conduce a un modelo conocido como análisis discriminante cuadrático (QDA).\nEn general, los modelos LDA o QDA funcionarán mejor que una regresión logística cuando las características que estamos usando estén más o menos distribuidas como Gaussianas y la regresión logística funcionará mejor en el caso contrario. Una ventaja de modelos como LDA y QDA (o generalizaciones de esta idea) es que puede ser más fácil o más natural incorporar información previa.\nEs importante tener en cuenta que los límites de decisión de LDA y QDA pueden ser calculados analíticamente y, por lo tanto, por lo general se calculan de esa manera. Para usar un LDA para dos clases y una característica, solo necesitamos calcular la media de cada distribución y promediar esos dos valores, y obtenemos la decisión de los límites. En el modelo anterior, lo hicimos, pero con un giro Bayesiano. Estimamos los parámetros de las dos Gaussianas y luego insertamos esas estimaciones en una fórmula predefinida."
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#regresión-de-poisson",
    "href": "06_Generalizando_modelos_lineales.html#regresión-de-poisson",
    "title": "7  Generalizando modelos lineales",
    "section": "8.8 Regresión de Poisson",
    "text": "8.8 Regresión de Poisson\nOtro modelo lineal generalizado muy popular es la regresión de Poisson. Este modelo asume que los datos se distribuyen de acuerdo con la distribución de Poisson.\nUn escenario en el que la distribución de Poisson es útil es cuando se analizan cosas, como la descomposición de un núcleo radioactivo, el número de hijos por pareja o el número de seguidores de Twitter. Lo que todos estos ejemplos tienen en común es que usualmente los modelamos usando números discretos no negativos {0, 1, 2, 3 …}. Este tipo de variable recibe el nombre de datos de conteo (count data).\n\n8.8.1 La distribución de Poisson\nImagina que estamos contando la cantidad de autos rojos que pasan por una avenida por hora. Podríamos usar la distribución de Poisson para describir estos datos. La distribución de Poisson se utiliza generalmente para describir la probabilidad que ocurra un número determinado de eventos independientes entre si en un intervalo de tiempo o espacio fijo. Esta distribución discreta se parametriza utilizando solo un valor, \\(\\mu\\) (la tasa, también comúnmente representada con la letra griega \\(\\lambda\\)). \\(\\mu\\) corresponde a la media y también a la varianza de la distribución. La función de probabilidad de masa de la distribución de Poisson es:\n\\[ f(x \\mid \\mu) = \\frac {e^{-\\mu}\\mu^x} {x!} \\tag{4.17}\\]\ndónde: * \\(\\mu\\) es el número promedio de eventos por unidad de tiempo / espacio * \\(x\\) es un valor entero positivo 0, 1, 2, … * \\(x!\\) es el factorial de x, k! = k × (k - 1) × (k - 2) × … × 2 × 1\nEn la siguiente gráfica, podemos ver algunos ejemplos de la familia de distribución de Poisson, para diferentes valores de \\(\\mu\\).\n\nmu_params = [0.5, 1.5, 3, 8]\nx = np.arange(0, max(mu_params) * 3)\nfor mu in mu_params:\n    y = pz.Poisson(mu).rv_frozen.pmf(x)\n    plt.plot(x, y, 'o-', label=f'μ = {mu:3.1f}')\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('f(x)');\n\n\n\n\nEs importante notar que \\(\\mu\\) puede ser un flotante, pero la distribución modela probabilidad de un número discreto de eventos. En la figura 4.10, los puntos representan los valores de la distribución, mientras que las líneas continuas son una ayuda visual que nos ayuda a comprender fácilmente la forma de la distribución. Recuerde, la distribución de Poisson es una distribución discreta.\nLa distribución de Poisson puede verse como un caso especial de la distribución binomial cuando la cantidad de intentos \\(n\\) es muy grande pero la probabilidad de éxito \\(p\\) es muy baja. Sin entrar en detalles matemáticos, tratemos de aclarar la afirmación anterior. Siguiendo el ejemplo del auto, podemos afirmar que o vemos el auto rojo o no, por lo que podemos usar una distribución binomial. En ese caso tenemos:\n\\[ x \\sim Bin(n, p) \\tag{4.18}\\]\nEntonces, la media de la distribución binomial es:\n\\[\\mathbf{E}[x] = np \\tag{4.19} \\]\nY la varianza viene dada por:\n\\[ \\mathbf {V}[x] = np (1 - p) \\tag{4.20}\\]\nPero tenga en cuenta que incluso si se encuentra en una avenida muy transitada, la posibilidad de ver un auto rojo en comparación con el número total de automóviles en una ciudad es muy pequeño y, por lo tanto, tenemos:\n\\[n >> p \\Rightarrow np \\simeq np (1-p) \\tag{4.21}\\]\nEntonces, podemos hacer la siguiente aproximación:\n\\[\\mathbf {V}[x] = np \\tag{4.22}\\]\nAhora la media y la varianza están representadas por el mismo número y podemos declarar con confianza que nuestra variable se distribuye como una distribución de Poisson:\n\\[x \\sim Poisson(\\mu = np) \\tag{4.23}\\]"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#el-modelo-de-poisson-inflado-de-ceros",
    "href": "06_Generalizando_modelos_lineales.html#el-modelo-de-poisson-inflado-de-ceros",
    "title": "7  Generalizando modelos lineales",
    "section": "8.9 El modelo de Poisson inflado de ceros",
    "text": "8.9 El modelo de Poisson inflado de ceros\nAl contar cosas, una posibilidad es no contar esas cosas, es decir obtener cero. El número cero puede ocurrir generalmente por muchas razones; obtuvimos un cero porque estábamos contando autos rojos y un auto rojo no pasó por la avenida o porque no logramos verlo (tal vez no vimos pasar un diminuto auto rojo detrás de un gran camión). Entonces, si usamos una distribución de Poisson, notaremos, por ejemplo, cuando realizamos una verificación predictiva posterior, que el modelo generó menos ceros en comparación con los datos.\n¿Cómo arreglamos eso? Podemos tratar de abordar la causa exacta por la cual nuestro modelo predice menos ceros de los observados e incluir ese factor en el modelo. Sin embargo, suele ser el caso, que es suficiente y más fácil para nuestro propósito, asumir que simplemente tenemos una mezcla de dos procesos:\n\nUno modelado por una distribución de Poisson con probabilidad \\(\\psi\\)\nOtra persona que da ceros adicionales con probabilidad \\(1 - \\psi\\).\n\nEsto se conoce como modelo Poisson inflado de ceros (ZeroInflatedPoisson). En algunos textos, encontrarás que \\(\\psi\\) se usa para representar los ceros extra y \\(1-\\psi\\) la probabilidad de Poisson.\nBásicamente una distribución ZIP nos dice que:\n\\[p(y_j = 0) = 1 - \\psi + (\\psi) e^{-\\mu} \\tag{4.24}\\]\n\\[p(y_j = k_i ) = \\psi \\frac{\\mu^x_i e^{-\\mu}}{x_i!} \\tag{4.25}\\]\nDonde \\(1-\\psi\\) es la probabilidad de ceros adicionales. Podríamos implementar fácilmente estas ecuaciones en un modelo PyMC. Sin embargo, podemos hacer algo aún más fácil y usar la distribución ZIP de PyMC.\n\n#np.random.seed(42)\nn = 100\nθ_real = 2.5\nψ = 0.1\n\n# Simulate some data\ncounts = np.array([(np.random.random() > (1-ψ)) * np.random.poisson(θ_real)\n                   for i in range(n)])\n\n\nwith pm.Model() as ZIP:\n    ψ = pm.Beta('ψ', 1., 1.)\n    θ = pm.Gamma('θ', 2., 0.1)\n    y = pm.ZeroInflatedPoisson('y', ψ, θ, observed=counts)\n    idata = pm.sample(1000)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [ψ, θ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\n\naz.plot_trace(idata);\n\n\n\n\n\naz.summary(idata)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      ψ\n      0.131\n      0.037\n      0.069\n      0.202\n      0.001\n      0.000\n      3599.0\n      2480.0\n      1.0\n    \n    \n      θ\n      2.445\n      0.506\n      1.473\n      3.369\n      0.008\n      0.006\n      3714.0\n      2602.0\n      1.0"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#regresión-de-poisson-y-regresión-zip",
    "href": "06_Generalizando_modelos_lineales.html#regresión-de-poisson-y-regresión-zip",
    "title": "7  Generalizando modelos lineales",
    "section": "8.10 Regresión de Poisson y regresión ZIP",
    "text": "8.10 Regresión de Poisson y regresión ZIP\nEl modelo ZIP puede parecer un poco aburrido, pero a veces necesitamos estimar distribuciones simples como esta u otra como las distribuciones de Poisson o Gaussianas. Además, podemos usar las distribuciones Poisson o ZIP como parte de un modelo lineal. Como vimos con la regresión logística (y softmax) podemos usar una función de enlace inverso para transformar el resultado de un modelo lineal en una variable adecuada para ser utilizada con otra distribución que no sea la normal. En la siguiente figura, vemos una posible implementación de una regresión ZIP. La regresión de Poisson será similar, pero sin la necesidad de incluir \\(\\phi\\) ya que no modelaremos un exceso de ceros. Observe que ahora usamos la función exponencial como la función de enlace inverso. Esta elección garantiza que los valores devueltos por el modelo lineal sean positivos.\nPara ejemplificar la implementación de un modelo de regresión ZIP, vamos a trabajar con un conjunto de datos tomado del Instituto de Investigación y Educación Digital.\nEl problema es el siguiente: trabajamos en la administración de un parque y queremos mejorar la experiencia de los visitantes. Por lo tanto, decidimos realizar una breve encuesta a 250 grupos que visitan el parque. Parte de los datos que recopilamos (a nivel de grupo) consiste en:\n\nLa cantidad de peces que capturaron (contar)\nCuántos niños había en el grupo (niño)\nYa sea que hayan traído o no una casa-rodante o “caravana” al parque (camper).\n\nUsando estos datos, vamos a construir un modelo que predice el número de peces capturados en función de las variables niño y caravana. Podemos usar Pandas para cargar los datos:\n\nfish_data = pd.read_csv('datos/fish.csv')\n\nLo dejo como un ejercicio para que explore el conjunto de datos utilizando gráficos y / o una función de Pandas, como describe(). Por ahora vamos a continuar traduciendo el diagrama de Kruschke anterior a PyMC3:\n\nwith pm.Model() as ZIP_reg:\n    ψ = pm.Beta('ψ', 1, 1)\n    α = pm.Normal('α', 0, 10)\n    β = pm.Normal('β', 0, 10, shape=2)\n    θ = pm.math.exp(α + β[0] * fish_data['child'] + β[1] * fish_data['camper'])\n    yl = pm.ZeroInflatedPoisson('yl', ψ, θ, observed=fish_data['count'])\n    idata_ZIP_reg = pm.sample()\naz.plot_trace(idata_ZIP_reg);\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [ψ, α, β]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\n\n\n\nPara entender mejor los resultados de nuestra inferencia, hagamos una gráfica.\n\nchildren = [0, 1, 2, 3, 4]\nfish_count_pred_0 = []\nfish_count_pred_1 = []\n\npost_ZIP_reg = az.extract(idata_ZIP_reg)\n\nfor n in children:\n    without_camper = post_ZIP_reg['α'] + post_ZIP_reg['β'].sel({\"β_dim_0\":0}) * n\n    with_camper = without_camper + post_ZIP_reg['β'].sel({\"β_dim_0\":1})\n    fish_count_pred_0.append(np.exp(without_camper))\n    fish_count_pred_1.append(np.exp(with_camper))\nplt.plot(children, fish_count_pred_0, 'C0.', alpha=0.01)\nplt.plot(children, fish_count_pred_1, 'C1.', alpha=0.01)\n\n    \nplt.xticks(children);\nplt.xlabel('Number of children')\nplt.ylabel('Fish caught')\nplt.plot([], 'C0o', label='without camper')\nplt.plot([], 'C1o', label='with camper')\nplt.legend();"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#regresión-por-cuantiles",
    "href": "06_Generalizando_modelos_lineales.html#regresión-por-cuantiles",
    "title": "7  Generalizando modelos lineales",
    "section": "8.11 Regresión por cuantiles",
    "text": "8.11 Regresión por cuantiles\nEn los ejemplos anteriores nos focalizamos en usar un modelo lineal para estimar la media de la variable respuesta, condicionada a una o más covariables. Quizá el caso más común sea usar la distribución Normal. Pero aprendimos que podemos aplicar la misma idea cambiando la distribución por otras como la Poisson, binomial, etc, según nuestras necesidades.\nLa regresión por cuantiles consiste en utilizar un modelo lineal para estimar un cuantil. Cuando el cuantil a estimar es la mediana, la motivación suele ser la necesidad de una regresión robusta. En ese caso la regresión por cuantiles cumpliría una función similar al modelo robusto donde reemplazamos la Gaussiana por una distribución t de Student. Otras veces la motivación surge del interés en modelar relaciones entre variables cuando no hay relación entre las medias de dichas variables, o cuando esta es muy debil. Una disciplina donde las regresiones por cuantiles son frecuentes es la ecología. Esto se debe posiblemente, a que la existencia de complejas interacciones entre variables, donde el efecto de una variable sobre otra es distinto para distintos rangos de la variable.\n\nx = np.linspace(-6, 6, 2000)\nquantiles =  np.array([0.2, 0.5, 0.8])\nkappas = (quantiles/(1-quantiles))**0.5\nfor q, m in zip(quantiles, [0, 0, -1]):\n    κ = (q/(1-q))**0.5\n    plt.plot(x, stats.laplace_asymmetric(κ, m, 1).pdf(x), label=f\"q={q:}, μ={m}, σ=1\")\nplt.yticks([]);\nplt.legend();\n\n\n\n\n\nquantiles = np.array([0.05, 0.5, 0.95])\nκ = (quantiles/(1-quantiles))**0.5\n\ny_con = np.stack([data.Longitud.values]* 3).T\nx_con = np.stack([data.Meses.values]* 3).T\n\n\nwith pm.Model() as model_q:\n    α = pm.Normal('α', 50, 3, shape=3)\n    β = pm.Normal('β', 0, 5, shape=3)\n    σ = pm.HalfNormal('σ', 5)\n\n    μ = pm.Deterministic('μ', α + β * x_con**0.5)\n    \n    y_pred = pm.AsymmetricLaplace('y_pred',  κ, μ, σ, observed=y_con)\n    \n    idata_q = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:11<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 11 seconds.\n\n\n\naz.summary(idata_q, var_names=\"~μ\")\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n      mcse_mean\n      mcse_sd\n      ess_bulk\n      ess_tail\n      r_hat\n    \n  \n  \n    \n      α[0]\n      45.402\n      0.230\n      44.963\n      45.833\n      0.006\n      0.004\n      1727.0\n      1965.0\n      1.0\n    \n    \n      α[1]\n      47.954\n      0.229\n      47.504\n      48.360\n      0.005\n      0.004\n      1774.0\n      1965.0\n      1.0\n    \n    \n      α[2]\n      52.390\n      0.185\n      52.092\n      52.763\n      0.005\n      0.003\n      1557.0\n      1939.0\n      1.0\n    \n    \n      β[0]\n      6.658\n      0.082\n      6.517\n      6.817\n      0.002\n      0.001\n      1713.0\n      2111.0\n      1.0\n    \n    \n      β[1]\n      7.895\n      0.078\n      7.739\n      8.035\n      0.002\n      0.001\n      1706.0\n      1849.0\n      1.0\n    \n    \n      β[2]\n      8.825\n      0.059\n      8.714\n      8.942\n      0.002\n      0.001\n      1530.0\n      1718.0\n      1.0\n    \n    \n      σ\n      0.548\n      0.011\n      0.528\n      0.568\n      0.000\n      0.000\n      2661.0\n      2451.0\n      1.0\n    \n  \n\n\n\n\n\nplt.plot(data.Meses, data.Longitud, \"k.\")\nfor idx, label in enumerate((\"q=0.1\", \"q=0.5\", \"q=0.9\")):\n    plt.plot(data.Meses.values, idata_q.posterior[\"μ\"].mean((\"chain\", \"draw\"))[:,idx],\n            label=label, lw=3);\n    \nplt.legend();\n\n\n\n\n\nwith pm.Model() as model_n:\n    α = pm.Normal('α', 50, 3)\n    β = pm.Normal('β', 0, 5)\n    σ = pm.HalfNormal('σ', 5)\n\n    μ = pm.Deterministic('μ', α + β * data.Meses.values**0.5)\n    \n    y_pred = pm.Normal('y_pred',  μ, σ, observed=data.Longitud.values)\n    \n    idata_n = pm.sample()\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β, σ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\n\nplt.plot(data.Meses, data.Longitud, \".\", color=\"0.8\")\nfor idx, label in enumerate([f\"{q=:}\" for q in quantiles]):\n    plt.plot(data.Meses.values, idata_q.posterior[\"μ\"].mean((\"chain\", \"draw\"))[:,idx],\n            label=label);\n    \nplt.legend();\n\nup = (idata_n.posterior[\"μ\"] + idata_n.posterior[\"σ\"]*1.65).mean((\"chain\", \"draw\"))\ndown = (idata_n.posterior[\"μ\"] - idata_n.posterior[\"σ\"]*1.65).mean((\"chain\", \"draw\"))\n\nplt.plot(data.Meses.values, down, \"C0\", label=\"μ - 1.65σ\", ls=\"--\");\nplt.plot(data.Meses.values, idata_n.posterior[\"μ\"].mean((\"chain\", \"draw\")), \"C1\", label=\"μ\",ls=\"--\");\nplt.plot(data.Meses.values, up, \"C2\", label=\"μ + 1.65σ\", ls=\"--\");\n\n\nplt.legend();"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#resumen",
    "href": "06_Generalizando_modelos_lineales.html#resumen",
    "title": "7  Generalizando modelos lineales",
    "section": "8.12 Resumen",
    "text": "8.12 Resumen"
  },
  {
    "objectID": "06_Generalizando_modelos_lineales.html#ejercicios",
    "href": "06_Generalizando_modelos_lineales.html#ejercicios",
    "title": "7  Generalizando modelos lineales",
    "section": "8.13 Ejercicios",
    "text": "8.13 Ejercicios\n\nEs conocido que para muchas especies el peso no escala con la altura/longitud, pero si lo hace con el logaritmo de peso. Use esa información para ajustar el conjunto de datos howell (sin distinción por edad). Repita el ajuste usando un polinomio de grado 2. Explique y compare ambos resultados.\nVuelva a correr el modelo_0 pero esta vez usando las variables petal_length y petal_width ¿En que difieren los resultados? ¿Cuán ancho o angosto es el intervalo HDI 94%?\nRepita el ejercicio 1, esta vez usando una distribución t de Student como prior ligeramente informativo. Pruebe con distintos valores de \\(\\nu\\).\nUse un modelo lineal (como los vistos en el capítulo anterior) para clasificar setosa o versicolor en función de sepal_length. ¿Cuán útil es este modelo comparado con una regresión logística?\nEn la sección Interpretando los coeficientes de una regresion logística vimos el efecto sobre el log_odds de cambiar la variable sepal_length en 1 unidad. Usando la figura 4.6 corrobore que el valor obtenido para log_odds_versicolor_i se corresponde con el valor de probability_versicolor_i. Haga lo mismo para log_odds_versicolor_f y probability_versicolor_f. Si solo sabemos que el valor de log_odds_versicolor es negativo que podemos decir de la probabilidad de versicolor, use la figura 4.6 como guía ¿Es este resultado evidente de la definición de log-odds?\nPara modelo_1 verifica cuanto cambian el valor de log-odd al incrementar sepal_leght de 5.5 a 6.5. ¿Cúal es el cambio en valores de probabilidad? ¿Cuál es el cambio en términos de log-odds y probabilidad al pasar de 4.5 a 5.5?\nEn el ejemplo de clases desbalanceadas cambie df = df[45:] por df = df[22:78]. Esto dejará más o menos el mismo número de datos, pero con las clases balanceadas. Compare con los resultados previos. ¿Cuál caso es más parecido a usar el conjunto de datos completo?\nSuponga que en vez de usar una regresión softmax usamos un modelo lineal simple codificando \\(\\text{setosa}=0\\), \\(\\text{versicolor}=1\\) y \\(\\text{virginica}=1\\). Bajo el modelo lineal simple que pasaría si cambiáramos el orden del código.\nCompara los likelihoods para el modelo_0 y para el modelo_lda. Usa la función pm.sample_posterior_predictive para generar datos a partir de estos dos modelos. ¿En que difirien los datos predichos para ambos modelos?\nExtienda el modelo ZIP_reg para incluir la variable persons. Usa esta variable para modelar el número de ceros extra. Deberás obtener un modelo que incluya dos modelos lineales, uno que conecte las variables children y camper a la tasa de Poisson y otro que conecte el número de personas con la variable \\(\\psi\\). Presta atención si es necesario usar una función inversa de enlace.\nUse los datos empleados en el ejemplo de la regresión logística robusta con un modelo de regresión logística simple. ¿Cuál es el efecto de los outliers? Pruebe agregando o eliminado outliers."
  },
  {
    "objectID": "07_Comparación_de_modelos.html",
    "href": "07_Comparación_de_modelos.html",
    "title": "8  Comparación de modelos",
    "section": "",
    "text": "9 La navaja de Occam: simplicidad y exactitud\nAl elegir entre explicaciones alternativas, existe un principio conocido como la navaja de Occam que establece de manera general que si tenemos dos o más explicaciones equivalentes para el mismo fenómeno, debemos elegir la más simple.\nHay muchas justificaciones para esta heurística; una de ellas está relacionada con el criterio de falsabilidad introducido por Popper, otra tiene una perspectiva más pragmática y afirma que, dado que los modelos más simples son más fáciles de entender que los modelos más complejos, es conveniente quedarse con el más simple. Otra justificación se basa en propia estadísticas Bayesiana, como veremos cuando analicemos los Factores de Bayes. Sin entrar en los detalles de estas justificaciones, vamos a aceptar este criterio como una regla útil por el momento, simplemente algo que suena como una guía razonable.\nOtro factor que generalmente debemos tener en cuenta al comparar modelos es su exactitud, es decir, qué tan bueno es un modelo ajustando los datos. Un cantidad comunmente usada en regresión es el coeficiente de determinación R², que podemos interpretar como la proporción de varianza explicada en una regresión lineal. En la sección anterior vimos algunos ejemplos de pruebas predictivas a posteriori a modo de evaluación de cual modelo ajusta mejor los datos. En definitiva, si tenemos dos (o más) modelos y uno de ellos explica los datos mejor que el otro, deberíamos preferir ese modelo, es decir, queremos el modelo con mayor exactitud ¿Verdad?\nIntuitivamente, parece que al comparar modelos, tendemos a preferir aquellos que mejor ajusten los datos y aquellos que sean más simples. Hasta ahora todo bien, pero ¿Qué hacemos si el modelo más simple es el peor ajustando los datos? O de forma más general, ¿Cómo balancear ambas contribuciones?"
  },
  {
    "objectID": "07_Comparación_de_modelos.html#pruebas-predictivas-a-posteriori",
    "href": "07_Comparación_de_modelos.html#pruebas-predictivas-a-posteriori",
    "title": "8  Comparación de modelos",
    "section": "8.1 Pruebas predictivas a posteriori",
    "text": "8.1 Pruebas predictivas a posteriori\nPreviamente hemos presentado y discutido las pruebas predictivas a posteriori como una forma de evaluar qué tan bien los modelos explican los mismos datos que se usan para ajustar al modelo. El propósito de este tipo de pruebas no es el de dictaminar que un modelo es incorrecto; ¡Esto ya lo sabemos! El objetivo del ejercicio es comprender qué tan bien estamos capturando los datos. Es frecuente que capturemos diferentes aspectos de los datos de diferentes maneras. Al realizar pruebas predictivas a posteriori, esperamos comprender mejor las limitaciones de un modelo, ya sea para tenerlas en cuenta o para intentar mejorar el modelo. Es esperable que un modelo no sea capaz de reproducir todos los aspectos de un problema y, por lo general, esto no es un problema ya que los modelos se construyen con un propósito en mente. Una prueba predictiva a posteriori es una forma de evaluar ese propósito, por lo tanto, si tenemos más de un modelo, podemos compararlos mediante pruebas predictivas a posteriori.\nComo ya vimos, las pruebas predictivas a posteriori a menudo se realizan mediante visualizaciones como en el siguiente ejemplo:\n\ndummy_data = np.loadtxt('datos/dummy.csv')\nx_1 = dummy_data[:,0]\ny_1 = dummy_data[:,1]\n\norder = 2\nx_1p = np.vstack([x_1**i for i in range(1, order+1)])\nx_1s = (x_1p - x_1p.mean(axis=1, keepdims=True)) / x_1p.std(axis=1, keepdims=True)\ny_1s = (y_1 - y_1.mean()) / y_1.std()\nplt.scatter(x_1s[0], y_1s)\nplt.xlabel('x')\nplt.ylabel('y');\n\n\n\n\nAhora, vamos a ajustar estos datos con dos modelos ligeramente diferentes, uno lineal y el otro un polinomio de orden 2, también conocido como modelo parabólico o cuadrático:\n\nwith pm.Model() as model_l:\n    α = pm.Normal('α', mu=0, sigma=1)\n    β = pm.Normal('β', mu=0, sigma=10)\n    ϵ = pm.HalfNormal('ϵ', 5)\n\n    μ = α + β * x_1s[0]\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y_1s)\n\n    idata_l = pm.sample(2000, idata_kwargs={\"log_likelihood\": True})\n    idata_l.extend(pm.sample_posterior_predictive(idata_l))\n\nwith pm.Model() as model_p:\n    α = pm.Normal('α', mu=0, sigma=1)\n    β = pm.Normal('β', mu=0, sigma=10, shape=order)\n    ϵ = pm.HalfNormal('ϵ', 5)\n\n    μ = α + pm.math.dot(β, x_1s)\n    \n    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y_1s)\n\n    idata_p = pm.sample(2000, idata_kwargs={\"log_likelihood\": True})\n    idata_p.extend(pm.sample_posterior_predictive(idata_p))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:05<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 5 seconds.\nSampling: [y_pred]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [α, β, ϵ]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:08<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 9 seconds.\nSampling: [y_pred]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\nAhora, vamos a visualizar el ajuste para ambos modelos:\n\nx_new = np.linspace(x_1s[0].min(), x_1s[0].max(), 100)\n\nposterior_l = az.extract(idata_l)\nposterior_p = az.extract(idata_p)\n\nα_l_post = posterior_l['α'].mean().item()\nβ_l_post = posterior_l['β'].mean().item()\ny_l_post = α_l_post + β_l_post *  x_new\n\nplt.plot(x_new, y_l_post, 'C0', label='modelo lineal')\n\nα_p_post = posterior_p['α'].mean().item()\nβ_p_post = posterior_p['β'].mean(\"sample\")\nidx = np.argsort(x_1s[0])\ny_p_post = α_p_post + np.dot(β_p_post, x_1s)\n\nplt.plot(x_1s[0][idx], y_p_post[idx], 'C1', label=f'polinomio de orden {order}')\n\nplt.plot(x_1s[0], y_1s, \"k.\")\nplt.legend();\n\n\n\n\nEl modelo de orden 2 parece estar haciendo un mejor trabajo, pero el modelo lineal no es tan malo. Veamos un prueba predictiva a posteriori\n\n_, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\naz.plot_ppc(idata_l, num_pp_samples=100, ax=axes[0], legend=False)\naxes[0].set_title('modelo lineal')\naz.plot_ppc(idata_p, num_pp_samples=100, ax=axes[1]);\naxes[1].set_title(f'polinomio de orden {order}');\n\n\n\n\nEn vez de comparar directamente la distribución de datos observados versus la distribución predicha podemos comparar estadísticos sumarios.\nEn el panel superior de la siguiente figura se muestra 2 KDEs, representando la distribución de las medias predichas por los modelos. El punto sobre eje x indica el valor observado.\nEn el segundo panel lo mismo pero para el rango intercuartil.\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 8), sharey=\"row\")\ncolors = [\"C0\", \"C1\"]\ntitles = [\"mediana\", \"rango intercuartil\"]\nmodelos = [\"lineal\", f'orden {order}']\nidatas = [idata_l, idata_p]\n\ndef iqr(x, a=-1):\n    return np.subtract(*np.percentile(x, [75, 25], axis=a))\n\nfor idata, c in zip(idatas, colors):\n    az.plot_bpv(idata, kind=\"t_stat\", t_stat=\"mean\", ax=axes[0], color=c)\n    \n\nfor idata, c in zip(idatas, colors):\n    az.plot_bpv(idata, kind=\"t_stat\", t_stat=iqr, ax=axes[1], color=c)\n\nfor ax, title, in zip(axes, titles):\n    ax.set_title(title)\n    for idx, (c, modelo) in enumerate(zip(colors, modelos)):\n        ax.legend_.legendHandles[idx]._alpha = 1\n        ax.legend_.legendHandles[idx]._color = c\n        ax.legend_._loc = 1\n        ax.legend_.texts[idx]._text = modelo + \" \" + ax.legend_.texts[idx]._text\n\n\n\n\nEn la figura anterior también se incluyen unos valores llamados bpv, Bayesian p-value. Los bpv son una forma numérica de resumir una comparación entre datos simulados y datos reales. Para obtenerlos se elige un estadístico sumario, como por ejemplo la mediana, el cual es calculado tanto para los datos como para las simulaciones. Luego contamos la cantidad de veces que el estadístico predicho es igual o mayor al calculado a partir de los datos observados. Si los valores observado y concuerdan con los predichos, deberíamos esperar un valor p de 0.5. Es decir la mitad de las predicciones están por debajo y la mitad por encima de lo observado. Caso contrario, estamos en presencia de una distribución predictiva a posteriori sesgada.\nSi estás familiarizado con los métodos frecuentistas es posible que ya conozcas el concepto de valor p. Además es posible que hayas escuchado que en estadística Bayesiana no se usan valores p. Por lo que este ejemplo puede generarte confusión. Veamos que está pasando. Estos bpv son efectivame valores p ya que se definen de como:\n\\[\\text{Bayesian p-value } \\triangleq p(T_{\\mathcal{sim}} \\le T_{\\mathcal{obs}} ) \\mid \\hat y)\\]\nEs decir, queremos estimar la probabilidad de obtener un estadístico \\(T_{sim}\\), a partir de las simulaciones, que sea igual o menor que la de obtener un valor de estadísitco \\(T_{obs}\\) a partir de los datos. En principio \\(T\\) puede ser casi cualquier cantidad derivada de los datos. En la figura anterior \\(T\\) es la mediana (panel superior) o el rango interquartil (panel inferior). En este ejemplo es razonable que la media de bien por que precisamente el modelo lineal está construido para capturar la media. Si en vez de graficar la media, evaluaramos la mediana, veriamos diferencias algo más grandes. En general un estadístico que sea ortogonal a lo que el modelo ajusta de forma directa será más informativo. Ante la duda puede ser conveniente evaluar más de un estadístico. En general es útil preguntarse que aspectos de los datos nos interesa capturar mejor.\nLo Bayesiano de estos valores p es que NO estamos usando una distribución de muestreo sino la distribución predictiva a posteriori. Además NO estamos asumiendo ninguna hipótesis nula para calcular el valor p. En cambio, estamos, permitiendo que los parámetros varíen de acuerdo con el modelo y los datos. Otra diferencia es que no estamos usando ningún método predefinido para declarar significación estadística, ni estamos realizando pruebas de hipótesis.\nOtra forma de usar los bvp, es preguntarse para cada valor observado, cual es la probabilidad de predecir un valor menor o igual\n\\[p(\\tilde y_i  \\le y_i \\mid y_i)\\]\nSi el modelo está bien calibrado la probabilidad debería ser la misma para todos los valores no importan si estos son altos, bajos, en las colas en el seno de la distribución etc. Es decir esperaríamos ver una distribución uniforme.\nSiguiedo esta idea, en la siguiente figura se muestra una gráfica que muestra la distribuciónes para el modelo lineal y el de order 2. La linea blanca indica la distribución uniforme esperada y la banda gris indica las desviaciones esperadas dado el tamaño finito de la muestra observada.\n\nfig, ax = plt.subplots(figsize=(10, 3))\n\nfor idata, c in zip(idatas, colors):\n    az.plot_bpv(idata, color=c, ax=ax)\n\n\n\n\nLas pruebas predictivas a posteriori, ya sea utilizando gráficos o resúmenes numéricos como los valores p bayesianos, o incluso una combinación de ambos son ideas muy flexibles. El concepto es lo suficientemente general para permitir que una analista use su imaginación para encontrar diferentes formas de explorar la distribución predictiva a posteriori y use las que mejor se ajusten a los fines de poder interpretar los datos y modelos.\nEn las siguientes secciones vamos a explorar otros métodos para comparar modelos."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#muchos-parámetros-pueden-conducir-a-sobreajuste",
    "href": "07_Comparación_de_modelos.html#muchos-parámetros-pueden-conducir-a-sobreajuste",
    "title": "8  Comparación de modelos",
    "section": "9.1 Muchos parámetros (pueden) conducir a sobreajuste",
    "text": "9.1 Muchos parámetros (pueden) conducir a sobreajuste\nVamos a comenzar por combinar polinomios cada vez más complejos en un conjunto de datos muy simple. En lugar de utilizar la maquinaria Bayesiana, usaremos la aproximación de mínimos cuadrados para ajustar modelos lineales. Recuerde que este último se puede interpretar desde una perspectiva Bayesiana como un modelo con a prioris planos. Entonces, en cierto sentido, seguimos siendo Bayesianos solo que estamos tomando un atajo ;-)\n\n_, ax = plt.subplots(1, 1, figsize=(12, 4))\n\n\nx0 = np.array([4., 5., 6., 9., 12, 14.])\ny0 = np.array([4.2, 6.1, 5., 10., 10, 14.])\n\norder = [0, 1, 5]\nax.plot(x0, y0, 'ko', zorder=3)\n\n\nax.set_yticks([])\nax.set_xticks([])\n\nx_n = np.linspace(x0.min(), x0.max(), 100)\nps = []\nfor i in order:\n    p = np.polynomial.Polynomial.fit(x0, y0, deg=i)\n    ps.append(p)\n    yhat = p(x0)\n    ybar = np.mean(y0)\n    ss_regression = np.sum((yhat-y0)**2)\n    ss_total = np.sum((ybar-y0)**2)\n    r2 = 1 - ss_regression / ss_total\n    ax.plot(x_n, p(x_n), label=f'orden {i}, $R^2$= {r2:.3f}')\n\n    \nax.legend(loc=2, fontsize=12);\n\n\n\n\nDe la figura anterior podemos ver que el aumento de la complejidad del modelo se acompaña de una mayor exactitud reflejada en el coeficiente de determinación R². De hecho, podemos ver que el polinomio de orden 5 se ajusta perfectamente a los datos, obteniendo un R²=1.\n¿Por qué el polinomio de grado 5 puede capturar los datos sin perder uno solo de ellos? La razón es que tenemos el mismo número de parámetros que de datos es decir 6. Por lo tanto, el modelo está actuando simplemente como una forma alternativa de expresar los datos. El modelo no está aprendiendo algo sobre los datos, ¡Está memorizando los datos! A partir de este simple ejemplo, podemos ver que un modelo con mayor ajuste no siempre es lo ideal.\nAhora agregaremos dos datos nuevos y sin volver a ajustar los modelos veremos como cambia el R². Se puede ver que al modelo lineal le va mejor en este caso que al polinomial.\n\n_, ax = plt.subplots( figsize=(12, 4))\nx_ = np.array([6.5, 10])\ny_ = np.array([7, 10])\n\nax.plot(x0, y0, 'ko', zorder=3)\nax.plot(x_, y_, 'ks', zorder=3)\n\nax.set_yticks([])\nax.set_xticks([])\n\nx1 = np.concatenate((x0, x_))\ny1 = np.concatenate((y0, y_))\n\nfor idx, i in enumerate(order):\n    yhat = ps[idx](x1)\n    ybar = np.mean(y1)\n    ss_regression = np.sum((yhat-y1)**2)\n    ss_total = np.sum((ybar-y1)**2)\n    r2 = 1 - ss_regression / ss_total\n    ax.plot(x_n, ps[idx](x_n), label=f'orden {i}, $R^2$= {r2:.3f}')\n\n    \nax.legend(loc=2, fontsize=12);\n\n\n\n\nCuando un modelo ajusta muy bien, el conjunto de datos utilizado para aprender los parámetros de ese modelo, pero muy mal otros conjuntos de datos, decimos que tenemos sobreajuste (overfitting). Este es un problema muy común al analizar datos.\nUna forma muy útil de pensar el sobreajuste es considerar que un conjunto de datos tiene dos componentes; la señal y el ruido. La señal es lo que queremos capturar (o aprender) de los datos. Si usamos un conjunto de datos es porque creemos que hay una señal allí, de lo contrario será un ejercicio fútil. El ruido, en cambio, no es útil y es el producto de los errores de medición, las limitaciones en la forma en que se generaron o capturaron los datos, la presencia de datos corruptos, etc. Un modelo sobreajusta cuando es tan flexible (para un conjunto de datos) que es capaz de aprender el ruido. Esto tiene como consecuencia que la señal queda oculta.\nEsta es una justificación práctica para la navaja de Occam. Y nos advierte que al menos en principio, siempre es posible crear un modelo tan complejo que explique todos los detalles, incluso los más irrelevantes. Tal como en el Imperio descripto por Borges, donde los cartógrafos alcanzaron tal nivel de sofisticación que crearon un mapa del Imperio cuyo tamaño era el del propio Imperio, y que coincidía punto por punto con él."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#muy-pocos-parámetros-conducen-a-un-subajuste",
    "href": "07_Comparación_de_modelos.html#muy-pocos-parámetros-conducen-a-un-subajuste",
    "title": "8  Comparación de modelos",
    "section": "9.2 Muy pocos parámetros conducen a un subajuste",
    "text": "9.2 Muy pocos parámetros conducen a un subajuste\nContinuando con el mismo ejemplo pero en el otro extremo de complejidad, tenemos el modelo de orden 0. Este modelo es simplemente una Gaussiana disfrazada de modelo lineal. Este modelo solo es capaz de capturar el valor de la media de \\(Y\\), y es por lo tanto totalente indiferente a los valores de \\(x\\). Decimos que este modelo ha subajustado los datos."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#el-equilibrio-entre-simplicidad-y-exactitud",
    "href": "07_Comparación_de_modelos.html#el-equilibrio-entre-simplicidad-y-exactitud",
    "title": "8  Comparación de modelos",
    "section": "9.3 El equilibrio entre simplicidad y exactitud",
    "text": "9.3 El equilibrio entre simplicidad y exactitud\nTodo debe hacerse tan simple como sea posible, pero no más simple es una cita que a menudo se atribuye a Einstein y es similar a la navaja de Occam. Al igual que en una dieta saludable, al modelar tenemos que mantener un balance. Idealmente, nos gustaría tener un modelo que ni sub-ajuste ni sobre-ajuste los datos. De alguna forma hay que balancear simplicidad y bondad de ajuste."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#medidas-de-exactitud-predictiva",
    "href": "07_Comparación_de_modelos.html#medidas-de-exactitud-predictiva",
    "title": "8  Comparación de modelos",
    "section": "9.4 Medidas de exactitud predictiva",
    "text": "9.4 Medidas de exactitud predictiva\nEn el ejemplo previo, es relativamente facil de ver que el modelo de orden 0 es demasiado simple mientras que el modelo de orde 5 es demasiado complejo. Pero que podemos decir de los otros dos modelos? Cómo podríamos establecer un ranking numérico de estos modelos? Para poder hacer esto necesitamos formalizar nuestra intuición sobre este balance entre simplicidad y exactitud\nVeamos un par de términos que nos serán de utilidad.\n\nExactitud dentro de la muestra (within-sample accuracy). La exactitud medida con los mismos datos usado para ajustar el modelo.\nExactitud fuera de la muestra (out-of-sample accuracy). La exactitud medida con datos no usados para ajustar el modelo.\n\nLa exactitud dentro de la muestra será, en promedio, menor a la exactitud fuera de la muestra. Es por ello que usar la exactitud dentro de la muestra para evaluar un modelo en general conducirá a pensar que tenemos un mejor modelo de lo que realmente es. Utilizar la exactitud fuera de la muestra es por lo tanto una mejor idea para evitar engañarnos a nosotros mismos. Sin embargo, esta aproximación requiere dejar datos fuera del ajuste, lo cual es un lujo que en general no nos podemos dar. Ya que este es un problema central en el análisis de datos existen varias propuestas para abordarlo. Dos aproximaciones muy populares son:\n\nValidación cruzada: esta es una estrategia empírica basada en dividir los datos disponibles en subconjuntos separados que se utilizan para ajustar y evaluar de forma alternativa\nCriterios de información: este es un término general usado para referirse a varias expresiones que aproximan la exactitud fuera de la muestra como la exactitud dentro de la muestra más un término que penaliza a modelos complejos.\n\n\n9.4.1 Validación cruzada\nLa validación cruzada es una solución simple y, en la mayoría de los casos, efectiva para comparar modelos. Tomamos nuestros datos y los dividimos en K porciones. Intentamos mantener las porciones más o menos iguales (en tamaño y, a veces, también en otras características, como, por ejemplo, un número igual de clases). Luego usamos K-1 porciones para entrenar el modelo y el resto para evaluarlo. Este proceso se repite sistemáticamente dejando, por cada iteración, una porción diferente fuera del conjunto de entrenamiento y usando esa porción como el conjunto de validación. Esto se repite hasta que hayamos completado K rondas de ajuste-evaluación. La exactitud del modelo será la del promedio a lo largo de las K rondas. Esto se conoce como validación cruzada K-fold. Cuando K es igual a la cantidad de puntos de datos, obtenemos lo que se conoce como validación cruzada dejando un punto afuera (LOOCV del inglés leave-one-out cross-validation). Por último una vez que hemos relizado la validación cruzada, usamos todos los datos para ajustar por última vez nuestro modelo y este es el modelo que se utiliza para hacer predicciones o para cualquier otro fin.\n\nLa validación cruzada es una práctica estádard en en machine learning. Y apenas hemos descripto los aspectos más esenciales de esta práctica. Para mayor información pueden leer The Hundred-Page Machine Learning Book](http://themlbook.com/) o Python Machine Learning, by Sebastian Raschka, o Python Data Science Handbook by Jake Vanderplas.\nLa validación cruzada es una idea muy simple y útil, pero para algunos modelos o para grandes cantidades de datos, el costo computacional de la validación cruzada puede estar más allá de nuestras posibilidades. Muchas personas han tratado de encontrar cantidades más simples de calcular que se aproximen a los resultados obtenidos con la validación cruzada o que funcionen en escenarios donde la validación cruzada no puede ser tan fácil de realizar. Y ese es el tema de la siguiente sección.\n\n\n9.4.2 Criterios de información\nLos criterios de información son una colección de herramientas estrechamente relacionadas que se utilizan para comparar modelos en términos de la bondad del ajuste y de la complejidad del modelo. En otras palabras, los criterios de información formalizan la intuición que desarrollamos al comienzo del capítulo. La forma exacta en que se derivan estas cantidades tiene que ver con un campo conocido como Teoría de la Información.\n\n9.4.2.1 El log-likelihood y la deviance\nUna forma intuitiva de medir qué tan bien un modelo se ajusta a los datos es calcular el error cuadrático medio entre los datos y las predicciones realizadas por el modelo:\n\\[\\frac{1}{n} \\sum_{i=1}^{n}  (y_i - \\operatorname{E} (y_i \\mid \\theta))^2\\]\n\\(\\operatorname{E} (y_i \\mid \\theta)\\) es el valor predicho dados los parámetros estimados. Es importante notar que esto es esencialmente el promedio entre la diferencia entre los datos observados y los predichos. Tomar el cuadrado de los errores asegura que las diferencias no se cancelen y enfatiza grandes errores comparado con otros alternativas como por ejemplo calcular el valor absoluto.\nEl error cuadrático medio, puede resultarnos familiar ya que es muy popular. Pero si nos detenemos a reflexionar sobre esta cantidad veremos que en principio no tiene nada de especial y bien podríamos idear otras expresiones similares. Cuando adoptamos una aproximación probabilista vemos que una expresión más general (y natural) es la siguiente:\n\\[ \\sum_{i=1}^{n} \\log p(y_i \\mid \\theta)\\]\nEsto es, la suma (sobre \\(n\\) datos) de los likelihoods (en escala logarítmica). Esto es natural por que al elegir un likelihood en un modelo estamos eleigiendo implícitamente una métrica para evaluar el ajuste del modelo. Cuando \\(p(y_i \\mid \\theta)\\) es una gaussiana entonces la suma de log-likelihood será proporcional al error cuadrático medio.\nCuando se discuten criterios de información y por razones puramente históricas suele ser común hablar de deviance que es simplemente:\n\\[-2\\ \\sum_{i=1}^{n} \\log \\ p(y_i \\mid \\theta)\\]\nEs decir el log-likelihood multiplicado por -2.\nLa deviance es usada en contextos Bayesianos y no Bayesianos, la diferencia es que bajo un marco Bayesiano \\(\\theta\\) representa una distribución de probabilidad y no una estimación puntual.\n\nCuanto menor es la deviance, mayor es el likelihood y mayor es el acuerdo entre las predicciones del modelo y los datos. Es decir, a menor deviance, mejor ajuste\nLa deviance se calcula a partir de los datos usados para ajustar el modelo es por lo tanto una forma de estimar la exactitud dentro de la muestra. Como ya vimos esto significa que en promedio la deviance va a tender a elegir modelos más complejos, necesitamos por lo tanto algún criterio para balancear esa tendencia.\n\nEn las siguientes secciones, aprenderemos sobre diferentes criterios de información. Los cuales tienen en común el uso de la desviace y un término de penalización. La diferencia radica en cómo se calculan cada uno de estos dos términos.\n\n\n9.4.2.2 Criterio de información de Akaike\nEste es un criterio de información muy conocido y ampliamente utilizado fuera del universo Bayesiano y se define como:\n\\[AIC = -2 \\sum_{i=1}^{n} \\log p(y_i \\mid \\hat{\\theta}_{mle}) + 2 k \\]\nDonde, k es el número de parámetros del modelo y \\(\\hat{\\theta}_{mle}\\) es la estimación por máxima verosimilitud para \\(\\theta\\).\nLa estimación de máxima verosimilitud es una práctica común para los no-bayesianos y, en general, es equivalente a la estimación Bayesiana del máximo a posteriori (MAP) cuando se usan priors planos. Es importante notar que $_{mle} $ es una estimación puntual y no una distribución.\nAcá vemos como el factor \\(-2\\) aparece de nuevo, como ya dijimos esto tiene razones históricas y no es relevante ya que es una constante. Lo importante, desde el punto de vista práctico, es que el primer término toma en cuenta cuan bien el modelo ajusta los datos, mientras que el segundo término penaliza la complejidad del modelo. Por lo tanto si dos modelos ajustan los datos igualmente bien. AIC dice que deberemos elegir aquel modelo con el menor número de parámetros, lo cual nos recuerda a la navaja de Ocam.\nAIC funciona bien en enfoques no-bayesianos, pero de lo contrario es problemático. Una de las razones es que no utiliza la distribución a posteriori y, por lo tanto, descarta información sobre la incertidumbre en la estimación. Además AIC asume que los priors son planos y, por lo tanto, AIC es incompatible con priors informativos y ligeramente informativos como los utilizados en este libro. Tampoco es buena idea usarlo con modelos jerárquicos por lo que ya dijimos. Además, la cantidad de parámetros de un modelo no es una buena medida de la complejidad del mismo cuando se usan priors informativos o estructuras como la jerárquica ya que estas son formas de regularizar el modelo que es una forma de reducir la cantidad efectiva de parámetros. Más adelante volveremos sobre esta idea de regularización.\n\n\n9.4.2.3 Widely applicable information criterion\nWAIC es algo así como la versión Bayesiana de AIC, al igual que este último WAIC se compone de dos términos uno que mide el ajuste y otro que penaliza. La siguiente expresión asume que la distribución a posteriori está representada como una muestra de tamaño S.\n\\[WAIC = -2 \\sum_i^n \\log \\left(\\frac{1}{S} \\sum_{s=1}^S p(y_i \\mid \\theta^s) \\right) + 2 \\sum_i^n  \\left( V_{s=1}^S \\log p(y_i \\mid \\theta^s) \\right)\\]\nEl primer térino es similar al criterio de Akaike, solo que evaluado para todas las observaciones y todas las muestras del posterior. El segundo término es un poco más dificil de justificar sin entrar en tecnicismos. Pero es tabién una forma de penalizar la complejidad del modelo. Lo importante desde el punto de vista práctico es que WAIC usa todo el posterior (y no una estimación puntual) para el cálculo de ambos términos, por lo que WAIC puede ser aplicado virtualmente a cualquier modelo Bayesiano.\n\n\n9.4.2.4 Validación cruzada de dejando uno afuera mediante muestreo de importancia usando un suavizado de Pareto\nEl problema clave con la validación cruzada dejando uno fuera es que es muy costosa ya que tenemos que reajustar el modelo tantas veces como datos tengamos. Por suerte, hay formas de evitar la fuerza bruta y aproximar la estimación ajustando una sola vez lo datos, y esto es lo que hace el método “muestreo de importancia usando un suavizado de Pareto”. El nombre es tan poco amigable que en la práctica le decimos LOO. Conceptualmente lo que estámos tratando de calcular es:\n\\[\n\\text{ELPD}_\\text{LOO-CV} = \\sum_{i=1}^{n} \\log\n    \\int \\ p(y_i \\mid \\theta) \\; p(\\theta \\mid y_{-i}) d\\theta\n\\]\n\\[\n\\sum_{i}^{n} \\log\n    \\left( \\frac{1}{s}\\sum_j^s \\mathbin{\\color{#E9692C}{p(y_i \\mid \\theta_{-i}^j)}} \\right)\n\\]\nEs posible aproximar \\(\\color{#E9692C}{p(y_i \\mid \\theta_{-i}^j})\\) usando importance sampling, que es una forma de approximar una distribución repesando valores obtenidos de otra distribución. En nuestro caso la distribución conocida, una vez ajustado un modelo, es el log-likelihood. Y queremos aproximar el log-likelihood si hubieramos eliminado una observación. Para ello necesitamos estimar la “importancia” (o peso) que cada observación tiene en determinar la distribución a posteriori. Una distribución será más “importante” (o pesada) mientras más cambie el posterior al eliminar esa observación. Intuitivamente una observación relativamente poco probable es más importante que una relativamente esperada. Mientras mayor la sorpresa, mayor la importancia. Por suerte estos pesos se puede estimar sin necesidad de reajustar el modelo, de hecho el peso de la observación \\(i\\) para la muestra del posterior \\(s\\) es:\n\\[\nw_s = \\frac{1}{p(y_i \\mid \\theta_s)}\n\\]\nEl problema es que bajo ciertas condiciones estos pesos puede no ser confiables. El principal problema es que unos pocos \\(w_s\\) podrían ser tan grandes que dominan el cálculo, y es aquí donde entra el suavizado de Pareto que basicamente consiste en reemplazar algunos de estos pesos por pesos obtenidos a partir de ajustar una distribución de Pareto ¿por qué una distribución de Pareto? Por que la teoría indica que los pesos debeŕian seguir esta distribución. Entonces para cada observation \\(y_i\\) , los pesos más grandes se usan para estimar una distribución de Pareto y esa distribución se usa para reemplazar esos pesos por pesos “suavizados”. Este procedimiento le da robustes a la estimación del ELPD y además provee de un diagóstico ya que valores de \\(k\\) (uno de los parámetros de la distribución de Pareto) mayores a 0.7 indican que posiblemente tengamos observaciones “muy influyentes”.\n\n\n9.4.2.5 Otros criterios de información\nOtro criterio de información muy usado es DIC, si usamos el bayesómetero™ DIC, es más bayesiano que AIC pero menos que WAIC. Aunque aún es popular, WAIC y LOO han demostrado ser más útiles tanto teóricamente como empíricamente que DIC. Por lo cual NO recomendamos su uso.\nOtro criterio muy usado es BIC (del inglés Bayesian Information Criteria), al igual que la regresión logística y la * sopa seca * de mi madre, este nombre puede ser engañoso. BIC se propuso como una forma de corregir algunos de los problemas con AIC y el autor propuso una justificación Bayesiana para ello. Pero BIC no es realmente Bayesiano en el sentido que al igual que AIC asume priors planos y utiliza una estimación por máxima verosimilitud.\nPero lo que es más importante, es que BIC difiere de AIC y WAIC en su objetivo. AIC y WAIC intentan reflejar cual modelo generaliza mejor a otros datos (exactitud predictiva) mientras que BIC intenta identificar cual es el modelo correcto y por lo tanto está más relacionado los factores de Bayes que con WAIC. Más adelante discutiremos Factores de Bayes y veremos como se diferenci de criterios como WAIC y LOO."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#calcular-los-criterios-de-información-con-arviz",
    "href": "07_Comparación_de_modelos.html#calcular-los-criterios-de-información-con-arviz",
    "title": "8  Comparación de modelos",
    "section": "9.5 Calcular los criterios de información con ArviZ",
    "text": "9.5 Calcular los criterios de información con ArviZ\nAfortunadamente, calcular los criterios de información con ArviZ es muy simple. Veamos:\n\nwaic_l = az.waic(idata_l)\nwaic_l\n\nComputed from 8000 posterior samples and 33 observations log-likelihood matrix.\n\n          Estimate       SE\nelpd_waic   -14.27     2.65\np_waic        2.36        -\n\n\n\nwaic_p = az.waic(idata_p)\nwaic_p\n\nComputed from 8000 posterior samples and 33 observations log-likelihood matrix.\n\n          Estimate       SE\nelpd_waic    -4.49     2.34\np_waic        2.61        -\n\n\nLo mismo para LOO.\n\nloo_l = az.loo(idata_l)\nloo_l\n\nComputed from 8000 posterior samples and 33 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo   -14.30     2.66\np_loo        2.39        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)       33  100.0%\n (0.5, 0.7]   (ok)          0    0.0%\n   (0.7, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\n\nloo_p = az.loo(idata_p)\nloo_p\n\nComputed from 8000 posterior samples and 33 observations log-likelihood matrix.\n\n         Estimate       SE\nelpd_loo    -4.52     2.35\np_loo        2.64        -\n------\n\nPareto k diagnostic values:\n                         Count   Pct.\n(-Inf, 0.5]   (good)       33  100.0%\n (0.5, 0.7]   (ok)          0    0.0%\n   (0.7, 1]   (bad)         0    0.0%\n   (1, Inf)   (very bad)    0    0.0%\n\n\nTanto az.waic como az.loo devuelven 3 valores\n\nUna estimación puntual del ELPD.\nEl error estándar de esa estimación\nEl número efectivo de parámetros\n\nAdemás LOO devuelve un diagnóstico basado en el parámetro k, correspondiente al ajuste de la distribución de Pareto.\nLos valores de WAIC o LOO no tienen sentido por si mismos, si no que deben ser interpretados de forma relativa. Es por ello que ArviZ ofrece dos funciones auxiliares para facilitar esta comparación veamos primero a az.compare.\n\ncmp_df = az.compare({'modelo_l':idata_l, 'modelo_p':idata_p})\ncmp_df\n\n\n\n\n\n  \n    \n      \n      rank\n      elpd_loo\n      p_loo\n      elpd_diff\n      weight\n      se\n      dse\n      warning\n      scale\n    \n  \n  \n    \n      modelo_p\n      0\n      -4.517563\n      2.638813\n      0.000000\n      1.000000e+00\n      2.345124\n      0.000000\n      False\n      log\n    \n    \n      modelo_l\n      1\n      -14.295518\n      2.387078\n      9.777955\n      2.291500e-13\n      2.657647\n      2.684915\n      False\n      log\n    \n  \n\n\n\n\nEn las filas tenemos los modelos comparados y en la columnas tenemos\n\nrank : el orden de los modelos (de mejor a peor)\nelpd : la estimación puntual del elpd usando\np : los parámetros efectivos\nelpd_diff : la diferencia entre el ELPD del mejor modelo y los demás modelos\nweight : el peso relativo de cada modelo. Si quisieramos hacer predicciones combinando los distintos modelos, en vez de elegir uno solo, este sería el peso que deberíamos asignar a cada modelo. En este caso vemos que el modelo polinomial se lleva todo el peso.\nse : el error estándard del ELPD\ndse : el error estándard de las difencias\nwarning : una advertencia sobre valores de k altos\nscale : la escala en la que se calcula el ELPD\n\nTambién podemos obtener más o menos la misma información de forma gráfica usando la función az.compareplot.\n\naz.plot_compare(cmp_df);\n\n\n\n\n\nLos círculos vacíos representan los valores del ELPD y lineas negras el error estándar.\nEl valor más alto del ELPD se indica con una línea gris discontinua vertical para facilitar la comparación con otros valores.\nPara todos los modelos, excepto el mejor, también obtenemos un triángulo que indica el valor de la diferencia del ELPD entre cada modelo y el mejor modelo. La barra de error gris que indica el error estándar de las diferencias entre las estimaciones puntuales.\n\nLa forma más sencilla de utilizar los criterios de información es elegir un único modelo. Simplemente elija el modelo con el valor más alto de ELPD. Si seguimos esta regla tendremos que aceptar que el modelo cuadrático es el mejor. Incluso si tenemos en cuenta los errores estandar podemos ver que estos no se solapan. Lo que nos da cierta seguridad que efectivamente los modelos son diferentes entre si. Si, en cambio, los errores estándar se superpusieran, deberíamos proporcionar una respuesta más matizada."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#promedio-de-modelos",
    "href": "07_Comparación_de_modelos.html#promedio-de-modelos",
    "title": "8  Comparación de modelos",
    "section": "9.6 Promedio de modelos",
    "text": "9.6 Promedio de modelos\nLa selección de modelos es atractiva por su simplicidad, pero podríamos estar descartando información sobre la incertidumbre en nuestros modelos. Esto es de alguna manera similar a calcular el posterior completo y luego solo mantener la media del posterior; esto puede conducirnos a confiar demasiado en lo que creemos saber.\nUna alternativa es seleccionar un solo modelo, pero informar y analizar los diferentes modelos junto con los valores de los criterios de información calculados, sus valores de error estándar y quizás también las pruebas predictivas a posteriori. Es importante poner todos estos números y pruebas en el contexto de nuestro problema para que nosotros y nuestra audiencia podamos tener una mejor idea de las posibles limitaciones y deficiencias de los modelos. Para quienes trabajan en el mundo académico, estos elementos se pueden utilizar para agregar elementos a la sección de discusión de un paper, presentación, tesis, etc. Y en la industria esto puede ser útil para informar a clientes sobre las ventajas y limitaciones de las predicciones o conclusiones del modelado.\nOtra posibilidad es promediar los modelos. De esta forma estamos introduciendo la incertidumbre que tenemos sobre la bondad de cada modelo. De esta fora podemos generar un metamodelo (y meta-predicciones) usando un promedio pesado de cada modelo.\n\nidata_w = az.weight_predictions(idatas, weights=[0.35, 0.65])\n\n\n_, ax = plt.subplots(figsize=(10, 6))\naz.plot_kde(idata_l.posterior_predictive['y_pred'].values, plot_kwargs={'color':'C0'}, label='modelo lineal', ax=ax)\naz.plot_kde(idata_p.posterior_predictive['y_pred'].values, plot_kwargs={'color':'C1'}, label='orden 2', ax=ax)\naz.plot_kde(idata_w.posterior_predictive['y_pred'].values, plot_kwargs={'color':'C2'}, label='modelo pesado', ax=ax)\n\nplt.plot(y_1s, np.zeros_like(y_1s), 'k|', label='observed data')\nplt.yticks([])\nplt.legend();\n\n\n\n\nHay otras formas de promediar modelos, como, por ejemplo, construir explícitamente un metamodelo que incluya todos los modelos de interés como casos particulares. Por ejemplo un polinomio de grado 2 contiene como caso particular un modelo lineal, o un modelo jerárquico es la versión continua entre dos extremos un modelo agrupado y uno desagrupado."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#factores-de-bayes",
    "href": "07_Comparación_de_modelos.html#factores-de-bayes",
    "title": "8  Comparación de modelos",
    "section": "9.7 Factores de Bayes",
    "text": "9.7 Factores de Bayes\nUna alternativa LOO, validación cruzada y los criterios de información son los factores de Bayes. Es común que factores de Bayes aparezcan en la literatura como una alternativa Bayesiana al contraste de hipótesis frecuentista.\nLa “manera Bayesiana” de comparar modelos es calcular la verosimilitud marginal de cada modelo \\(p(y \\mid M_k)\\), es decir, la probabilidad de los datos observados \\(Y\\) dado el modelo \\(M_k\\). Esta cantidad, la verosimilitud marginal, es simplemente la constante de normalización del teorema de Bayes. Podemos ver esto si escribimos el teorema de Bayes y hacemos explícito el hecho de que todas las inferencias dependen del modelo.\n\\[p (\\theta \\mid Y, M_k ) = \\frac{p(Y \\mid \\theta, M_k) p(\\theta \\mid M_k)}{p(Y \\mid M_k)}\\]\ndónde:\n\n\\(y\\) son los datos\n\\(\\theta\\) los parámetros\n\\(M_k\\) un modelo de k modelos competidores\n\nSi nuestro objetivo principal es elegir solo un modelo, el mejor, de un conjunto de modelos podemos elegir el que tiene el mayor valor de \\(p(y \\mid M_k)\\). Esto está bien si asumimos que todos los modelos tienen la misma probabilidad a priori. De lo contrario debemos calcular:\n\\[p(M_k \\mid y) \\propto p(y \\mid M_k) p(M_k)\\]\nSi en cambio, nuestro objetivo principal es comparar comparar modelos para determinar cuáles son más probables y en qué medida. Esto se puede lograr utilizando los factores de Bayes:\n\\[FB_{01} = \\frac{p(y \\mid M_0)}{p(y \\mid M_1)}\\]\nes decir, el cociente entre la verosimilitud marginal de dos modelos. Cuanto mayor sea el FB, mejor el modelo en el numerador (\\(M_0\\) en este ejemplo). Para facilitar la interpretación de los FB, Harold Jeffreys propuso una escala para la interpretación de los Factores de Bayes con niveles de apoyo o fuerza. Esta es solo una manera de poner números en palabras.\n\n1-3: anecdótico\n3-10: moderado\n10-30: fuerte\n30-100: muy fuerte\n\\(>\\) 100: extremo\n\nHay que tener en cuenta que si se obtiene números por debajo de 1, entonces el soporte es para el modelo en el denominador, también hay tablas disponibles para esos casos. O simplemente podemos tomar la inversa de los valores del valor obtenido.\nEs muy importante recordar que estas reglas son solo convenciones, guías simples en el mejor de los casos. Los resultados siempre deben ponerse en el contexto de nuestros problemas y deben ir acompañados de suficientes detalles para que otros puedan evaluar por sí mismos si están de acuerdo con nuestras conclusiones. No es lo mismo la prueba necesaria para asegurar algo en física de partículas, o en un juzgado, o para decidir realziar una evacuación frente a una catástrofe natural que se avecina."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#algunas-observaciones",
    "href": "07_Comparación_de_modelos.html#algunas-observaciones",
    "title": "8  Comparación de modelos",
    "section": "9.8 Algunas observaciones",
    "text": "9.8 Algunas observaciones\nAhora discutiremos brevemente algunos hechos clave sobre la verosimilitud marginal\n\nEl bueno\n\nNavaja de Occam incluida: Los modelos con más parámetros tienen una penalización mayor que los modelos con menos parámetros. La razón intuitiva es que cuanto mayor es el número de parámetros, más se extiende el prior con respecto al likelihood.\n\nEl malo\n\nPara muchas problema la verosimilitud marginal no puede ser calculada analiticamente. Y aproximarla numéricamente suele ser una tarea difícil que suele requerir de métodos especializados. Esto se debe a que es necesario calcular una integral de una función altamente variable en un espacio de parámetros de gran dimensión.\n\n\n\\[p(y \\mid M_k) = \\int_{\\theta_k} p(y \\mid \\theta_k, M_k) \\; p(\\theta_k | M_k) \\; d\\theta_k\\]\n\nEl feo\n\nLa probabilidad marginal depende sensiblemente de la distribución a prior para los parámetros en cada modelo \\(p(\\theta_k \\mid M_k)\\).\n\n\nEs importante notar que lo bueno y lo feo están relacionados. Usar la verosimilitud marginal para comparar modelos es una buena idea porque ya incluye una penalización para modelos complejos (lo que nos ayuda a prevenir el sobreajuste) y, al mismo tiempo, un cambio en el prior afectará los cálculos de la verosimilitud marginal. Al principio esto suena un poco tonto; ya sabemos que los priors afectan los cálculos (de lo contrario, simplemente podríamos evitarlos), pero el punto aquí es la palabra sensiblemente. Estamos hablando que cambios en el posterior que apenas tendrían efecto en el posterior tendrán un gran impacto en el valor de la verosimilitud marginal.\nEl uso de los FB suele ser una divisoria de aguas entre Bayesianos. La dificultad de su cálculo y la sensibilidad a los priors son algunos de los argumentos. Otra razón es que al igual que lo p-valores y en general las pruebas de hipótesis los BF favorecen el pensamiento dicotómico por sobre la estimación del “tamaño del efecto”. Es decir en vez de hacernos preguntas del estilo ¿Cuantos años más de vida puede proporcionar, en promedio, un tratamiento oncológico? terminamos preguntando si la diferencia entre tratar y no tratar a un paciente es “estadísticamente significativa”. Ojo que esta última pregunta puede ser útil en algunos contextos, el punto es que en muchos otros contextos, ese tipo de preguntas no es la pregunta más relevante."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#cálculo-de-los-fb",
    "href": "07_Comparación_de_modelos.html#cálculo-de-los-fb",
    "title": "8  Comparación de modelos",
    "section": "9.9 Cálculo de los FB",
    "text": "9.9 Cálculo de los FB\nLa verosimilitud marginal generalmente no está disponible en forma cerrada, excepto para algunos modelos. Por esta razón, se han ideado muchos métodos para calcular la probabilidad marginal. Algunos de estos métodos son tan simples e ingenuos que funciona muy mal en la práctica. La mayoría de los métodos útiles se han propuesto originalmente en el campo de la mecánica estadística. Esta conexión se explica porque la verosimilitud marginal es análoga a una cantidad central en física estadística conocida como función de partición que a su vez está estrechamente relacionada con otra cantidad muy importante, la energía libre. Muchas de las conexiones entre la mecánica estadística y la inferencia bayesiana se resumen aquí.\n\n9.9.1 Analiticamente\nPara algunos modelos, como el modelo beta-binomial, podemos calcular la verosimilitud marginal analíticamente. Si escribimos este modelo como:\n\\[\\theta \\sim Beta(\\alpha, \\beta)\\] \\[y \\sim Bin(n=1, p=\\theta)\\]\nla verosimilitud marginal será:\n\\[p(y) = \\binom {n}{h} \\frac{B(\\alpha + h,\\ \\beta + n - h)} {B(\\alpha, \\beta)}\\]\ndónde:\n\n\\(B\\) es la función beta no confundirse con la distribución \\(Beta\\)\n\\(n\\) es el número de intentos\n\\(h\\) es el número de éxito\n\nComo solo nos importa el valor relativo de la verosimilitud marginal bajo dos modelos diferentes (para los mismos datos), podemos omitir el coeficiente binomial \\(\\binom {n}{h}\\), por lo que podemos escribir:\n\\[p(y) \\propto \\frac{B(\\alpha + h,\\ \\beta + n - h)} {B(\\alpha, \\beta)}\\]\nEsta expresión ha sido codificada en la siguiente celda, pero con un giro. Usaremos la función betaln en lugar de la función beta, esto se hace para evitar el overflow.\n\ndef beta_binom(prior, y):\n    \"\"\"\n    Calcula la probabilidad marginal, analíticamente, para un modelo beta-binomial.\n\n     prior : tupla\n         tupla de parámetro alfa y beta para el prior (distribución beta)\n     y : array\n         array con \"1\" y \"0\" correspondientes al éxito y falla respectivamente\n    \"\"\"\n    alpha, beta = prior\n    h = np.sum(y)\n    n = len(y)\n    p_y = np.exp(betaln(alpha + h, beta + n - h) - betaln(alpha, beta))\n    return p_y\n\nNuestros datos para este ejemplo consisten en 100 “lanzamientos de una moneda” y el mismo número de “caras” y cecas” observadas. Compararemos dos modelos uno con un prior uniforme y otro con un prior más concentrado alrededor de \\(\\theta = 0.5\\)\n\ny = np.repeat([1, 0], [50, 50])  # 50 \"caras\" y 50 \"cecas\"\npriors = ((1, 1), (30, 30))\n\n\nfor a, b in priors:\n    x = np.linspace(0, 1, 300)\n    x_pdf = pz.Beta(a, b).rv_frozen.pdf(x)\n    plt.plot(x, x_pdf, label=rf\"$\\alpha$ = {a:d}, $\\beta$ = {b:d}\")\n    plt.yticks([])\n    plt.xlabel(\"$\\\\theta$\")\n    plt.legend()\n\n\n\n\nLa siguiente celda devuelve el factor de Bayes\n\nBF = beta_binom(priors[1], y) / beta_binom(priors[0], y)\nprint(round(BF))\n\n5\n\n\nVemos que el modelo con el prior \\(\\text{beta}(30, 30)\\), más concentrado, tiene \\(\\approx 5\\) veces más apoyo que el modelo con el $(1, 1). Esto es esperable ya que el prior para el primer caso se concentra alrededor de \\(\\theta = 0.5\\) y los datos \\(Y\\) tienen el mismo número de caras y cruces, es decir acuerdan con un valor de \\(\\theta\\) alrededor de 0.5.\n\n\n9.9.2 Sequential Monte Carlo\nEl método Sequential Monte Carlo es un método de meustreo que básicamente progresa mediante una serie de secuencias sucesivas desde el prior al posterior. Un subproducto de este proceso es la estimación de la verosimilitud marginal. En realidad, por razones numéricas, el valor devuelto es el logaritmo de la verosimilitud marginal.\n\nmodels = []\nidatas = []\nfor alpha, beta in priors:\n    with pm.Model() as model:\n        a = pm.Beta(\"a\", alpha, beta)\n        yl = pm.Bernoulli(\"yl\", a, observed=y)\n        idata = pm.sample_smc(random_seed=42)\n        models.append(model)\n        idatas.append(idata)\n\nInitializing SMC sampler...\nSampling 4 chains in 4 jobs\n\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:00<?  Stage: 2 Beta: 1.000]\n    \n    \n\n\n    \n\n\n/home/osvaldo/proyectos/00_BM/arviz/arviz/data/base.py:221: UserWarning: More chains (4) than draws (3). Passed array should have shape (chains, draws, *shape)\n  warnings.warn(\nInitializing SMC sampler...\nSampling 4 chains in 4 jobs\n\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:00<?  Stage: 0 Beta: 1.000]\n    \n    \n\n\n    \n\n\n/home/osvaldo/proyectos/00_BM/arviz/arviz/data/base.py:221: UserWarning: More chains (4) than draws (1). Passed array should have shape (chains, draws, *shape)\n  warnings.warn(\n\n\n\nBF_smc = np.exp(\n    idatas[1].sample_stats[\"log_marginal_likelihood\"].mean()\n    - idatas[0].sample_stats[\"log_marginal_likelihood\"].mean()\n)\nnp.round(BF_smc).item()\n\n5.0\n\n\nComo podemos ver en la celda anterior, ¡SMC da esencialmente la misma respuesta que el cálculo analítico!\nNota: En la celda de arriba calculamos una diferencia (en lugar de una división) porque estamos en la escala logarítmica, por la misma razón tomamos la exponencial antes de devolver el resultado. Finalmente, la razón por la que calculamos la media es porque obtenemos un valor logarítmico de probabilidad marginal por cadena.\nLa ventaja de usar SMC para calcular la verosimilitud marginal es que podemos usarlo para una gama más amplia de modelos, ya que ya no necesitamos conocer una expresión en forma cerrada. El costo que pagamos por esta flexibilidad es un cálculo más costoso. Además hay que tener en cuenta que SMC (con un kernel Metropolis independiente implementado en PyMC) no es tan eficiente como NUTS. A medida que aumenta la dimensionalidad del problema, una estimación más precisa de la posterior y la verosimilitud marginal requerirá un mayor número de muestras del posterior."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#factores-de-bayes-e-inferencia",
    "href": "07_Comparación_de_modelos.html#factores-de-bayes-e-inferencia",
    "title": "8  Comparación de modelos",
    "section": "9.10 Factores de bayes e inferencia",
    "text": "9.10 Factores de bayes e inferencia\nHasta ahora hemos usado los factores de Bayes para juzgar qué modelo parece ser mejor para explicar los datos, y obtenemos que uno de los modelos es \\(\\approx 5\\) mejor que el otro.\nPero, ¿qué pasa con el posterior que obtenemos de estos modelos? ¿Qué tan diferentes son?\n\naz.summary(idatas[0], var_names=\"a\", kind=\"stats\").round(2)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      a\n      0.5\n      0.05\n      0.4\n      0.59\n    \n  \n\n\n\n\n\naz.summary(idatas[1], var_names=\"a\", kind=\"stats\").round(2)\n\n\n\n\n\n  \n    \n      \n      mean\n      sd\n      hdi_3%\n      hdi_97%\n    \n  \n  \n    \n      a\n      0.5\n      0.04\n      0.42\n      0.57\n    \n  \n\n\n\n\nPodemos argumentar que los resultados son bastante similares, tenemos el mismo valor medio para \\(\\theta\\) y un posterior ligeramente más ancho para model_0, como se esperaba ya que este modelo tiene un prior más amplio. También podemos verificar la distribución predictiva posterior para ver qué tan similares son.\n\nppc_0 = pm.sample_posterior_predictive(idatas[0], model=models[0]).posterior_predictive\nppc_1 = pm.sample_posterior_predictive(idatas[1], model=models[1]).posterior_predictive\n\nSampling: [yl]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\nSampling: [yl]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00<00:00]\n    \n    \n\n\n\n_, ax = plt.subplots(figsize=(9, 6))\n\nbins = np.linspace(0.2, 0.8, 8)\nax = az.plot_dist(\n    ppc_0[\"yl\"].mean(\"yl_dim_2\"),\n    label=\"model_0\",\n    kind=\"hist\",\n    hist_kwargs={\"alpha\": 0.5, \"bins\": bins},\n)\nax = az.plot_dist(\n    ppc_1[\"yl\"].mean(\"yl_dim_2\"),\n    label=\"model_1\",\n    color=\"C1\",\n    kind=\"hist\",\n    hist_kwargs={\"alpha\": 0.5, \"bins\": bins},\n    ax=ax,\n)\nax.legend()\nax.set_xlabel(\"$\\\\theta$\")\nax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.1f\"))\nax.set_yticks([]);\n\n\n\n\nEn este ejemplo, los datos observados son más consistentes con el modelo_1, por que el prior se concentra en torno al valor correcto de \\(\\theta\\), mientras que el modelo_0, asigna la misma probabilidad a todos los valores posibles de \\(\\theta\\). Esta diferencia entre los modelos es capturada por el factor de Bayes. Podríamos decir que los factores de Bayes miden qué modelo, en su conjunto, es mejor para explicar los datos. Y esto incluye los detalles del prior, sin importar cuan similares son las predicciones de los modelos. En muchos escenarios lo que nos interesa al comparar modelos es cuan similares son las predicciones. Que es lo que estima LOO o validación cruzada."
  },
  {
    "objectID": "07_Comparación_de_modelos.html#cociente-de-savage-dickey",
    "href": "07_Comparación_de_modelos.html#cociente-de-savage-dickey",
    "title": "8  Comparación de modelos",
    "section": "9.11 Cociente de Savage-Dickey",
    "text": "9.11 Cociente de Savage-Dickey\nPara los ejemplos anteriores hemos comparado dos modelos beta-binomiales, podríamos haber comparado dos modleos completamente diferentes. Pero hay veces que queremos comparar una hipótesis nula H_0 (o modelo nulo) contra una alternativa H_1. Por ejemplo, para responder a la pregunta ¿Está sesgada esta moneda?, podríamos comparar el valor \\(\\theta = 0.5\\) (que representa el no-sesgo) con el resultado de un modelo en el que permitimos que \\(\\theta\\) varíe. Para este tipo de comparación, el modelo nulo está anidado dentro de la alternativa, lo que significa que el valor nulo es un valor particular del modelo que estamos construyendo. En esos casos, calcular el factor de Bayes es muy fácil y no requiere ningún método especial. Solo necesitamos comparar el prior y el posterior evaluados en el valor nulo (por ejemplo \\(\\theta = 0.5\\) ), bajo el modelo alternativo. Podemos ver que esto es cierto a partir de la siguiente expresión:\n\\[\nBF_{01} = \\frac{p(y \\mid H_0)}{p(y \\mid H_1)} \\frac{p(\\theta=0.5 \\mid y, H_1)}{p(\\theta=0.5 \\mid H_1)}\n\\]\nQue es cierta solo cuando H_0 es un caso particular de H_1.\nHagámoslo con PyMC y ArviZ. Solo necesitamos obtener muestras del prior y del posterior para un modelo. Probemos con el modelo beta-binomial con prior uniforme.\n\nwith pm.Model() as model_uni:\n    a = pm.Beta(\"a\", 1, 1)\n    yl = pm.Bernoulli(\"yl\", a, observed=y)\n    idata_uni = pm.sample(2000, random_seed=42)\n    idata_uni.extend(pm.sample_prior_predictive(8000))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:03<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 4 seconds.\nSampling: [a, yl]\n\n\nY ahora llamamos a la función de ArviZ az.plot_bf\n\naz.plot_bf(idata_uni, var_name=\"a\", ref_val=0.5);\n\n\n\n\nEl gráfico muestra un KDE para el prior (azul) y otro para el posterior (turquesa). Los dos puntos negros muestran que evaluamos ambas distribuciones en el valor 0.5. Podemos ver que el factor de Bayes a favor de la hipótesis nula, BF_01, es \\(\\approx 8\\), lo que podemos interpretar como una evidencia moderada a favor de la hipótesis nula (ver la escala de Jeffreys que discutimos antes).\nComo ya comentamos, los factores de Bayes miden qué modelo, en su conjunto, es mejor para explicar los datos. Y esto incluye el prior, incluso si el prior tiene un impacto relativamente bajo en el cómputo del posterior. También podemos ver este efecto del prior al comparar un segundo modelo con el modelo nulo.\nSi en cambio nuestro modelo fuera un beta-binomial con beta prior (30, 30), el BF_01 sería más bajo (anecdótico en la escala de Jeffrey). Esto se debe a que, según este modelo, el valor de \\(\\theta=0.5\\) es mucho más probable priori que para un prior uniforme y, por lo tanto, el posterior y el prior serán mucho más similares. Es decir, no hay demasiada sorpresa al ver la que el posterior se concentra alrededor de 0.5 después de recopilar datos.\nVamos a calcularlo para verlo por nosotros mismos.\n\nwith pm.Model() as model_conc:\n    a = pm.Beta(\"a\", 30, 30)\n    yl = pm.Bernoulli(\"yl\", a, observed=y)\n    idata_conc = pm.sample(2000, random_seed=42)\n    idata_conc.extend(pm.sample_prior_predictive(8000))\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [a]\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:04<00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 4 seconds.\nSampling: [a, yl]\n\n\n\naz.plot_bf(idata_conc, var_name=\"a\", ref_val=0.5);"
  }
]